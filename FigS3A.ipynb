{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "821facf5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "netNMFMU imported\n",
      "netNMFGD imported\n"
     ]
    }
   ],
   "source": [
    "import simulations as sims\n",
    "import numpy as np \n",
    "import matplotlib.pyplot as plt\n",
    "from opt_einsum import contract \n",
    "from scipy.stats import spearmanr\n",
    "import torch.nn as nn\n",
    "import torch\n",
    "from sklearn.decomposition import NMF\n",
    "import itertools\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import netNMFsc\n",
    "import scipy\n",
    "\n",
    "#scHPF imports\n",
    "from schpf import scHPF, run_trials, run_trials_pool\n",
    "from schpf import load_model, save_model\n",
    "\n",
    "\n",
    "#slalom imports\n",
    "import slalom\n",
    "from slalom import plotFactors, plotRelevance, saveFA, dumpFA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0e7e644e",
   "metadata": {},
   "outputs": [],
   "source": [
    "from spectra import spectra as spc"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecc8bd54",
   "metadata": {},
   "source": [
    "## Simulations\n",
    "\n",
    "- Edge recovery \n",
    "- K estimation\n",
    "- version of the correlated factors without NMF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e9fddb28",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "spectra.spectra.SPECTRA_Model"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spc.SPECTRA_Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8c44784",
   "metadata": {},
   "outputs": [],
   "source": [
    "N = 60\n",
    "k = 5\n",
    "p = 50\n",
    "rho = 0.8\n",
    "sigma = 4.0\n",
    "def simulate_corr_data(rho,N,k,p, sigma):\n",
    "    \"\"\" \n",
    "    first two coordinates are correlated\n",
    "    \"\"\"\n",
    "    theta_star = np.abs(np.random.standard_cauchy(size = (p,k)))\n",
    "    #theta_star[theta_star < 2.0] = 0\n",
    "    np.argsort(theta_star, axis = 1)\n",
    "    cov = np.eye(k) \n",
    "    cov[0,1] = cov[0,1] + rho\n",
    "    cov[1,0] = cov[1,0] + rho\n",
    "    lst = []\n",
    "    for i in range(N):\n",
    "        a = np.exp(np.random.multivariate_normal(np.zeros(k),cov))\n",
    "        lst.append(a)\n",
    "    A_star = np.array(lst)\n",
    "    global_mean = contract('ik,jk->ij',A_star,theta_star) \n",
    "    global_mean = global_mean + np.random.normal(loc= 0,scale = sigma, size = global_mean.shape)\n",
    "    # adding random noise \n",
    "    global_mean[global_mean < 0] = 0\n",
    "    data = np.random.poisson(global_mean) \n",
    "    return(data,A_star,theta_star)\n",
    "\n",
    "#find best permutation\n",
    "def best_permutation(true_theta, estimate):\n",
    "    max_reward = -1.0*np.inf \n",
    "    k = true_theta.shape[1]\n",
    "    for permutation in itertools.permutations(list(range(k))):\n",
    "        reward = 0\n",
    "        for i in range(k):\n",
    "            if estimate.shape[1] == k:\n",
    "                reward += spearmanr(estimate[:,permutation][:,i],true_theta[:,i]).correlation\n",
    "            else:\n",
    "                reward += spearmanr(estimate[permutation,:][i,:],true_theta[:,i]).correlation\n",
    "        reward = reward/k\n",
    "        if np.isnan(reward):\n",
    "            reward = 0.0\n",
    "        if reward > max_reward:\n",
    "            best_perm = permutation\n",
    "            max_reward = reward\n",
    "    return best_perm, max_reward"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "b3c8e858",
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_A_matrix(theta_star,n_top):\n",
    "    # get top markers for 0 and 1\n",
    "    lst0 = np.argsort(theta_star[:,0])[::-1][:n_top]\n",
    "    lst1 = np.argsort(theta_star[:,1])[::-1][:n_top]\n",
    "    A = np.zeros((p,p))\n",
    "    for i in lst0:\n",
    "        for j in lst0:\n",
    "            if i != j:\n",
    "                A[i,j] = 1\n",
    "    for i in lst1:\n",
    "        for j in lst1:\n",
    "            if i != j:\n",
    "                A[i,j] = 1\n",
    "    return(A) \n",
    "\n",
    "def construct_A_matrix_w(theta_star):\n",
    "    # get top markers for 0 and 1\n",
    "    theta = theta_star/theta_star.sum(axis = 1, keepdims =True)\n",
    "    theta[~np.isfinite(theta)] = 0.0\n",
    "    EA = contract('ik,jk->ij', theta, theta)\n",
    "    A = np.random.binomial(1, EA - np.diag(np.diag(EA)))\n",
    "    return(A) "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "253940ee",
   "metadata": {},
   "source": [
    "## New stuff "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "80b9fabb",
   "metadata": {},
   "outputs": [],
   "source": [
    "data, A_star,theta_star = simulate_corr_data(rho,N,k,p,sigma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "0622d1ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "theta = theta_star/theta_star.sum(axis = 0, keepdims =True)\n",
    "A = construct_A_matrix_w(theta_star)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "2bcd8b73",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A_star.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3e855af1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0, 0, 0, ..., 1, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       ...,\n",
       "       [0, 0, 0, ..., 0, 0, 0],\n",
       "       [1, 0, 0, ..., 0, 0, 0],\n",
       "       [0, 0, 0, ..., 0, 0, 0]])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "A"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0b6c4a11",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[ 4,  1,  0, ..., 12, 18,  5],\n",
       "       [ 5, 11,  2, ...,  4, 26, 14],\n",
       "       [ 1, 16,  5, ...,  5, 14, 11],\n",
       "       ...,\n",
       "       [13, 10,  9, ..., 39, 22, 11],\n",
       "       [ 1,  8,  1, ..., 15, 32,  8],\n",
       "       [16,  2,  5, ...,  3, 45, 13]])"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8b85bc7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 47%|████▋     | 4658/10000 [00:09<00:10, 496.85it/s]\n"
     ]
    }
   ],
   "source": [
    "model = spc.SPECTRA_Model(X = data, labels = None, L = k, adj_matrix = A, use_weights = False, lam = 1, delta=0.0,use_cell_types = False, kappa = 0.0, rho = 0.0)\n",
    "model.train(X = data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "951d721f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(50, 5)"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.internal_model.theta.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "a8a220e7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n"
     ]
    }
   ],
   "source": [
    "nmf = NMF(n_components=k)\n",
    "Y_nmf = nmf.fit_transform(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "25d918fe",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(60, 5)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.internal_model.alpha.detach().numpy().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "084fa8b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "#slalom, HPF, Spectra, "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "20ecbe10",
   "metadata": {},
   "outputs": [],
   "source": [
    "n_top = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "id": "16b678bf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([50, 5])"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.internal_model.theta.exp().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "id": "5765891e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([176, 297, 187])"
      ]
     },
     "execution_count": 132,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "theta_star.argsort(axis = 0)[-1*n_top:,:][:,2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8532e11e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_mask(input_lst, G_input, D):\n",
    "    input_mask = np.zeros((G_input, D))\n",
    "    for i in range(len(input_lst)):\n",
    "        for input_ in input_lst[i]:\n",
    "            input_mask[i,input_]  = 1.0 \n",
    "    return input_mask"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "58e22c5a",
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'theta' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m/scratch/lsftmp/11219833.tmpdir/ipykernel_529973/4214051472.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtheta\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0margsort\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0maxis\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m: name 'theta' is not defined"
     ]
    }
   ],
   "source": [
    "theta.argsort(axis = 0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "11bc70a3",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [01:44<00:00, 95.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:204.078918  pct:100.000000000\n",
      "[Iter.   10]  loss:41.247459  pct:-79.788472345\n",
      "[Iter.   20]  loss:33.173874  pct:-19.573534044\n",
      "[Iter.   30]  loss:27.698977  pct:-16.503641995\n",
      "[Iter.   40]  loss:23.549879  pct:-14.979244595\n",
      "[Iter.   50]  loss:20.318878  pct:-13.719819495\n",
      "[Iter.   60]  loss:17.745943  pct:-12.662781293\n",
      "[Iter.   70]  loss:15.628215  pct:-11.933590822\n",
      "[Iter.   80]  loss:13.869526  pct:-11.253293771\n",
      "[Iter.   90]  loss:12.398685  pct:-10.604842713\n",
      "[Iter.  100]  loss:11.151853  pct:-10.056162763\n",
      "[Iter.  110]  loss:10.084271  pct:-9.573128469\n",
      "[Iter.  120]  loss:9.163227  pct:-9.133474401\n",
      "[Iter.  130]  loss:8.364876  pct:-8.712555967\n",
      "[Iter.  140]  loss:7.669641  pct:-8.311352325\n",
      "[Iter.  150]  loss:7.062251  pct:-7.919416846\n",
      "[Iter.  160]  loss:6.529931  pct:-7.537540816\n",
      "[Iter.  170]  loss:6.062918  pct:-7.151873957\n",
      "[Iter.  180]  loss:5.652761  pct:-6.765020868\n",
      "[Iter.  190]  loss:5.291849  pct:-6.384692381\n",
      "[Iter.  200]  loss:4.973930  pct:-6.007715755\n",
      "[Iter.  210]  loss:4.693635  pct:-5.635280388\n",
      "[Iter.  220]  loss:4.446259  pct:-5.270455965\n",
      "[Iter.  230]  loss:4.227933  pct:-4.910332275\n",
      "[Iter.  240]  loss:4.035224  pct:-4.557994942\n",
      "[Iter.  250]  loss:3.864989  pct:-4.218734749\n",
      "[Iter.  260]  loss:3.714551  pct:-3.892310481\n",
      "[Iter.  270]  loss:3.581593  pct:-3.579393704\n",
      "[Iter.  280]  loss:3.464061  pct:-3.281543845\n",
      "[Iter.  290]  loss:3.360125  pct:-3.000414472\n",
      "[Iter.  300]  loss:3.268203  pct:-2.735659124\n",
      "[Iter.  310]  loss:3.186901  pct:-2.487693356\n",
      "[Iter.  320]  loss:3.114971  pct:-2.257042246\n",
      "[Iter.  330]  loss:3.051341  pct:-2.042703730\n",
      "[Iter.  340]  loss:2.995034  pct:-1.845329984\n",
      "[Iter.  350]  loss:2.945205  pct:-1.663728858\n",
      "[Iter.  360]  loss:2.901094  pct:-1.497723160\n",
      "[Iter.  370]  loss:2.862026  pct:-1.346639307\n",
      "[Iter.  380]  loss:2.827432  pct:-1.208751031\n",
      "[Iter.  390]  loss:2.796793  pct:-1.083622815\n",
      "[Iter.  400]  loss:2.769634  pct:-0.971067124\n",
      "[Iter.  410]  loss:2.745562  pct:-0.869163397\n",
      "[Iter.  420]  loss:2.724219  pct:-0.777337414\n",
      "[Iter.  430]  loss:2.705282  pct:-0.695156502\n",
      "[Iter.  440]  loss:2.688415  pct:-0.623463550\n",
      "[Iter.  450]  loss:2.673300  pct:-0.562236837\n",
      "[Iter.  460]  loss:2.659990  pct:-0.497884841\n",
      "[Iter.  470]  loss:2.648212  pct:-0.442779014\n",
      "[Iter.  480]  loss:2.637749  pct:-0.395096713\n",
      "[Iter.  490]  loss:2.628451  pct:-0.352491729\n",
      "[Iter.  500]  loss:2.620180  pct:-0.314671182\n",
      "[Iter.  510]  loss:2.612814  pct:-0.281141670\n",
      "[Iter.  520]  loss:2.606223  pct:-0.252241641\n",
      "[Iter.  530]  loss:2.600342  pct:-0.225654916\n",
      "[Iter.  540]  loss:2.595094  pct:-0.201822268\n",
      "[Iter.  550]  loss:2.590410  pct:-0.180520871\n",
      "[Iter.  560]  loss:2.586228  pct:-0.161436323\n",
      "[Iter.  570]  loss:2.582490  pct:-0.144504341\n",
      "[Iter.  580]  loss:2.579146  pct:-0.129489695\n",
      "[Iter.  590]  loss:2.576142  pct:-0.116494006\n",
      "[Iter.  600]  loss:2.573448  pct:-0.104570777\n",
      "[Iter.  610]  loss:2.571032  pct:-0.093896296\n",
      "[Iter.  620]  loss:2.568845  pct:-0.085045116\n",
      "[Iter.  630]  loss:2.566877  pct:-0.076606682\n",
      "[Iter.  640]  loss:2.565105  pct:-0.069030452\n",
      "[Iter.  650]  loss:2.563506  pct:-0.062330192\n",
      "[Iter.  660]  loss:2.562063  pct:-0.056314449\n",
      "[Iter.  670]  loss:2.560759  pct:-0.050883718\n",
      "[Iter.  680]  loss:2.559581  pct:-0.045993698\n",
      "[Iter.  690]  loss:2.558509  pct:-0.041879113\n",
      "[Iter.  700]  loss:2.557537  pct:-0.038010781\n",
      "[Iter.  710]  loss:2.556654  pct:-0.034520089\n",
      "[Iter.  720]  loss:2.555851  pct:-0.031389345\n",
      "[Iter.  730]  loss:2.555122  pct:-0.028544728\n",
      "[Iter.  740]  loss:2.554458  pct:-0.025977521\n",
      "[Iter.  750]  loss:2.553853  pct:-0.023678914\n",
      "[Iter.  760]  loss:2.553302  pct:-0.021593338\n",
      "[Iter.  770]  loss:2.552800  pct:-0.019646431\n",
      "[Iter.  780]  loss:2.552342  pct:-0.017931825\n",
      "[Iter.  790]  loss:2.551924  pct:-0.016393749\n",
      "[Iter.  800]  loss:2.551544  pct:-0.014892262\n",
      "[Iter.  810]  loss:2.551198  pct:-0.013558275\n",
      "[Iter.  820]  loss:2.550885  pct:-0.012279800\n",
      "[Iter.  830]  loss:2.550603  pct:-0.011056916\n",
      "[Iter.  840]  loss:2.550334  pct:-0.010515981\n",
      "[Iter.  850]  loss:2.550087  pct:-0.009713114\n",
      "[Iter.  860]  loss:2.549863  pct:-0.008760416\n",
      "[Iter.  870]  loss:2.549663  pct:-0.007872910\n",
      "[Iter.  880]  loss:2.549474  pct:-0.007387279\n",
      "[Iter.  890]  loss:2.549295  pct:-0.007032461\n",
      "[Iter.  900]  loss:2.549127  pct:-0.006602748\n",
      "[Iter.  910]  loss:2.548972  pct:-0.006060713\n",
      "[Iter.  920]  loss:2.548830  pct:-0.005565343\n",
      "[Iter.  930]  loss:2.548700  pct:-0.005116659\n",
      "[Iter.  940]  loss:2.548583  pct:-0.004602423\n",
      "[Iter.  950]  loss:2.548478  pct:-0.004106822\n",
      "[Iter.  960]  loss:2.548387  pct:-0.003555026\n",
      "[Iter.  970]  loss:2.548296  pct:-0.003583220\n",
      "[Iter.  980]  loss:2.548207  pct:-0.003508500\n",
      "[Iter.  990]  loss:2.548121  pct:-0.003349566\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.546316  pct:100.000000000\n",
      "[Iter.    2]  loss:2.546315  pct:-0.000056180\n",
      "[Iter.    4]  loss:2.546315  pct:0.000009363\n",
      "[Iter.    6]  loss:2.546315  pct:-0.000009363\n",
      "[Iter.    8]  loss:2.546315  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.546315\n",
      "Best loss: 2.546315 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  1%|▏         | 143/10000 [00:02<02:32, 64.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3015.165039  pct:100.000000000\n",
      "[Iter.   10]  loss:494.626434  pct:-83.595376756\n",
      "[Iter.   20]  loss:473.331757  pct:-4.305204141\n",
      "[Iter.   30]  loss:455.623047  pct:-3.741289164\n",
      "[Iter.   40]  loss:439.889252  pct:-3.453248310\n",
      "[Iter.   50]  loss:425.526154  pct:-3.265162331\n",
      "[Iter.   60]  loss:412.161865  pct:-3.140650279\n",
      "[Iter.   70]  loss:399.631500  pct:-3.040156319\n",
      "[Iter.   80]  loss:387.853333  pct:-2.947257090\n",
      "[Iter.   90]  loss:376.756256  pct:-2.861152783\n",
      "[Iter.  100]  loss:366.274170  pct:-2.782192999\n",
      "[Iter.  110]  loss:356.347443  pct:-2.710190374\n",
      "[Iter.  120]  loss:346.926758  pct:-2.643679647\n",
      "[Iter.  130]  loss:337.968842  pct:-2.582077069\n",
      "[Iter.  140]  loss:329.438293  pct:-2.524063478\n",
      "[Iter.  150]  loss:321.301727  pct:-2.469830109\n",
      "[Iter.  160]  loss:313.528717  pct:-2.419224546\n",
      "[Iter.  170]  loss:306.092773  pct:-2.371694585\n",
      "[Iter.  180]  loss:298.970001  pct:-2.326997837\n",
      "[Iter.  190]  loss:292.138763  pct:-2.284924161\n",
      "[Iter.  200]  loss:285.579590  pct:-2.245225354\n",
      "[Iter.  210]  loss:279.274750  pct:-2.207734835\n",
      "[Iter.  220]  loss:273.208130  pct:-2.172276541\n",
      "[Iter.  230]  loss:267.364868  pct:-2.138758360\n",
      "[Iter.  240]  loss:261.731628  pct:-2.106948375\n",
      "[Iter.  250]  loss:256.295807  pct:-2.076868419\n",
      "[Iter.  260]  loss:251.045929  pct:-2.048366688\n",
      "[Iter.  270]  loss:245.971893  pct:-2.021158306\n",
      "[Iter.  280]  loss:241.063889  pct:-1.995351865\n",
      "[Iter.  290]  loss:236.312973  pct:-1.970811786\n",
      "[Iter.  300]  loss:231.711212  pct:-1.947316224\n",
      "[Iter.  310]  loss:227.250778  pct:-1.924997033\n",
      "[Iter.  320]  loss:222.924805  pct:-1.903612188\n",
      "[Iter.  330]  loss:218.726730  pct:-1.883179553\n",
      "[Iter.  340]  loss:214.650467  pct:-1.863632955\n",
      "[Iter.  350]  loss:210.690521  pct:-1.844834412\n",
      "[Iter.  360]  loss:206.841476  pct:-1.826871364\n",
      "[Iter.  370]  loss:203.098358  pct:-1.809655564\n",
      "[Iter.  380]  loss:199.456741  pct:-1.793031147\n",
      "[Iter.  390]  loss:195.912140  pct:-1.777127921\n",
      "[Iter.  400]  loss:192.460480  pct:-1.761840873\n",
      "[Iter.  410]  loss:189.097931  pct:-1.747137299\n",
      "[Iter.  420]  loss:185.820908  pct:-1.732976823\n",
      "[Iter.  430]  loss:182.625977  pct:-1.719360362\n",
      "[Iter.  440]  loss:179.509964  pct:-1.706226371\n",
      "[Iter.  450]  loss:176.469849  pct:-1.693563571\n",
      "[Iter.  460]  loss:173.502762  pct:-1.681356229\n",
      "[Iter.  470]  loss:170.606033  pct:-1.669557582\n",
      "[Iter.  480]  loss:167.777039  pct:-1.658203228\n",
      "[Iter.  490]  loss:165.013458  pct:-1.647174337\n",
      "[Iter.  500]  loss:162.312988  pct:-1.636514984\n",
      "[Iter.  510]  loss:159.673416  pct:-1.626223614\n",
      "[Iter.  520]  loss:157.092590  pct:-1.616315269\n",
      "[Iter.  530]  loss:154.568619  pct:-1.606677662\n",
      "[Iter.  540]  loss:152.099731  pct:-1.597275921\n",
      "[Iter.  550]  loss:149.684021  pct:-1.588241101\n",
      "[Iter.  560]  loss:147.319702  pct:-1.579539908\n",
      "[Iter.  570]  loss:145.005249  pct:-1.571041138\n",
      "[Iter.  580]  loss:142.739105  pct:-1.562801219\n",
      "[Iter.  590]  loss:140.519806  pct:-1.554794191\n",
      "[Iter.  600]  loss:138.345856  pct:-1.547077425\n",
      "[Iter.  610]  loss:136.215958  pct:-1.539545988\n",
      "[Iter.  620]  loss:134.128799  pct:-1.532242066\n",
      "[Iter.  630]  loss:132.083160  pct:-1.525130357\n",
      "[Iter.  640]  loss:130.077805  pct:-1.518252462\n",
      "[Iter.  650]  loss:128.111679  pct:-1.511499594\n",
      "[Iter.  660]  loss:126.183517  pct:-1.505063110\n",
      "[Iter.  670]  loss:124.292473  pct:-1.498646301\n",
      "[Iter.  680]  loss:122.437469  pct:-1.492450278\n",
      "[Iter.  690]  loss:120.617508  pct:-1.486441655\n",
      "[Iter.  700]  loss:118.831802  pct:-1.480469624\n",
      "[Iter.  710]  loss:117.079254  pct:-1.474814135\n",
      "[Iter.  720]  loss:115.359001  pct:-1.469306414\n",
      "[Iter.  730]  loss:113.670219  pct:-1.463935819\n",
      "[Iter.  740]  loss:112.012230  pct:-1.458596201\n",
      "[Iter.  750]  loss:110.384209  pct:-1.453431685\n",
      "[Iter.  760]  loss:108.785370  pct:-1.448430736\n",
      "[Iter.  770]  loss:107.214897  pct:-1.443643313\n",
      "[Iter.  780]  loss:105.672241  pct:-1.438844774\n",
      "[Iter.  790]  loss:104.156662  pct:-1.434226440\n",
      "[Iter.  800]  loss:102.667511  pct:-1.429722279\n",
      "[Iter.  810]  loss:101.204239  pct:-1.425253306\n",
      "[Iter.  820]  loss:99.766205  pct:-1.420922753\n",
      "[Iter.  830]  loss:98.352798  pct:-1.416718592\n",
      "[Iter.  840]  loss:96.963631  pct:-1.412433411\n",
      "[Iter.  850]  loss:95.597816  pct:-1.408584022\n",
      "[Iter.  860]  loss:94.255089  pct:-1.404558923\n",
      "[Iter.  870]  loss:92.935036  pct:-1.400511227\n",
      "[Iter.  880]  loss:91.636925  pct:-1.396793956\n",
      "[Iter.  890]  loss:90.360344  pct:-1.393085608\n",
      "[Iter.  900]  loss:89.104744  pct:-1.389547584\n",
      "[Iter.  910]  loss:87.869781  pct:-1.385967131\n",
      "[Iter.  920]  loss:86.655075  pct:-1.382393811\n",
      "[Iter.  930]  loss:85.460075  pct:-1.379030246\n",
      "[Iter.  940]  loss:84.284439  pct:-1.375655575\n",
      "[Iter.  950]  loss:83.127663  pct:-1.372467374\n",
      "[Iter.  960]  loss:81.989510  pct:-1.369162851\n",
      "[Iter.  970]  loss:80.869476  pct:-1.366068988\n",
      "[Iter.  980]  loss:79.767296  pct:-1.362912846\n",
      "[Iter.  990]  loss:78.682533  pct:-1.359908922\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.713382  pct:100.000000000\n",
      "[Iter.    2]  loss:2.712814  pct:-0.020947653\n",
      "[Iter.    4]  loss:2.712680  pct:-0.004921621\n",
      "[Iter.    6]  loss:2.712618  pct:-0.002285151\n",
      "[Iter.    8]  loss:2.712607  pct:-0.000421884\n",
      "[Iter.   10]  loss:2.712591  pct:-0.000571303\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.712591\n",
      "Best loss: 2.712591 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1820/10000 [00:26<01:59, 68.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:373.462189  pct:100.000000000\n",
      "[Iter.   10]  loss:73.838470  pct:-80.228662315\n",
      "[Iter.   20]  loss:61.781754  pct:-16.328503074\n",
      "[Iter.   30]  loss:53.832203  pct:-12.867149560\n",
      "[Iter.   40]  loss:47.753956  pct:-11.291098528\n",
      "[Iter.   50]  loss:42.842529  pct:-10.284857993\n",
      "[Iter.   60]  loss:38.756931  pct:-9.536313703\n",
      "[Iter.   70]  loss:35.286564  pct:-8.954185264\n",
      "[Iter.   80]  loss:32.291801  pct:-8.486976605\n",
      "[Iter.   90]  loss:29.675690  pct:-8.101473556\n",
      "[Iter.  100]  loss:27.368105  pct:-7.776010553\n",
      "[Iter.  110]  loss:25.316547  pct:-7.496162214\n",
      "[Iter.  120]  loss:23.480757  pct:-7.251346740\n",
      "[Iter.  130]  loss:21.829052  pct:-7.034291122\n",
      "[Iter.  140]  loss:20.336180  pct:-6.838923835\n",
      "[Iter.  150]  loss:18.981541  pct:-6.661226794\n",
      "[Iter.  160]  loss:17.748280  pct:-6.497160211\n",
      "[Iter.  170]  loss:16.622253  pct:-6.344424253\n",
      "[Iter.  180]  loss:15.591619  pct:-6.200326156\n",
      "[Iter.  190]  loss:14.646227  pct:-6.063466397\n",
      "[Iter.  200]  loss:13.777422  pct:-5.931936864\n",
      "[Iter.  210]  loss:12.977633  pct:-5.805066273\n",
      "[Iter.  220]  loss:12.240389  pct:-5.680886329\n",
      "[Iter.  230]  loss:11.559883  pct:-5.559510893\n",
      "[Iter.  240]  loss:10.931086  pct:-5.439480008\n",
      "[Iter.  250]  loss:10.349456  pct:-5.320878228\n",
      "[Iter.  260]  loss:9.811004  pct:-5.202709757\n",
      "[Iter.  270]  loss:9.312127  pct:-5.084867845\n",
      "[Iter.  280]  loss:8.849625  pct:-4.966668452\n",
      "[Iter.  290]  loss:8.420577  pct:-4.848200938\n",
      "[Iter.  300]  loss:8.022373  pct:-4.728937785\n",
      "[Iter.  310]  loss:7.652627  pct:-4.608932072\n",
      "[Iter.  320]  loss:7.309164  pct:-4.488176411\n",
      "[Iter.  330]  loss:6.989965  pct:-4.367101441\n",
      "[Iter.  340]  loss:6.693258  pct:-4.244765339\n",
      "[Iter.  350]  loss:6.417372  pct:-4.121850177\n",
      "[Iter.  360]  loss:6.160794  pct:-3.998170930\n",
      "[Iter.  370]  loss:5.922122  pct:-3.874042366\n",
      "[Iter.  380]  loss:5.700066  pct:-3.749608180\n",
      "[Iter.  390]  loss:5.493417  pct:-3.625384341\n",
      "[Iter.  400]  loss:5.301111  pct:-3.500663598\n",
      "[Iter.  410]  loss:5.122129  pct:-3.376298153\n",
      "[Iter.  420]  loss:4.955524  pct:-3.252660334\n",
      "[Iter.  430]  loss:4.800421  pct:-3.129895664\n",
      "[Iter.  440]  loss:4.656025  pct:-3.007982448\n",
      "[Iter.  450]  loss:4.521595  pct:-2.887235284\n",
      "[Iter.  460]  loss:4.396428  pct:-2.768202216\n",
      "[Iter.  470]  loss:4.279885  pct:-2.650852312\n",
      "[Iter.  480]  loss:4.171373  pct:-2.535393295\n",
      "[Iter.  490]  loss:4.070333  pct:-2.422243977\n",
      "[Iter.  500]  loss:3.976246  pct:-2.311522373\n",
      "[Iter.  510]  loss:3.888640  pct:-2.203232794\n",
      "[Iter.  520]  loss:3.807063  pct:-2.097824065\n",
      "[Iter.  530]  loss:3.731100  pct:-1.995311868\n",
      "[Iter.  540]  loss:3.660367  pct:-1.895788897\n",
      "[Iter.  550]  loss:3.594508  pct:-1.799222156\n",
      "[Iter.  560]  loss:3.533176  pct:-1.706282396\n",
      "[Iter.  570]  loss:3.476073  pct:-1.616199078\n",
      "[Iter.  580]  loss:3.422901  pct:-1.529660475\n",
      "[Iter.  590]  loss:3.373389  pct:-1.446475879\n",
      "[Iter.  600]  loss:3.327280  pct:-1.366864984\n",
      "[Iter.  610]  loss:3.284348  pct:-1.290297201\n",
      "[Iter.  620]  loss:3.244379  pct:-1.216945701\n",
      "[Iter.  630]  loss:3.207153  pct:-1.147413358\n",
      "[Iter.  640]  loss:3.172495  pct:-1.080638218\n",
      "[Iter.  650]  loss:3.140227  pct:-1.017111455\n",
      "[Iter.  660]  loss:3.110180  pct:-0.956839715\n",
      "[Iter.  670]  loss:3.082204  pct:-0.899491232\n",
      "[Iter.  680]  loss:3.056149  pct:-0.845347456\n",
      "[Iter.  690]  loss:3.031891  pct:-0.793748495\n",
      "[Iter.  700]  loss:3.009302  pct:-0.745045559\n",
      "[Iter.  710]  loss:2.988267  pct:-0.698997863\n",
      "[Iter.  720]  loss:2.968675  pct:-0.655616458\n",
      "[Iter.  730]  loss:2.950440  pct:-0.614270041\n",
      "[Iter.  740]  loss:2.933451  pct:-0.575804153\n",
      "[Iter.  750]  loss:2.917630  pct:-0.539338098\n",
      "[Iter.  760]  loss:2.902898  pct:-0.504934616\n",
      "[Iter.  770]  loss:2.889177  pct:-0.472657073\n",
      "[Iter.  780]  loss:2.876395  pct:-0.442396596\n",
      "[Iter.  790]  loss:2.864491  pct:-0.413876243\n",
      "[Iter.  800]  loss:2.853399  pct:-0.387214030\n",
      "[Iter.  810]  loss:2.843067  pct:-0.362081554\n",
      "[Iter.  820]  loss:2.833440  pct:-0.338608440\n",
      "[Iter.  830]  loss:2.824469  pct:-0.316627553\n",
      "[Iter.  840]  loss:2.816108  pct:-0.296015399\n",
      "[Iter.  850]  loss:2.808317  pct:-0.276659928\n",
      "[Iter.  860]  loss:2.801057  pct:-0.258503846\n",
      "[Iter.  870]  loss:2.794289  pct:-0.241622623\n",
      "[Iter.  880]  loss:2.787983  pct:-0.225672155\n",
      "[Iter.  890]  loss:2.782102  pct:-0.210952098\n",
      "[Iter.  900]  loss:2.776620  pct:-0.197060892\n",
      "[Iter.  910]  loss:2.771508  pct:-0.184106336\n",
      "[Iter.  920]  loss:2.766737  pct:-0.172135755\n",
      "[Iter.  930]  loss:2.762288  pct:-0.160807808\n",
      "[Iter.  940]  loss:2.758138  pct:-0.150217398\n",
      "[Iter.  950]  loss:2.754270  pct:-0.140251897\n",
      "[Iter.  960]  loss:2.750662  pct:-0.130987515\n",
      "[Iter.  970]  loss:2.747292  pct:-0.122534977\n",
      "[Iter.  980]  loss:2.744147  pct:-0.114458269\n",
      "[Iter.  990]  loss:2.741210  pct:-0.107030642\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.697176  pct:100.000000000\n",
      "[Iter.    2]  loss:2.697180  pct:0.000141433\n",
      "[Iter.    4]  loss:2.697180  pct:0.000000000\n",
      "[Iter.    6]  loss:2.697179  pct:-0.000017679\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.697179\n",
      "Best loss: 2.697179 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  7%|▋         | 661/10000 [00:06<01:35, 97.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3847.273682  pct:100.000000000\n",
      "[Iter.   10]  loss:689.786926  pct:-82.070761731\n",
      "[Iter.   20]  loss:663.564148  pct:-3.801576592\n",
      "[Iter.   30]  loss:640.781189  pct:-3.433422233\n",
      "[Iter.   40]  loss:619.753296  pct:-3.281602742\n",
      "[Iter.   50]  loss:600.214539  pct:-3.152666949\n",
      "[Iter.   60]  loss:582.015747  pct:-3.032047765\n",
      "[Iter.   70]  loss:565.020935  pct:-2.919991787\n",
      "[Iter.   80]  loss:549.107422  pct:-2.816446648\n",
      "[Iter.   90]  loss:534.165588  pct:-2.721113010\n",
      "[Iter.  100]  loss:520.099304  pct:-2.633319047\n",
      "[Iter.  110]  loss:506.823120  pct:-2.552624850\n",
      "[Iter.  120]  loss:494.263123  pct:-2.478181650\n",
      "[Iter.  130]  loss:482.354370  pct:-2.409395299\n",
      "[Iter.  140]  loss:471.042328  pct:-2.345172541\n",
      "[Iter.  150]  loss:460.280609  pct:-2.284660659\n",
      "[Iter.  160]  loss:450.028778  pct:-2.227300228\n",
      "[Iter.  170]  loss:440.248077  pct:-2.173350052\n",
      "[Iter.  180]  loss:430.901520  pct:-2.123020655\n",
      "[Iter.  190]  loss:421.955292  pct:-2.076165346\n",
      "[Iter.  200]  loss:413.377533  pct:-2.032859632\n",
      "[Iter.  210]  loss:405.134644  pct:-1.994034205\n",
      "[Iter.  220]  loss:397.214111  pct:-1.955037011\n",
      "[Iter.  230]  loss:389.590515  pct:-1.919266203\n",
      "[Iter.  240]  loss:382.245148  pct:-1.885407151\n",
      "[Iter.  250]  loss:375.160645  pct:-1.853392572\n",
      "[Iter.  260]  loss:368.311615  pct:-1.825625806\n",
      "[Iter.  270]  loss:361.694000  pct:-1.796743430\n",
      "[Iter.  280]  loss:355.293335  pct:-1.769635459\n",
      "[Iter.  290]  loss:349.105560  pct:-1.741596042\n",
      "[Iter.  300]  loss:343.113251  pct:-1.716474973\n",
      "[Iter.  310]  loss:337.306030  pct:-1.692508362\n",
      "[Iter.  320]  loss:331.672516  pct:-1.670149330\n",
      "[Iter.  330]  loss:326.204224  pct:-1.648702251\n",
      "[Iter.  340]  loss:320.892700  pct:-1.628281626\n",
      "[Iter.  350]  loss:315.732269  pct:-1.608148426\n",
      "[Iter.  360]  loss:310.714752  pct:-1.589168285\n",
      "[Iter.  370]  loss:305.834106  pct:-1.570780183\n",
      "[Iter.  380]  loss:301.084290  pct:-1.553069718\n",
      "[Iter.  390]  loss:296.459534  pct:-1.536033603\n",
      "[Iter.  400]  loss:291.954315  pct:-1.519674017\n",
      "[Iter.  410]  loss:287.562683  pct:-1.504218931\n",
      "[Iter.  420]  loss:283.281403  pct:-1.488816446\n",
      "[Iter.  430]  loss:279.105621  pct:-1.474075323\n",
      "[Iter.  440]  loss:275.031494  pct:-1.459708041\n",
      "[Iter.  450]  loss:271.054810  pct:-1.445901526\n",
      "[Iter.  460]  loss:267.171875  pct:-1.432527457\n",
      "[Iter.  470]  loss:263.378967  pct:-1.419650820\n",
      "[Iter.  480]  loss:259.672791  pct:-1.407165043\n",
      "[Iter.  490]  loss:256.050110  pct:-1.395094440\n",
      "[Iter.  500]  loss:252.507736  pct:-1.383468907\n",
      "[Iter.  510]  loss:249.043182  pct:-1.372058490\n",
      "[Iter.  520]  loss:245.653366  pct:-1.361135949\n",
      "[Iter.  530]  loss:242.336105  pct:-1.350382775\n",
      "[Iter.  540]  loss:239.088715  pct:-1.340035874\n",
      "[Iter.  550]  loss:235.908981  pct:-1.329938672\n",
      "[Iter.  560]  loss:232.794510  pct:-1.320200451\n",
      "[Iter.  570]  loss:229.743347  pct:-1.310667817\n",
      "[Iter.  580]  loss:226.753525  pct:-1.301374958\n",
      "[Iter.  590]  loss:223.822906  pct:-1.292424578\n",
      "[Iter.  600]  loss:220.949722  pct:-1.283686397\n",
      "[Iter.  610]  loss:218.132217  pct:-1.275179192\n",
      "[Iter.  620]  loss:215.368683  pct:-1.266908015\n",
      "[Iter.  630]  loss:212.657486  pct:-1.258863110\n",
      "[Iter.  640]  loss:209.997208  pct:-1.250968574\n",
      "[Iter.  650]  loss:207.386276  pct:-1.243317197\n",
      "[Iter.  660]  loss:204.823441  pct:-1.235778828\n",
      "[Iter.  670]  loss:202.307053  pct:-1.228564432\n",
      "[Iter.  680]  loss:199.835815  pct:-1.221527945\n",
      "[Iter.  690]  loss:197.408768  pct:-1.214520893\n",
      "[Iter.  700]  loss:195.024384  pct:-1.207841061\n",
      "[Iter.  710]  loss:192.681396  pct:-1.201381601\n",
      "[Iter.  720]  loss:190.376801  pct:-1.196065624\n",
      "[Iter.  730]  loss:188.112152  pct:-1.189561139\n",
      "[Iter.  740]  loss:185.887390  pct:-1.182678492\n",
      "[Iter.  750]  loss:183.700134  pct:-1.176656393\n",
      "[Iter.  760]  loss:181.549286  pct:-1.170847478\n",
      "[Iter.  770]  loss:179.434036  pct:-1.165110413\n",
      "[Iter.  780]  loss:177.353424  pct:-1.159541537\n",
      "[Iter.  790]  loss:175.306519  pct:-1.154139272\n",
      "[Iter.  800]  loss:173.292130  pct:-1.149066820\n",
      "[Iter.  810]  loss:171.310471  pct:-1.143536606\n",
      "[Iter.  820]  loss:169.360428  pct:-1.138309128\n",
      "[Iter.  830]  loss:167.441040  pct:-1.133315404\n",
      "[Iter.  840]  loss:165.551697  pct:-1.128363310\n",
      "[Iter.  850]  loss:163.691620  pct:-1.123562573\n",
      "[Iter.  860]  loss:161.860199  pct:-1.118823859\n",
      "[Iter.  870]  loss:160.056854  pct:-1.114137223\n",
      "[Iter.  880]  loss:158.280899  pct:-1.109577724\n",
      "[Iter.  890]  loss:156.531647  pct:-1.105156927\n",
      "[Iter.  900]  loss:154.808289  pct:-1.100964687\n",
      "[Iter.  910]  loss:153.108917  pct:-1.097726326\n",
      "[Iter.  920]  loss:151.436020  pct:-1.092619143\n",
      "[Iter.  930]  loss:149.785645  pct:-1.089816919\n",
      "[Iter.  940]  loss:148.160248  pct:-1.085148536\n",
      "[Iter.  950]  loss:146.559479  pct:-1.080430862\n",
      "[Iter.  960]  loss:144.981476  pct:-1.076697968\n",
      "[Iter.  970]  loss:143.425507  pct:-1.073219340\n",
      "[Iter.  980]  loss:141.891815  pct:-1.069329607\n",
      "[Iter.  990]  loss:140.379730  pct:-1.065660453\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:3.867388  pct:100.000000000\n",
      "[Iter.    2]  loss:3.865515  pct:-0.048443367\n",
      "[Iter.    4]  loss:3.864825  pct:-0.017862050\n",
      "[Iter.    6]  loss:3.864611  pct:-0.005521198\n",
      "[Iter.    8]  loss:3.864481  pct:-0.003368425\n",
      "[Iter.   10]  loss:3.864398  pct:-0.002140811\n",
      "[Iter.   12]  loss:3.864387  pct:-0.000296142\n",
      "[Iter.   14]  loss:3.864373  pct:-0.000364008\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 3.864373\n",
      "Best loss: 3.864373 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:22<00:00, 69.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:326.474365  pct:100.000000000\n",
      "[Iter.   10]  loss:77.720818  pct:-76.193896529\n",
      "[Iter.   20]  loss:62.968998  pct:-18.980525518\n",
      "[Iter.   30]  loss:53.821148  pct:-14.527545830\n",
      "[Iter.   40]  loss:47.266792  pct:-12.178030151\n",
      "[Iter.   50]  loss:42.092037  pct:-10.947971810\n",
      "[Iter.   60]  loss:37.859875  pct:-10.054544178\n",
      "[Iter.   70]  loss:34.309372  pct:-9.378009840\n",
      "[Iter.   80]  loss:31.274147  pct:-8.846635022\n",
      "[Iter.   90]  loss:28.642752  pct:-8.413963576\n",
      "[Iter.  100]  loss:26.336475  pct:-8.051867174\n",
      "[Iter.  110]  loss:24.297274  pct:-7.742880198\n",
      "[Iter.  120]  loss:22.481138  pct:-7.474646883\n",
      "[Iter.  130]  loss:20.854019  pct:-7.237707663\n",
      "[Iter.  140]  loss:19.389027  pct:-7.024988860\n",
      "[Iter.  150]  loss:18.064434  pct:-6.831661098\n",
      "[Iter.  160]  loss:16.862474  pct:-6.653735216\n",
      "[Iter.  170]  loss:15.768442  pct:-6.487970027\n",
      "[Iter.  180]  loss:14.770007  pct:-6.331855809\n",
      "[Iter.  190]  loss:13.856731  pct:-6.183312645\n",
      "[Iter.  200]  loss:13.019689  pct:-6.040694472\n",
      "[Iter.  210]  loss:12.251189  pct:-5.902594122\n",
      "[Iter.  220]  loss:11.544529  pct:-5.768095303\n",
      "[Iter.  230]  loss:10.893878  pct:-5.636011485\n",
      "[Iter.  240]  loss:10.294118  pct:-5.505477998\n",
      "[Iter.  250]  loss:9.740690  pct:-5.376154617\n",
      "[Iter.  260]  loss:9.229600  pct:-5.246961627\n",
      "[Iter.  270]  loss:8.757200  pct:-5.118311888\n",
      "[Iter.  280]  loss:8.320279  pct:-4.989278624\n",
      "[Iter.  290]  loss:7.915932  pct:-4.859776181\n",
      "[Iter.  300]  loss:7.541530  pct:-4.729733841\n",
      "[Iter.  310]  loss:7.194679  pct:-4.599211140\n",
      "[Iter.  320]  loss:6.873250  pct:-4.467590360\n",
      "[Iter.  330]  loss:6.575247  pct:-4.335695579\n",
      "[Iter.  340]  loss:6.298894  pct:-4.202920643\n",
      "[Iter.  350]  loss:6.042539  pct:-4.069845740\n",
      "[Iter.  360]  loss:5.804703  pct:-3.936033449\n",
      "[Iter.  370]  loss:5.583989  pct:-3.802332375\n",
      "[Iter.  380]  loss:5.379129  pct:-3.668698950\n",
      "[Iter.  390]  loss:5.188978  pct:-3.534981536\n",
      "[Iter.  400]  loss:5.012430  pct:-3.402356628\n",
      "[Iter.  410]  loss:4.848493  pct:-3.270620486\n",
      "[Iter.  420]  loss:4.696136  pct:-3.142340509\n",
      "[Iter.  430]  loss:4.554675  pct:-3.012302772\n",
      "[Iter.  440]  loss:4.423402  pct:-2.882155228\n",
      "[Iter.  450]  loss:4.301483  pct:-2.756219826\n",
      "[Iter.  460]  loss:4.188260  pct:-2.632197980\n",
      "[Iter.  470]  loss:4.083097  pct:-2.510879308\n",
      "[Iter.  480]  loss:3.985404  pct:-2.392619022\n",
      "[Iter.  490]  loss:3.894684  pct:-2.276316377\n",
      "[Iter.  500]  loss:3.810420  pct:-2.163577717\n",
      "[Iter.  510]  loss:3.732146  pct:-2.054197275\n",
      "[Iter.  520]  loss:3.659451  pct:-1.947794635\n",
      "[Iter.  530]  loss:3.591930  pct:-1.845115192\n",
      "[Iter.  540]  loss:3.529224  pct:-1.745752987\n",
      "[Iter.  550]  loss:3.470972  pct:-1.650563795\n",
      "[Iter.  560]  loss:3.416879  pct:-1.558436170\n",
      "[Iter.  570]  loss:3.366634  pct:-1.470502246\n",
      "[Iter.  580]  loss:3.319970  pct:-1.386065804\n",
      "[Iter.  590]  loss:3.276626  pct:-1.305547361\n",
      "[Iter.  600]  loss:3.236370  pct:-1.228581447\n",
      "[Iter.  610]  loss:3.198988  pct:-1.155063299\n",
      "[Iter.  620]  loss:3.164261  pct:-1.085565010\n",
      "[Iter.  630]  loss:3.132017  pct:-1.019004627\n",
      "[Iter.  640]  loss:3.102073  pct:-0.956066974\n",
      "[Iter.  650]  loss:3.074259  pct:-0.896623382\n",
      "[Iter.  660]  loss:3.048427  pct:-0.840265489\n",
      "[Iter.  670]  loss:3.024431  pct:-0.787155982\n",
      "[Iter.  680]  loss:3.002146  pct:-0.736840130\n",
      "[Iter.  690]  loss:2.981448  pct:-0.689442502\n",
      "[Iter.  700]  loss:2.962224  pct:-0.644776983\n",
      "[Iter.  710]  loss:2.944371  pct:-0.602713887\n",
      "[Iter.  720]  loss:2.927789  pct:-0.563152527\n",
      "[Iter.  730]  loss:2.912388  pct:-0.526032574\n",
      "[Iter.  740]  loss:2.898078  pct:-0.491345339\n",
      "[Iter.  750]  loss:2.884784  pct:-0.458733597\n",
      "[Iter.  760]  loss:2.872436  pct:-0.428036821\n",
      "[Iter.  770]  loss:2.860966  pct:-0.399315431\n",
      "[Iter.  780]  loss:2.850304  pct:-0.372649120\n",
      "[Iter.  790]  loss:2.840402  pct:-0.347401511\n",
      "[Iter.  800]  loss:2.831199  pct:-0.324018671\n",
      "[Iter.  810]  loss:2.822649  pct:-0.301972840\n",
      "[Iter.  820]  loss:2.814703  pct:-0.281542860\n",
      "[Iter.  830]  loss:2.807315  pct:-0.262466036\n",
      "[Iter.  840]  loss:2.800451  pct:-0.244489641\n",
      "[Iter.  850]  loss:2.794070  pct:-0.227882919\n",
      "[Iter.  860]  loss:2.788136  pct:-0.212352796\n",
      "[Iter.  870]  loss:2.782618  pct:-0.197925637\n",
      "[Iter.  880]  loss:2.777491  pct:-0.184257843\n",
      "[Iter.  890]  loss:2.772720  pct:-0.171773548\n",
      "[Iter.  900]  loss:2.768284  pct:-0.159987907\n",
      "[Iter.  910]  loss:2.764158  pct:-0.149039409\n",
      "[Iter.  920]  loss:2.760318  pct:-0.138920060\n",
      "[Iter.  930]  loss:2.756746  pct:-0.129387649\n",
      "[Iter.  940]  loss:2.753421  pct:-0.120612677\n",
      "[Iter.  950]  loss:2.750327  pct:-0.112385080\n",
      "[Iter.  960]  loss:2.747447  pct:-0.104692326\n",
      "[Iter.  970]  loss:2.744769  pct:-0.097495320\n",
      "[Iter.  980]  loss:2.742273  pct:-0.090945455\n",
      "[Iter.  990]  loss:2.739948  pct:-0.084785808\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.706203  pct:100.000000000\n",
      "[Iter.    2]  loss:2.706205  pct:0.000088101\n",
      "[Iter.    4]  loss:2.706205  pct:0.000008810\n",
      "[Iter.    6]  loss:2.706205  pct:-0.000008810\n",
      "[Iter.    8]  loss:2.706205  pct:-0.000008810\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.706205\n",
      "Best loss: 2.706205 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▌        | 1555/10000 [00:26<02:26, 57.62it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:289.698120  pct:100.000000000\n",
      "[Iter.   10]  loss:64.184647  pct:-77.844299928\n",
      "[Iter.   20]  loss:52.092850  pct:-18.839079927\n",
      "[Iter.   30]  loss:44.772373  pct:-14.052747296\n",
      "[Iter.   40]  loss:39.247440  pct:-12.340049156\n",
      "[Iter.   50]  loss:34.864037  pct:-11.168636070\n",
      "[Iter.   60]  loss:31.269388  pct:-10.310476686\n",
      "[Iter.   70]  loss:28.251930  pct:-9.649878478\n",
      "[Iter.   80]  loss:25.674778  pct:-9.122039558\n",
      "[Iter.   90]  loss:23.444288  pct:-8.687474268\n",
      "[Iter.  100]  loss:21.493580  pct:-8.320612544\n",
      "[Iter.  110]  loss:19.773197  pct:-8.004170088\n",
      "[Iter.  120]  loss:18.245539  pct:-7.725905169\n",
      "[Iter.  130]  loss:16.881382  pct:-7.476659059\n",
      "[Iter.  140]  loss:15.657474  pct:-7.250048753\n",
      "[Iter.  150]  loss:14.555029  pct:-7.041012359\n",
      "[Iter.  160]  loss:13.558705  pct:-6.845218868\n",
      "[Iter.  170]  loss:12.655718  pct:-6.659835568\n",
      "[Iter.  180]  loss:11.835307  pct:-6.482530175\n",
      "[Iter.  190]  loss:11.088384  pct:-6.310976462\n",
      "[Iter.  200]  loss:10.407135  pct:-6.143804948\n",
      "[Iter.  210]  loss:9.784830  pct:-5.979598764\n",
      "[Iter.  220]  loss:9.215590  pct:-5.817572824\n",
      "[Iter.  230]  loss:8.694295  pct:-5.656670061\n",
      "[Iter.  240]  loss:8.216440  pct:-5.496187242\n",
      "[Iter.  250]  loss:7.777989  pct:-5.336262452\n",
      "[Iter.  260]  loss:7.375412  pct:-5.175860457\n",
      "[Iter.  270]  loss:7.005528  pct:-5.015083700\n",
      "[Iter.  280]  loss:6.665489  pct:-4.853876942\n",
      "[Iter.  290]  loss:6.352722  pct:-4.692334530\n",
      "[Iter.  300]  loss:6.064946  pct:-4.529956301\n",
      "[Iter.  310]  loss:5.800056  pct:-4.367552646\n",
      "[Iter.  320]  loss:5.556155  pct:-4.205153080\n",
      "[Iter.  330]  loss:5.331529  pct:-4.042832787\n",
      "[Iter.  340]  loss:5.124612  pct:-3.881003060\n",
      "[Iter.  350]  loss:4.933965  pct:-3.720234620\n",
      "[Iter.  360]  loss:4.758258  pct:-3.561169831\n",
      "[Iter.  370]  loss:4.596320  pct:-3.403308417\n",
      "[Iter.  380]  loss:4.447061  pct:-3.247350579\n",
      "[Iter.  390]  loss:4.309468  pct:-3.094026986\n",
      "[Iter.  400]  loss:4.182609  pct:-2.943732131\n",
      "[Iter.  410]  loss:4.065663  pct:-2.796000167\n",
      "[Iter.  420]  loss:3.957848  pct:-2.651843769\n",
      "[Iter.  430]  loss:3.858447  pct:-2.511479079\n",
      "[Iter.  440]  loss:3.766795  pct:-2.375357471\n",
      "[Iter.  450]  loss:3.682290  pct:-2.243440047\n",
      "[Iter.  460]  loss:3.604364  pct:-2.116229046\n",
      "[Iter.  470]  loss:3.532521  pct:-1.993213562\n",
      "[Iter.  480]  loss:3.466279  pct:-1.875217494\n",
      "[Iter.  490]  loss:3.405200  pct:-1.762071480\n",
      "[Iter.  500]  loss:3.348881  pct:-1.653918383\n",
      "[Iter.  510]  loss:3.296965  pct:-1.550266954\n",
      "[Iter.  520]  loss:3.249082  pct:-1.452322426\n",
      "[Iter.  530]  loss:3.204921  pct:-1.359171707\n",
      "[Iter.  540]  loss:3.164206  pct:-1.270419043\n",
      "[Iter.  550]  loss:3.126657  pct:-1.186665702\n",
      "[Iter.  560]  loss:3.092047  pct:-1.106941739\n",
      "[Iter.  570]  loss:3.060137  pct:-1.032008367\n",
      "[Iter.  580]  loss:3.030717  pct:-0.961383913\n",
      "[Iter.  590]  loss:3.003588  pct:-0.895132597\n",
      "[Iter.  600]  loss:2.978571  pct:-0.832904626\n",
      "[Iter.  610]  loss:2.955511  pct:-0.774199570\n",
      "[Iter.  620]  loss:2.934247  pct:-0.719455974\n",
      "[Iter.  630]  loss:2.914637  pct:-0.668312055\n",
      "[Iter.  640]  loss:2.896546  pct:-0.620693463\n",
      "[Iter.  650]  loss:2.879864  pct:-0.575940596\n",
      "[Iter.  660]  loss:2.864472  pct:-0.534463667\n",
      "[Iter.  670]  loss:2.850280  pct:-0.495461014\n",
      "[Iter.  680]  loss:2.837186  pct:-0.459383318\n",
      "[Iter.  690]  loss:2.825116  pct:-0.425419417\n",
      "[Iter.  700]  loss:2.813980  pct:-0.394171990\n",
      "[Iter.  710]  loss:2.803713  pct:-0.364874479\n",
      "[Iter.  720]  loss:2.794244  pct:-0.337723383\n",
      "[Iter.  730]  loss:2.785517  pct:-0.312340309\n",
      "[Iter.  740]  loss:2.777466  pct:-0.289027855\n",
      "[Iter.  750]  loss:2.770018  pct:-0.268130874\n",
      "[Iter.  760]  loss:2.763143  pct:-0.248203287\n",
      "[Iter.  770]  loss:2.756795  pct:-0.229743193\n",
      "[Iter.  780]  loss:2.750935  pct:-0.212560310\n",
      "[Iter.  790]  loss:2.745526  pct:-0.196615468\n",
      "[Iter.  800]  loss:2.740533  pct:-0.181884148\n",
      "[Iter.  810]  loss:2.735924  pct:-0.168182926\n",
      "[Iter.  820]  loss:2.731658  pct:-0.155891417\n",
      "[Iter.  830]  loss:2.727718  pct:-0.144247310\n",
      "[Iter.  840]  loss:2.724077  pct:-0.133477506\n",
      "[Iter.  850]  loss:2.720712  pct:-0.123547036\n",
      "[Iter.  860]  loss:2.717602  pct:-0.114279535\n",
      "[Iter.  870]  loss:2.714725  pct:-0.105882808\n",
      "[Iter.  880]  loss:2.712067  pct:-0.097915211\n",
      "[Iter.  890]  loss:2.709606  pct:-0.090732207\n",
      "[Iter.  900]  loss:2.707329  pct:-0.084030567\n",
      "[Iter.  910]  loss:2.705224  pct:-0.077769427\n",
      "[Iter.  920]  loss:2.703274  pct:-0.072066079\n",
      "[Iter.  930]  loss:2.701469  pct:-0.066782180\n",
      "[Iter.  940]  loss:2.699798  pct:-0.061866869\n",
      "[Iter.  950]  loss:2.698249  pct:-0.057366044\n",
      "[Iter.  960]  loss:2.696814  pct:-0.053184176\n",
      "[Iter.  970]  loss:2.695485  pct:-0.049269502\n",
      "[Iter.  980]  loss:2.694255  pct:-0.045631914\n",
      "[Iter.  990]  loss:2.693116  pct:-0.042272372\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.677018  pct:100.000000000\n",
      "[Iter.    2]  loss:2.677018  pct:0.000008906\n",
      "[Iter.    4]  loss:2.677017  pct:-0.000017812\n",
      "[Iter.    6]  loss:2.677017  pct:-0.000017812\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.677017\n",
      "Best loss: 2.677017 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 21%|██        | 2121/10000 [00:24<01:29, 87.85it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:409.027679  pct:100.000000000\n",
      "[Iter.   10]  loss:72.612846  pct:-82.247447385\n",
      "[Iter.   20]  loss:62.444656  pct:-14.003293508\n",
      "[Iter.   30]  loss:54.843967  pct:-12.171880471\n",
      "[Iter.   40]  loss:48.814610  pct:-10.993657446\n",
      "[Iter.   50]  loss:43.811584  pct:-10.249032213\n",
      "[Iter.   60]  loss:39.621227  pct:-9.564495917\n",
      "[Iter.   70]  loss:36.062302  pct:-8.982371003\n",
      "[Iter.   80]  loss:32.995823  pct:-8.503280684\n",
      "[Iter.   90]  loss:30.317911  pct:-8.115911417\n",
      "[Iter.  100]  loss:27.954256  pct:-7.796233318\n",
      "[Iter.  110]  loss:25.849813  pct:-7.528165271\n",
      "[Iter.  120]  loss:23.964430  pct:-7.293606233\n",
      "[Iter.  130]  loss:22.266926  pct:-7.083431794\n",
      "[Iter.  140]  loss:20.731195  pct:-6.896912375\n",
      "[Iter.  150]  loss:19.336639  pct:-6.726848188\n",
      "[Iter.  160]  loss:18.066816  pct:-6.566927416\n",
      "[Iter.  170]  loss:16.907257  pct:-6.418171463\n",
      "[Iter.  180]  loss:15.845776  pct:-6.278259512\n",
      "[Iter.  190]  loss:14.872076  pct:-6.144852698\n",
      "[Iter.  200]  loss:13.977281  pct:-6.016614061\n",
      "[Iter.  210]  loss:13.153658  pct:-5.892581870\n",
      "[Iter.  220]  loss:12.394396  pct:-5.772250502\n",
      "[Iter.  230]  loss:11.693862  pct:-5.652021096\n",
      "[Iter.  240]  loss:11.046778  pct:-5.533537494\n",
      "[Iter.  250]  loss:10.448445  pct:-5.416352352\n",
      "[Iter.  260]  loss:9.894725  pct:-5.299548950\n",
      "[Iter.  270]  loss:9.381927  pct:-5.182542388\n",
      "[Iter.  280]  loss:8.906718  pct:-5.065146062\n",
      "[Iter.  290]  loss:8.466091  pct:-4.947131879\n",
      "[Iter.  300]  loss:8.057323  pct:-4.828293160\n",
      "[Iter.  310]  loss:7.677958  pct:-4.708324910\n",
      "[Iter.  320]  loss:7.325742  pct:-4.587374640\n",
      "[Iter.  330]  loss:6.998609  pct:-4.465529759\n",
      "[Iter.  340]  loss:6.694683  pct:-4.342663076\n",
      "[Iter.  350]  loss:6.412269  pct:-4.218481688\n",
      "[Iter.  360]  loss:6.149767  pct:-4.093741722\n",
      "[Iter.  370]  loss:5.905718  pct:-3.968427997\n",
      "[Iter.  380]  loss:5.678818  pct:-3.842032919\n",
      "[Iter.  390]  loss:5.467802  pct:-3.715846673\n",
      "[Iter.  400]  loss:5.271555  pct:-3.589140556\n",
      "[Iter.  410]  loss:5.088996  pct:-3.463087047\n",
      "[Iter.  420]  loss:4.919185  pct:-3.336831766\n",
      "[Iter.  430]  loss:4.761210  pct:-3.211400157\n",
      "[Iter.  440]  loss:4.614241  pct:-3.086805765\n",
      "[Iter.  450]  loss:4.477489  pct:-2.963697062\n",
      "[Iter.  460]  loss:4.350266  pct:-2.841402666\n",
      "[Iter.  470]  loss:4.231877  pct:-2.721412124\n",
      "[Iter.  480]  loss:4.121728  pct:-2.602838188\n",
      "[Iter.  490]  loss:4.019232  pct:-2.486716050\n",
      "[Iter.  500]  loss:3.923865  pct:-2.372783248\n",
      "[Iter.  510]  loss:3.835118  pct:-2.261724945\n",
      "[Iter.  520]  loss:3.752536  pct:-2.153292043\n",
      "[Iter.  530]  loss:3.675688  pct:-2.047895717\n",
      "[Iter.  540]  loss:3.604180  pct:-1.945436174\n",
      "[Iter.  550]  loss:3.537640  pct:-1.846202960\n",
      "[Iter.  560]  loss:3.475726  pct:-1.750142338\n",
      "[Iter.  570]  loss:3.418117  pct:-1.657475921\n",
      "[Iter.  580]  loss:3.364498  pct:-1.568668208\n",
      "[Iter.  590]  loss:3.314613  pct:-1.482681849\n",
      "[Iter.  600]  loss:3.268194  pct:-1.400424988\n",
      "[Iter.  610]  loss:3.224997  pct:-1.321736530\n",
      "[Iter.  620]  loss:3.184810  pct:-1.246120705\n",
      "[Iter.  630]  loss:3.147404  pct:-1.174519266\n",
      "[Iter.  640]  loss:3.112586  pct:-1.106227873\n",
      "[Iter.  650]  loss:3.080204  pct:-1.040372396\n",
      "[Iter.  660]  loss:3.050066  pct:-0.978426791\n",
      "[Iter.  670]  loss:3.022019  pct:-0.919556582\n",
      "[Iter.  680]  loss:2.995920  pct:-0.863634667\n",
      "[Iter.  690]  loss:2.971622  pct:-0.811050694\n",
      "[Iter.  700]  loss:2.949013  pct:-0.760830252\n",
      "[Iter.  710]  loss:2.927972  pct:-0.713474069\n",
      "[Iter.  720]  loss:2.908394  pct:-0.668645093\n",
      "[Iter.  730]  loss:2.890166  pct:-0.626739429\n",
      "[Iter.  740]  loss:2.873199  pct:-0.587078160\n",
      "[Iter.  750]  loss:2.857404  pct:-0.549710569\n",
      "[Iter.  760]  loss:2.842703  pct:-0.514492371\n",
      "[Iter.  770]  loss:2.829017  pct:-0.481457855\n",
      "[Iter.  780]  loss:2.816272  pct:-0.450489589\n",
      "[Iter.  790]  loss:2.804405  pct:-0.421399620\n",
      "[Iter.  800]  loss:2.793356  pct:-0.393962997\n",
      "[Iter.  810]  loss:2.783066  pct:-0.368404872\n",
      "[Iter.  820]  loss:2.773485  pct:-0.344255297\n",
      "[Iter.  830]  loss:2.764558  pct:-0.321864707\n",
      "[Iter.  840]  loss:2.756247  pct:-0.300619315\n",
      "[Iter.  850]  loss:2.748502  pct:-0.280981733\n",
      "[Iter.  860]  loss:2.741287  pct:-0.262533446\n",
      "[Iter.  870]  loss:2.734562  pct:-0.245325406\n",
      "[Iter.  880]  loss:2.728298  pct:-0.229066741\n",
      "[Iter.  890]  loss:2.722455  pct:-0.214151325\n",
      "[Iter.  900]  loss:2.717011  pct:-0.199950888\n",
      "[Iter.  910]  loss:2.711940  pct:-0.186671220\n",
      "[Iter.  920]  loss:2.707203  pct:-0.174641984\n",
      "[Iter.  930]  loss:2.702795  pct:-0.162846943\n",
      "[Iter.  940]  loss:2.698680  pct:-0.152253685\n",
      "[Iter.  950]  loss:2.694843  pct:-0.142175828\n",
      "[Iter.  960]  loss:2.691265  pct:-0.132752484\n",
      "[Iter.  970]  loss:2.687927  pct:-0.124052256\n",
      "[Iter.  980]  loss:2.684812  pct:-0.115895165\n",
      "[Iter.  990]  loss:2.681914  pct:-0.107939709\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.637794  pct:100.000000000\n",
      "[Iter.    2]  loss:2.637760  pct:-0.001256360\n",
      "[Iter.    4]  loss:2.637749  pct:-0.000424818\n",
      "[Iter.    6]  loss:2.637744  pct:-0.000189813\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.637744\n",
      "Best loss: 2.637744 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 14%|█▎        | 1356/10000 [00:13<01:27, 98.43it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:302.792816  pct:100.000000000\n",
      "[Iter.   10]  loss:68.425354  pct:-77.401922915\n",
      "[Iter.   20]  loss:55.475208  pct:-18.925946252\n",
      "[Iter.   30]  loss:47.535881  pct:-14.311487033\n",
      "[Iter.   40]  loss:41.601170  pct:-12.484698561\n",
      "[Iter.   50]  loss:36.910233  pct:-11.275973942\n",
      "[Iter.   60]  loss:33.073593  pct:-10.394514312\n",
      "[Iter.   70]  loss:29.858824  pct:-9.720048710\n",
      "[Iter.   80]  loss:27.116896  pct:-9.182974256\n",
      "[Iter.   90]  loss:24.746386  pct:-8.741819601\n",
      "[Iter.  100]  loss:22.674963  pct:-8.370606571\n",
      "[Iter.  110]  loss:20.849283  pct:-8.051522639\n",
      "[Iter.  120]  loss:19.228872  pct:-7.772022195\n",
      "[Iter.  130]  loss:17.782276  pct:-7.523042033\n",
      "[Iter.  140]  loss:16.484610  pct:-7.297527822\n",
      "[Iter.  150]  loss:15.315792  pct:-7.090356085\n",
      "[Iter.  160]  loss:14.259402  pct:-6.897389328\n",
      "[Iter.  170]  loss:13.301805  pct:-6.715546419\n",
      "[Iter.  180]  loss:12.431604  pct:-6.541977411\n",
      "[Iter.  190]  loss:11.639118  pct:-6.374770032\n",
      "[Iter.  200]  loss:10.916032  pct:-6.212552747\n",
      "[Iter.  210]  loss:10.255214  pct:-6.053647606\n",
      "[Iter.  220]  loss:9.650476  pct:-5.896885730\n",
      "[Iter.  230]  loss:9.096368  pct:-5.741765428\n",
      "[Iter.  240]  loss:8.588104  pct:-5.587544360\n",
      "[Iter.  250]  loss:8.121474  pct:-5.433445712\n",
      "[Iter.  260]  loss:7.692745  pct:-5.278956052\n",
      "[Iter.  270]  loss:7.298572  pct:-5.123959453\n",
      "[Iter.  280]  loss:6.935953  pct:-4.968354358\n",
      "[Iter.  290]  loss:6.602196  pct:-4.811983543\n",
      "[Iter.  300]  loss:6.294861  pct:-4.655040381\n",
      "[Iter.  310]  loss:6.011725  pct:-4.497889258\n",
      "[Iter.  320]  loss:5.750823  pct:-4.339884301\n",
      "[Iter.  330]  loss:5.510344  pct:-4.181652756\n",
      "[Iter.  340]  loss:5.288614  pct:-4.023882252\n",
      "[Iter.  350]  loss:5.084152  pct:-3.866088878\n",
      "[Iter.  360]  loss:4.895556  pct:-3.709474155\n",
      "[Iter.  370]  loss:4.721604  pct:-3.553265179\n",
      "[Iter.  380]  loss:4.561142  pct:-3.398471529\n",
      "[Iter.  390]  loss:4.413084  pct:-3.246082133\n",
      "[Iter.  400]  loss:4.276471  pct:-3.095633484\n",
      "[Iter.  410]  loss:4.150438  pct:-2.947122505\n",
      "[Iter.  420]  loss:4.034119  pct:-2.802564631\n",
      "[Iter.  430]  loss:3.926786  pct:-2.660634945\n",
      "[Iter.  440]  loss:3.827738  pct:-2.522359563\n",
      "[Iter.  450]  loss:3.736338  pct:-2.387836881\n",
      "[Iter.  460]  loss:3.652007  pct:-2.257050419\n",
      "[Iter.  470]  loss:3.574170  pct:-2.131341746\n",
      "[Iter.  480]  loss:3.502339  pct:-2.009731470\n",
      "[Iter.  490]  loss:3.436059  pct:-1.892446328\n",
      "[Iter.  500]  loss:3.374903  pct:-1.779823683\n",
      "[Iter.  510]  loss:3.318416  pct:-1.673746312\n",
      "[Iter.  520]  loss:3.266303  pct:-1.570433941\n",
      "[Iter.  530]  loss:3.218218  pct:-1.472138487\n",
      "[Iter.  540]  loss:3.173845  pct:-1.378800187\n",
      "[Iter.  550]  loss:3.132890  pct:-1.290385148\n",
      "[Iter.  560]  loss:3.095087  pct:-1.206654797\n",
      "[Iter.  570]  loss:3.060208  pct:-1.126929316\n",
      "[Iter.  580]  loss:3.028024  pct:-1.051673951\n",
      "[Iter.  590]  loss:2.998332  pct:-0.980586916\n",
      "[Iter.  600]  loss:2.970919  pct:-0.914271374\n",
      "[Iter.  610]  loss:2.945626  pct:-0.851340405\n",
      "[Iter.  620]  loss:2.922279  pct:-0.792619710\n",
      "[Iter.  630]  loss:2.900733  pct:-0.737281109\n",
      "[Iter.  640]  loss:2.880855  pct:-0.685288312\n",
      "[Iter.  650]  loss:2.862521  pct:-0.636421764\n",
      "[Iter.  660]  loss:2.845603  pct:-0.591015603\n",
      "[Iter.  670]  loss:2.829994  pct:-0.548506312\n",
      "[Iter.  680]  loss:2.815579  pct:-0.509382716\n",
      "[Iter.  690]  loss:2.802272  pct:-0.472632211\n",
      "[Iter.  700]  loss:2.789987  pct:-0.438385619\n",
      "[Iter.  710]  loss:2.778653  pct:-0.406219319\n",
      "[Iter.  720]  loss:2.768195  pct:-0.376386254\n",
      "[Iter.  730]  loss:2.758556  pct:-0.348197530\n",
      "[Iter.  740]  loss:2.749653  pct:-0.322733624\n",
      "[Iter.  750]  loss:2.741415  pct:-0.299630223\n",
      "[Iter.  760]  loss:2.733813  pct:-0.277292538\n",
      "[Iter.  770]  loss:2.726798  pct:-0.256609679\n",
      "[Iter.  780]  loss:2.720319  pct:-0.237579411\n",
      "[Iter.  790]  loss:2.714331  pct:-0.220134435\n",
      "[Iter.  800]  loss:2.708808  pct:-0.203483104\n",
      "[Iter.  810]  loss:2.703706  pct:-0.188336763\n",
      "[Iter.  820]  loss:2.698972  pct:-0.175103272\n",
      "[Iter.  830]  loss:2.694597  pct:-0.162098063\n",
      "[Iter.  840]  loss:2.690554  pct:-0.150035950\n",
      "[Iter.  850]  loss:2.686815  pct:-0.138963213\n",
      "[Iter.  860]  loss:2.683359  pct:-0.128614693\n",
      "[Iter.  870]  loss:2.680168  pct:-0.118935656\n",
      "[Iter.  880]  loss:2.677220  pct:-0.109994815\n",
      "[Iter.  890]  loss:2.674499  pct:-0.101629039\n",
      "[Iter.  900]  loss:2.671990  pct:-0.093825255\n",
      "[Iter.  910]  loss:2.669662  pct:-0.087105208\n",
      "[Iter.  920]  loss:2.667506  pct:-0.080768930\n",
      "[Iter.  930]  loss:2.665518  pct:-0.074515135\n",
      "[Iter.  940]  loss:2.663674  pct:-0.069177139\n",
      "[Iter.  950]  loss:2.661955  pct:-0.064552740\n",
      "[Iter.  960]  loss:2.660360  pct:-0.059910177\n",
      "[Iter.  970]  loss:2.658884  pct:-0.055492031\n",
      "[Iter.  980]  loss:2.657515  pct:-0.051478784\n",
      "[Iter.  990]  loss:2.656247  pct:-0.047719332\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.638178  pct:100.000000000\n",
      "[Iter.    2]  loss:2.638178  pct:-0.000018074\n",
      "[Iter.    4]  loss:2.638177  pct:-0.000009037\n",
      "[Iter.    6]  loss:2.638178  pct:0.000009037\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.638178\n",
      "Best loss: 2.638178 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 12%|█▏        | 1241/10000 [00:12<01:30, 96.39it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:431.125854  pct:100.000000000\n",
      "[Iter.   10]  loss:89.906212  pct:-79.146177724\n",
      "[Iter.   20]  loss:76.653923  pct:-14.740125899\n",
      "[Iter.   30]  loss:66.869965  pct:-12.763806532\n",
      "[Iter.   40]  loss:59.705505  pct:-10.714016781\n",
      "[Iter.   50]  loss:53.898064  pct:-9.726811079\n",
      "[Iter.   60]  loss:49.055653  pct:-8.984387773\n",
      "[Iter.   70]  loss:44.921352  pct:-8.427775417\n",
      "[Iter.   80]  loss:41.336746  pct:-7.979737876\n",
      "[Iter.   90]  loss:38.191349  pct:-7.609203612\n",
      "[Iter.  100]  loss:35.404346  pct:-7.297470852\n",
      "[Iter.  110]  loss:32.915188  pct:-7.030658320\n",
      "[Iter.  120]  loss:30.677212  pct:-6.799220121\n",
      "[Iter.  130]  loss:28.653818  pct:-6.595754682\n",
      "[Iter.  140]  loss:26.815733  pct:-6.414800171\n",
      "[Iter.  150]  loss:25.139097  pct:-6.252433021\n",
      "[Iter.  160]  loss:23.604368  pct:-6.104948761\n",
      "[Iter.  170]  loss:22.195292  pct:-5.969559016\n",
      "[Iter.  180]  loss:20.898043  pct:-5.844702870\n",
      "[Iter.  190]  loss:19.700943  pct:-5.728286156\n",
      "[Iter.  200]  loss:18.594019  pct:-5.618634892\n",
      "[Iter.  210]  loss:17.568619  pct:-5.514677409\n",
      "[Iter.  220]  loss:16.617159  pct:-5.415678357\n",
      "[Iter.  230]  loss:15.733056  pct:-5.320421061\n",
      "[Iter.  240]  loss:14.910492  pct:-5.228253948\n",
      "[Iter.  250]  loss:14.144313  pct:-5.138523180\n",
      "[Iter.  260]  loss:13.429928  pct:-5.050687437\n",
      "[Iter.  270]  loss:12.763204  pct:-4.964466032\n",
      "[Iter.  280]  loss:12.140490  pct:-4.878979143\n",
      "[Iter.  290]  loss:11.558420  pct:-4.794447483\n",
      "[Iter.  300]  loss:11.013990  pct:-4.710243879\n",
      "[Iter.  310]  loss:10.504510  pct:-4.625757403\n",
      "[Iter.  320]  loss:10.027456  pct:-4.541417407\n",
      "[Iter.  330]  loss:9.580530  pct:-4.457023838\n",
      "[Iter.  340]  loss:9.161683  pct:-4.371857056\n",
      "[Iter.  350]  loss:8.768985  pct:-4.286311635\n",
      "[Iter.  360]  loss:8.400722  pct:-4.199610938\n",
      "[Iter.  370]  loss:8.055224  pct:-4.112707811\n",
      "[Iter.  380]  loss:7.731001  pct:-4.025009131\n",
      "[Iter.  390]  loss:7.426677  pct:-3.936413331\n",
      "[Iter.  400]  loss:7.140977  pct:-3.846934196\n",
      "[Iter.  410]  loss:6.872721  pct:-3.756575229\n",
      "[Iter.  420]  loss:6.620800  pct:-3.665529949\n",
      "[Iter.  430]  loss:6.384171  pct:-3.574024674\n",
      "[Iter.  440]  loss:6.161880  pct:-3.481901287\n",
      "[Iter.  450]  loss:5.953037  pct:-3.389270057\n",
      "[Iter.  460]  loss:5.756801  pct:-3.296411593\n",
      "[Iter.  470]  loss:5.572386  pct:-3.203426256\n",
      "[Iter.  480]  loss:5.399091  pct:-3.109889151\n",
      "[Iter.  490]  loss:5.236230  pct:-3.016449943\n",
      "[Iter.  500]  loss:5.083158  pct:-2.923313272\n",
      "[Iter.  510]  loss:4.939292  pct:-2.830258769\n",
      "[Iter.  520]  loss:4.804084  pct:-2.737389373\n",
      "[Iter.  530]  loss:4.676979  pct:-2.645784398\n",
      "[Iter.  540]  loss:4.557521  pct:-2.554164392\n",
      "[Iter.  550]  loss:4.445246  pct:-2.463502638\n",
      "[Iter.  560]  loss:4.339708  pct:-2.374184983\n",
      "[Iter.  570]  loss:4.240514  pct:-2.285731050\n",
      "[Iter.  580]  loss:4.147244  pct:-2.199493489\n",
      "[Iter.  590]  loss:4.059572  pct:-2.113987845\n",
      "[Iter.  600]  loss:3.977154  pct:-2.030213287\n",
      "[Iter.  610]  loss:3.899684  pct:-1.947865022\n",
      "[Iter.  620]  loss:3.826856  pct:-1.867531124\n",
      "[Iter.  630]  loss:3.758406  pct:-1.788673714\n",
      "[Iter.  640]  loss:3.694070  pct:-1.711797316\n",
      "[Iter.  650]  loss:3.633592  pct:-1.637170081\n",
      "[Iter.  660]  loss:3.576752  pct:-1.564290780\n",
      "[Iter.  670]  loss:3.523330  pct:-1.493575683\n",
      "[Iter.  680]  loss:3.473117  pct:-1.425160065\n",
      "[Iter.  690]  loss:3.425921  pct:-1.358891930\n",
      "[Iter.  700]  loss:3.381562  pct:-1.294804026\n",
      "[Iter.  710]  loss:3.339872  pct:-1.232886477\n",
      "[Iter.  720]  loss:3.300673  pct:-1.173649851\n",
      "[Iter.  730]  loss:3.263845  pct:-1.115773611\n",
      "[Iter.  740]  loss:3.229232  pct:-1.060494352\n",
      "[Iter.  750]  loss:3.196706  pct:-1.007258951\n",
      "[Iter.  760]  loss:3.166132  pct:-0.956394916\n",
      "[Iter.  770]  loss:3.137418  pct:-0.906924106\n",
      "[Iter.  780]  loss:3.110429  pct:-0.860221509\n",
      "[Iter.  790]  loss:3.085063  pct:-0.815539619\n",
      "[Iter.  800]  loss:3.061222  pct:-0.772761944\n",
      "[Iter.  810]  loss:3.038812  pct:-0.732065562\n",
      "[Iter.  820]  loss:3.017746  pct:-0.693229765\n",
      "[Iter.  830]  loss:2.997962  pct:-0.655579823\n",
      "[Iter.  840]  loss:2.979373  pct:-0.620078977\n",
      "[Iter.  850]  loss:2.961905  pct:-0.586281083\n",
      "[Iter.  860]  loss:2.945489  pct:-0.554232262\n",
      "[Iter.  870]  loss:2.930063  pct:-0.523737607\n",
      "[Iter.  880]  loss:2.915557  pct:-0.495070049\n",
      "[Iter.  890]  loss:2.901934  pct:-0.467235687\n",
      "[Iter.  900]  loss:2.889132  pct:-0.441166503\n",
      "[Iter.  910]  loss:2.877100  pct:-0.416458366\n",
      "[Iter.  920]  loss:2.865793  pct:-0.393008222\n",
      "[Iter.  930]  loss:2.855157  pct:-0.371131262\n",
      "[Iter.  940]  loss:2.845153  pct:-0.350368372\n",
      "[Iter.  950]  loss:2.835747  pct:-0.330617212\n",
      "[Iter.  960]  loss:2.826899  pct:-0.311998066\n",
      "[Iter.  970]  loss:2.818577  pct:-0.294394602\n",
      "[Iter.  980]  loss:2.810754  pct:-0.277567975\n",
      "[Iter.  990]  loss:2.803400  pct:-0.261630286\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.683829  pct:100.000000000\n",
      "[Iter.    2]  loss:2.683824  pct:-0.000213205\n",
      "[Iter.    4]  loss:2.683823  pct:-0.000017767\n",
      "[Iter.    6]  loss:2.683822  pct:-0.000026651\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.683822\n",
      "Best loss: 2.683822 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  4%|▎         | 350/10000 [00:05<02:24, 66.66it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:917.518921  pct:100.000000000\n",
      "[Iter.   10]  loss:157.660233  pct:-82.816679450\n",
      "[Iter.   20]  loss:141.102982  pct:-10.501856245\n",
      "[Iter.   30]  loss:130.027679  pct:-7.849091494\n",
      "[Iter.   40]  loss:120.638565  pct:-7.220858220\n",
      "[Iter.   50]  loss:112.512642  pct:-6.735759127\n",
      "[Iter.   60]  loss:105.379875  pct:-6.339524699\n",
      "[Iter.   70]  loss:99.046867  pct:-6.009693788\n",
      "[Iter.   80]  loss:93.370758  pct:-5.730730779\n",
      "[Iter.   90]  loss:88.243210  pct:-5.491599645\n",
      "[Iter.  100]  loss:83.580078  pct:-5.284408537\n",
      "[Iter.  110]  loss:79.314781  pct:-5.103245931\n",
      "[Iter.  120]  loss:75.393997  pct:-4.943320700\n",
      "[Iter.  130]  loss:71.774170  pct:-4.801214162\n",
      "[Iter.  140]  loss:68.419281  pct:-4.674228792\n",
      "[Iter.  150]  loss:65.299301  pct:-4.560088637\n",
      "[Iter.  160]  loss:62.388969  pct:-4.456910985\n",
      "[Iter.  170]  loss:59.666832  pct:-4.363171048\n",
      "[Iter.  180]  loss:57.114471  pct:-4.277687369\n",
      "[Iter.  190]  loss:54.716076  pct:-4.199278183\n",
      "[Iter.  200]  loss:52.457916  pct:-4.127049684\n",
      "[Iter.  210]  loss:50.327957  pct:-4.060319697\n",
      "[Iter.  220]  loss:48.315601  pct:-3.998484974\n",
      "[Iter.  230]  loss:46.411434  pct:-3.941102092\n",
      "[Iter.  240]  loss:44.607132  pct:-3.887624349\n",
      "[Iter.  250]  loss:42.895348  pct:-3.837467884\n",
      "[Iter.  260]  loss:41.269371  pct:-3.790566235\n",
      "[Iter.  270]  loss:39.723320  pct:-3.746243247\n",
      "[Iter.  280]  loss:38.251846  pct:-3.704306925\n",
      "[Iter.  290]  loss:36.849945  pct:-3.664924390\n",
      "[Iter.  300]  loss:35.513252  pct:-3.627394308\n",
      "[Iter.  310]  loss:34.237770  pct:-3.591566800\n",
      "[Iter.  320]  loss:33.019737  pct:-3.557570584\n",
      "[Iter.  330]  loss:31.855820  pct:-3.524914608\n",
      "[Iter.  340]  loss:30.742817  pct:-3.493875805\n",
      "[Iter.  350]  loss:29.677906  pct:-3.463934002\n",
      "[Iter.  360]  loss:28.658480  pct:-3.434967226\n",
      "[Iter.  370]  loss:27.682060  pct:-3.407087394\n",
      "[Iter.  380]  loss:26.746429  pct:-3.379917500\n",
      "[Iter.  390]  loss:25.849516  pct:-3.353395377\n",
      "[Iter.  400]  loss:24.989220  pct:-3.328094237\n",
      "[Iter.  410]  loss:24.163786  pct:-3.303159291\n",
      "[Iter.  420]  loss:23.371588  pct:-3.278452240\n",
      "[Iter.  430]  loss:22.610893  pct:-3.254783166\n",
      "[Iter.  440]  loss:21.880318  pct:-3.231077841\n",
      "[Iter.  450]  loss:21.178398  pct:-3.207995266\n",
      "[Iter.  460]  loss:20.503843  pct:-3.185107866\n",
      "[Iter.  470]  loss:19.855394  pct:-3.162572667\n",
      "[Iter.  480]  loss:19.231915  pct:-3.140103046\n",
      "[Iter.  490]  loss:18.632269  pct:-3.117971505\n",
      "[Iter.  500]  loss:18.055433  pct:-3.095895810\n",
      "[Iter.  510]  loss:17.500425  pct:-3.073910917\n",
      "[Iter.  520]  loss:16.966290  pct:-3.052130495\n",
      "[Iter.  530]  loss:16.452160  pct:-3.030300986\n",
      "[Iter.  540]  loss:15.957208  pct:-3.008432968\n",
      "[Iter.  550]  loss:15.480680  pct:-2.986281959\n",
      "[Iter.  560]  loss:15.021754  pct:-2.964509227\n",
      "[Iter.  570]  loss:14.579777  pct:-2.942249574\n",
      "[Iter.  580]  loss:14.154040  pct:-2.920047640\n",
      "[Iter.  590]  loss:13.743876  pct:-2.897857217\n",
      "[Iter.  600]  loss:13.348667  pct:-2.875530158\n",
      "[Iter.  610]  loss:12.967899  pct:-2.852478215\n",
      "[Iter.  620]  loss:12.600975  pct:-2.829481297\n",
      "[Iter.  630]  loss:12.247338  pct:-2.806423635\n",
      "[Iter.  640]  loss:11.906457  pct:-2.783309642\n",
      "[Iter.  650]  loss:11.577896  pct:-2.759518055\n",
      "[Iter.  660]  loss:11.261142  pct:-2.735854061\n",
      "[Iter.  670]  loss:10.955770  pct:-2.711725778\n",
      "[Iter.  680]  loss:10.661338  pct:-2.687466302\n",
      "[Iter.  690]  loss:10.377449  pct:-2.662787924\n",
      "[Iter.  700]  loss:10.103727  pct:-2.637658773\n",
      "[Iter.  710]  loss:9.839769  pct:-2.612481200\n",
      "[Iter.  720]  loss:9.585194  pct:-2.587212362\n",
      "[Iter.  730]  loss:9.339650  pct:-2.561695562\n",
      "[Iter.  740]  loss:9.102829  pct:-2.535653592\n",
      "[Iter.  750]  loss:8.874474  pct:-2.508620213\n",
      "[Iter.  760]  loss:8.654214  pct:-2.481946277\n",
      "[Iter.  770]  loss:8.441729  pct:-2.455281505\n",
      "[Iter.  780]  loss:8.236758  pct:-2.428061475\n",
      "[Iter.  790]  loss:8.039082  pct:-2.399932753\n",
      "[Iter.  800]  loss:7.848378  pct:-2.372203719\n",
      "[Iter.  810]  loss:7.664390  pct:-2.344281711\n",
      "[Iter.  820]  loss:7.486956  pct:-2.315049761\n",
      "[Iter.  830]  loss:7.315749  pct:-2.286736550\n",
      "[Iter.  840]  loss:7.150610  pct:-2.257297942\n",
      "[Iter.  850]  loss:6.991301  pct:-2.227913105\n",
      "[Iter.  860]  loss:6.837592  pct:-2.198580927\n",
      "[Iter.  870]  loss:6.689312  pct:-2.168601920\n",
      "[Iter.  880]  loss:6.546245  pct:-2.138731410\n",
      "[Iter.  890]  loss:6.408238  pct:-2.108180875\n",
      "[Iter.  900]  loss:6.275062  pct:-2.078212374\n",
      "[Iter.  910]  loss:6.146592  pct:-2.047309685\n",
      "[Iter.  920]  loss:6.022652  pct:-2.016394141\n",
      "[Iter.  930]  loss:5.903100  pct:-1.985033432\n",
      "[Iter.  940]  loss:5.787742  pct:-1.954199371\n",
      "[Iter.  950]  loss:5.676441  pct:-1.923054267\n",
      "[Iter.  960]  loss:5.569058  pct:-1.891727224\n",
      "[Iter.  970]  loss:5.465459  pct:-1.860262051\n",
      "[Iter.  980]  loss:5.365512  pct:-1.828693642\n",
      "[Iter.  990]  loss:5.269084  pct:-1.797188929\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.638051  pct:100.000000000\n",
      "[Iter.    2]  loss:2.638049  pct:-0.000072301\n",
      "[Iter.    4]  loss:2.638045  pct:-0.000144603\n",
      "[Iter.    6]  loss:2.638043  pct:-0.000090377\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.638043\n",
      "Best loss: 2.638043 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [01:38<00:00, 101.32it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:194.972214  pct:100.000000000\n",
      "[Iter.   10]  loss:39.444012  pct:-79.769417013\n",
      "[Iter.   20]  loss:31.200216  pct:-20.899992273\n",
      "[Iter.   30]  loss:25.758348  pct:-17.441763151\n",
      "[Iter.   40]  loss:21.834497  pct:-15.233317534\n",
      "[Iter.   50]  loss:18.818556  pct:-13.812736595\n",
      "[Iter.   60]  loss:16.412840  pct:-12.783743683\n",
      "[Iter.   70]  loss:14.447615  pct:-11.973706153\n",
      "[Iter.   80]  loss:12.815346  pct:-11.297843574\n",
      "[Iter.   90]  loss:11.442787  pct:-10.710273597\n",
      "[Iter.  100]  loss:10.277730  pct:-10.181585701\n",
      "[Iter.  110]  loss:9.281578  pct:-9.692334059\n",
      "[Iter.  120]  loss:8.424932  pct:-9.229535451\n",
      "[Iter.  130]  loss:7.684934  pct:-8.783428820\n",
      "[Iter.  140]  loss:7.043389  pct:-8.348079115\n",
      "[Iter.  150]  loss:6.485635  pct:-7.918836958\n",
      "[Iter.  160]  loss:5.999614  pct:-7.493800988\n",
      "[Iter.  170]  loss:5.575325  pct:-7.071941789\n",
      "[Iter.  180]  loss:5.204333  pct:-6.654171838\n",
      "[Iter.  190]  loss:4.879603  pct:-6.239615667\n",
      "[Iter.  200]  loss:4.595079  pct:-5.830873790\n",
      "[Iter.  210]  loss:4.345623  pct:-5.428782825\n",
      "[Iter.  220]  loss:4.126807  pct:-5.035304486\n",
      "[Iter.  230]  loss:3.934726  pct:-4.654463518\n",
      "[Iter.  240]  loss:3.766049  pct:-4.286882267\n",
      "[Iter.  250]  loss:3.617933  pct:-3.932930682\n",
      "[Iter.  260]  loss:3.487822  pct:-3.596279125\n",
      "[Iter.  270]  loss:3.373493  pct:-3.277943989\n",
      "[Iter.  280]  loss:3.273007  pct:-2.978700879\n",
      "[Iter.  290]  loss:3.184666  pct:-2.699092008\n",
      "[Iter.  300]  loss:3.106990  pct:-2.439057258\n",
      "[Iter.  310]  loss:3.038658  pct:-2.199282363\n",
      "[Iter.  320]  loss:2.978534  pct:-1.978633756\n",
      "[Iter.  330]  loss:2.925640  pct:-1.775851667\n",
      "[Iter.  340]  loss:2.879091  pct:-1.591065261\n",
      "[Iter.  350]  loss:2.838115  pct:-1.423253347\n",
      "[Iter.  360]  loss:2.802040  pct:-1.271086025\n",
      "[Iter.  370]  loss:2.770270  pct:-1.133808110\n",
      "[Iter.  380]  loss:2.742283  pct:-1.010262705\n",
      "[Iter.  390]  loss:2.717608  pct:-0.899793844\n",
      "[Iter.  400]  loss:2.695850  pct:-0.800624715\n",
      "[Iter.  410]  loss:2.676657  pct:-0.711943657\n",
      "[Iter.  420]  loss:2.659723  pct:-0.632660634\n",
      "[Iter.  430]  loss:2.644765  pct:-0.562394768\n",
      "[Iter.  440]  loss:2.631551  pct:-0.499614710\n",
      "[Iter.  450]  loss:2.619874  pct:-0.443722624\n",
      "[Iter.  460]  loss:2.609554  pct:-0.393919125\n",
      "[Iter.  470]  loss:2.600426  pct:-0.349785990\n",
      "[Iter.  480]  loss:2.592326  pct:-0.311488635\n",
      "[Iter.  490]  loss:2.585137  pct:-0.277347431\n",
      "[Iter.  500]  loss:2.578758  pct:-0.246752491\n",
      "[Iter.  510]  loss:2.573090  pct:-0.219774347\n",
      "[Iter.  520]  loss:2.568051  pct:-0.195842915\n",
      "[Iter.  530]  loss:2.563568  pct:-0.174558291\n",
      "[Iter.  540]  loss:2.559577  pct:-0.155686390\n",
      "[Iter.  550]  loss:2.556023  pct:-0.138845873\n",
      "[Iter.  560]  loss:2.552856  pct:-0.123900041\n",
      "[Iter.  570]  loss:2.550033  pct:-0.110595832\n",
      "[Iter.  580]  loss:2.547497  pct:-0.099470680\n",
      "[Iter.  590]  loss:2.545226  pct:-0.089143868\n",
      "[Iter.  600]  loss:2.543192  pct:-0.079893588\n",
      "[Iter.  610]  loss:2.541369  pct:-0.071670166\n",
      "[Iter.  620]  loss:2.539736  pct:-0.064272658\n",
      "[Iter.  630]  loss:2.538271  pct:-0.057677008\n",
      "[Iter.  640]  loss:2.536942  pct:-0.052356311\n",
      "[Iter.  650]  loss:2.535741  pct:-0.047346478\n",
      "[Iter.  660]  loss:2.534657  pct:-0.042761767\n",
      "[Iter.  670]  loss:2.533677  pct:-0.038660081\n",
      "[Iter.  680]  loss:2.532789  pct:-0.035023959\n",
      "[Iter.  690]  loss:2.531986  pct:-0.031722756\n",
      "[Iter.  700]  loss:2.531257  pct:-0.028785530\n",
      "[Iter.  710]  loss:2.530595  pct:-0.026165924\n",
      "[Iter.  720]  loss:2.529993  pct:-0.023779725\n",
      "[Iter.  730]  loss:2.529446  pct:-0.021636781\n",
      "[Iter.  740]  loss:2.528947  pct:-0.019728041\n",
      "[Iter.  750]  loss:2.528491  pct:-0.017997259\n",
      "[Iter.  760]  loss:2.528076  pct:-0.016435238\n",
      "[Iter.  770]  loss:2.527696  pct:-0.015023314\n",
      "[Iter.  780]  loss:2.527349  pct:-0.013733354\n",
      "[Iter.  790]  loss:2.527031  pct:-0.012593781\n",
      "[Iter.  800]  loss:2.526739  pct:-0.011557547\n",
      "[Iter.  810]  loss:2.526471  pct:-0.010586993\n",
      "[Iter.  820]  loss:2.526225  pct:-0.009729363\n",
      "[Iter.  830]  loss:2.525999  pct:-0.008946978\n",
      "[Iter.  840]  loss:2.525791  pct:-0.008230446\n",
      "[Iter.  850]  loss:2.525600  pct:-0.007570368\n",
      "[Iter.  860]  loss:2.525424  pct:-0.006995096\n",
      "[Iter.  870]  loss:2.525261  pct:-0.006448023\n",
      "[Iter.  880]  loss:2.525111  pct:-0.005929165\n",
      "[Iter.  890]  loss:2.524973  pct:-0.005466863\n",
      "[Iter.  900]  loss:2.524846  pct:-0.005032811\n",
      "[Iter.  910]  loss:2.524727  pct:-0.004693120\n",
      "[Iter.  920]  loss:2.524618  pct:-0.004315606\n",
      "[Iter.  930]  loss:2.524517  pct:-0.004013593\n",
      "[Iter.  940]  loss:2.524423  pct:-0.003711542\n",
      "[Iter.  950]  loss:2.524337  pct:-0.003437790\n",
      "[Iter.  960]  loss:2.524256  pct:-0.003173453\n",
      "[Iter.  970]  loss:2.524182  pct:-0.002946872\n",
      "[Iter.  980]  loss:2.524113  pct:-0.002720269\n",
      "[Iter.  990]  loss:2.524050  pct:-0.002521985\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.522520  pct:100.000000000\n",
      "[Iter.    2]  loss:2.522518  pct:-0.000094516\n",
      "[Iter.    4]  loss:2.522517  pct:-0.000028355\n",
      "[Iter.    6]  loss:2.522517  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.522517\n",
      "Best loss: 2.522517 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  6%|▋         | 648/10000 [00:14<03:32, 44.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:2822.960938  pct:100.000000000\n",
      "[Iter.   10]  loss:436.038727  pct:-84.553848671\n",
      "[Iter.   20]  loss:419.001465  pct:-3.907281834\n",
      "[Iter.   30]  loss:403.891907  pct:-3.606087179\n",
      "[Iter.   40]  loss:389.950836  pct:-3.451683563\n",
      "[Iter.   50]  loss:377.006531  pct:-3.319471128\n",
      "[Iter.   60]  loss:364.926422  pct:-3.204217343\n",
      "[Iter.   70]  loss:353.609589  pct:-3.101127463\n",
      "[Iter.   80]  loss:342.968079  pct:-3.009395207\n",
      "[Iter.   90]  loss:332.931091  pct:-2.926507722\n",
      "[Iter.  100]  loss:323.443024  pct:-2.849859288\n",
      "[Iter.  110]  loss:314.458527  pct:-2.777768080\n",
      "[Iter.  120]  loss:305.938995  pct:-2.709270231\n",
      "[Iter.  130]  loss:297.847076  pct:-2.644945256\n",
      "[Iter.  140]  loss:290.148712  pct:-2.584670076\n",
      "[Iter.  150]  loss:282.811340  pct:-2.528831430\n",
      "[Iter.  160]  loss:275.806183  pct:-2.476971914\n",
      "[Iter.  170]  loss:269.107910  pct:-2.428615862\n",
      "[Iter.  180]  loss:262.693634  pct:-2.383533104\n",
      "[Iter.  190]  loss:256.542908  pct:-2.341406689\n",
      "[Iter.  200]  loss:250.637756  pct:-2.301818210\n",
      "[Iter.  210]  loss:244.961960  pct:-2.264541700\n",
      "[Iter.  220]  loss:239.500793  pct:-2.229393652\n",
      "[Iter.  230]  loss:234.240646  pct:-2.196296312\n",
      "[Iter.  240]  loss:229.169495  pct:-2.164932437\n",
      "[Iter.  250]  loss:224.275894  pct:-2.135362943\n",
      "[Iter.  260]  loss:219.549698  pct:-2.107313542\n",
      "[Iter.  270]  loss:214.981537  pct:-2.080695649\n",
      "[Iter.  280]  loss:210.562866  pct:-2.055372158\n",
      "[Iter.  290]  loss:206.285843  pct:-2.031233423\n",
      "[Iter.  300]  loss:202.143204  pct:-2.008203327\n",
      "[Iter.  310]  loss:198.128220  pct:-1.986207825\n",
      "[Iter.  320]  loss:194.234695  pct:-1.965153766\n",
      "[Iter.  330]  loss:190.456863  pct:-1.944983116\n",
      "[Iter.  340]  loss:186.789169  pct:-1.925734797\n",
      "[Iter.  350]  loss:183.226486  pct:-1.907328524\n",
      "[Iter.  360]  loss:179.764038  pct:-1.889709393\n",
      "[Iter.  370]  loss:176.397430  pct:-1.872792635\n",
      "[Iter.  380]  loss:173.122559  pct:-1.856530346\n",
      "[Iter.  390]  loss:169.935501  pct:-1.840925597\n",
      "[Iter.  400]  loss:166.832489  pct:-1.825994018\n",
      "[Iter.  410]  loss:163.810211  pct:-1.811564312\n",
      "[Iter.  420]  loss:160.865387  pct:-1.797704916\n",
      "[Iter.  430]  loss:157.994995  pct:-1.784343978\n",
      "[Iter.  440]  loss:155.196106  pct:-1.771504951\n",
      "[Iter.  450]  loss:152.466049  pct:-1.759101329\n",
      "[Iter.  460]  loss:149.802277  pct:-1.747125079\n",
      "[Iter.  470]  loss:147.202347  pct:-1.735574297\n",
      "[Iter.  480]  loss:144.663879  pct:-1.724474821\n",
      "[Iter.  490]  loss:142.184769  pct:-1.713704021\n",
      "[Iter.  500]  loss:139.763016  pct:-1.703243570\n",
      "[Iter.  510]  loss:137.396545  pct:-1.693202114\n",
      "[Iter.  520]  loss:135.083481  pct:-1.683495439\n",
      "[Iter.  530]  loss:132.822052  pct:-1.674097246\n",
      "[Iter.  540]  loss:130.610535  pct:-1.665022713\n",
      "[Iter.  550]  loss:128.447327  pct:-1.656227818\n",
      "[Iter.  560]  loss:126.330917  pct:-1.647686532\n",
      "[Iter.  570]  loss:124.259933  pct:-1.639332580\n",
      "[Iter.  580]  loss:122.232780  pct:-1.631381056\n",
      "[Iter.  590]  loss:120.248299  pct:-1.623526687\n",
      "[Iter.  600]  loss:118.305092  pct:-1.615995244\n",
      "[Iter.  610]  loss:116.402054  pct:-1.608585053\n",
      "[Iter.  620]  loss:114.537766  pct:-1.601594017\n",
      "[Iter.  630]  loss:112.711464  pct:-1.594497297\n",
      "[Iter.  640]  loss:110.921745  pct:-1.587876304\n",
      "[Iter.  650]  loss:109.167656  pct:-1.581375546\n",
      "[Iter.  660]  loss:107.448219  pct:-1.575042196\n",
      "[Iter.  670]  loss:105.762512  pct:-1.568855308\n",
      "[Iter.  680]  loss:104.109604  pct:-1.562848963\n",
      "[Iter.  690]  loss:102.488701  pct:-1.556919779\n",
      "[Iter.  700]  loss:100.898788  pct:-1.551305072\n",
      "[Iter.  710]  loss:99.339142  pct:-1.545753552\n",
      "[Iter.  720]  loss:97.809044  pct:-1.540277008\n",
      "[Iter.  730]  loss:96.307655  pct:-1.535020168\n",
      "[Iter.  740]  loss:94.834259  pct:-1.529884926\n",
      "[Iter.  750]  loss:93.388084  pct:-1.524949566\n",
      "[Iter.  760]  loss:91.968575  pct:-1.520011784\n",
      "[Iter.  770]  loss:90.575005  pct:-1.515267529\n",
      "[Iter.  780]  loss:89.206734  pct:-1.510649522\n",
      "[Iter.  790]  loss:87.863228  pct:-1.506058796\n",
      "[Iter.  800]  loss:86.543800  pct:-1.501683381\n",
      "[Iter.  810]  loss:85.248016  pct:-1.497258026\n",
      "[Iter.  820]  loss:83.975220  pct:-1.493051317\n",
      "[Iter.  830]  loss:82.724876  pct:-1.488943199\n",
      "[Iter.  840]  loss:81.496506  pct:-1.484886675\n",
      "[Iter.  850]  loss:80.289566  pct:-1.480971100\n",
      "[Iter.  860]  loss:79.103630  pct:-1.477073588\n",
      "[Iter.  870]  loss:77.938255  pct:-1.473225382\n",
      "[Iter.  880]  loss:76.792938  pct:-1.469518497\n",
      "[Iter.  890]  loss:75.667297  pct:-1.465812997\n",
      "[Iter.  900]  loss:74.560890  pct:-1.462199925\n",
      "[Iter.  910]  loss:73.473289  pct:-1.458674521\n",
      "[Iter.  920]  loss:72.404030  pct:-1.455303895\n",
      "[Iter.  930]  loss:71.352707  pct:-1.452022683\n",
      "[Iter.  940]  loss:70.319016  pct:-1.448706645\n",
      "[Iter.  950]  loss:69.302559  pct:-1.445493224\n",
      "[Iter.  960]  loss:68.303040  pct:-1.442254607\n",
      "[Iter.  970]  loss:67.320038  pct:-1.439177108\n",
      "[Iter.  980]  loss:66.353302  pct:-1.436029852\n",
      "[Iter.  990]  loss:65.402466  pct:-1.432989999\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.658697  pct:100.000000000\n",
      "[Iter.    2]  loss:2.658116  pct:-0.021880690\n",
      "[Iter.    4]  loss:2.657995  pct:-0.004556485\n",
      "[Iter.    6]  loss:2.657967  pct:-0.001049474\n",
      "[Iter.    8]  loss:2.657931  pct:-0.001345494\n",
      "[Iter.   10]  loss:2.657928  pct:-0.000089701\n",
      "[Iter.   12]  loss:2.657920  pct:-0.000322923\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.657920\n",
      "Best loss: 2.657920 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 26%|██▌       | 2618/10000 [00:27<01:17, 95.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:397.356689  pct:100.000000000\n",
      "[Iter.   10]  loss:77.283081  pct:-80.550703409\n",
      "[Iter.   20]  loss:65.382675  pct:-15.398462020\n",
      "[Iter.   30]  loss:57.026711  pct:-12.780089892\n",
      "[Iter.   40]  loss:50.705360  pct:-11.084893449\n",
      "[Iter.   50]  loss:45.583069  pct:-10.102071109\n",
      "[Iter.   60]  loss:41.311626  pct:-9.370677581\n",
      "[Iter.   70]  loss:37.676281  pct:-8.799811997\n",
      "[Iter.   80]  loss:34.534203  pct:-8.339672384\n",
      "[Iter.   90]  loss:31.785572  pct:-7.959154458\n",
      "[Iter.  100]  loss:29.357689  pct:-7.638318241\n",
      "[Iter.  110]  loss:27.196127  pct:-7.362847849\n",
      "[Iter.  120]  loss:25.259022  pct:-7.122724435\n",
      "[Iter.  130]  loss:23.513531  pct:-6.910366698\n",
      "[Iter.  140]  loss:21.933414  pct:-6.720029799\n",
      "[Iter.  150]  loss:20.497345  pct:-6.547405062\n",
      "[Iter.  160]  loss:19.187761  pct:-6.389040463\n",
      "[Iter.  170]  loss:17.990030  pct:-6.242161339\n",
      "[Iter.  180]  loss:16.891855  pct:-6.104353529\n",
      "[Iter.  190]  loss:15.882708  pct:-5.974167015\n",
      "[Iter.  200]  loss:14.953644  pct:-5.849530323\n",
      "[Iter.  210]  loss:14.096786  pct:-5.730090347\n",
      "[Iter.  220]  loss:13.305415  pct:-5.613842173\n",
      "[Iter.  230]  loss:12.573514  pct:-5.500776642\n",
      "[Iter.  240]  loss:11.895870  pct:-5.389454187\n",
      "[Iter.  250]  loss:11.267825  pct:-5.279521978\n",
      "[Iter.  260]  loss:10.685207  pct:-5.170631894\n",
      "[Iter.  270]  loss:10.144295  pct:-5.062256722\n",
      "[Iter.  280]  loss:9.641737  pct:-4.954092595\n",
      "[Iter.  290]  loss:9.174545  pct:-4.845513800\n",
      "[Iter.  300]  loss:8.740000  pct:-4.736425657\n",
      "[Iter.  310]  loss:8.335581  pct:-4.627219175\n",
      "[Iter.  320]  loss:7.959060  pct:-4.517035086\n",
      "[Iter.  330]  loss:7.608376  pct:-4.406094459\n",
      "[Iter.  340]  loss:7.281633  pct:-4.294512363\n",
      "[Iter.  350]  loss:6.977135  pct:-4.181729289\n",
      "[Iter.  360]  loss:6.693276  pct:-4.068421287\n",
      "[Iter.  370]  loss:6.428577  pct:-3.954692862\n",
      "[Iter.  380]  loss:6.181733  pct:-3.839803930\n",
      "[Iter.  390]  loss:5.951486  pct:-3.724627976\n",
      "[Iter.  400]  loss:5.736704  pct:-3.608876122\n",
      "[Iter.  410]  loss:5.536275  pct:-3.493799758\n",
      "[Iter.  420]  loss:5.349223  pct:-3.378665923\n",
      "[Iter.  430]  loss:5.174665  pct:-3.263234331\n",
      "[Iter.  440]  loss:5.011751  pct:-3.148305483\n",
      "[Iter.  450]  loss:4.859697  pct:-3.033955688\n",
      "[Iter.  460]  loss:4.717768  pct:-2.920535037\n",
      "[Iter.  470]  loss:4.585277  pct:-2.808343262\n",
      "[Iter.  480]  loss:4.461585  pct:-2.697581183\n",
      "[Iter.  490]  loss:4.346122  pct:-2.587931841\n",
      "[Iter.  500]  loss:4.238327  pct:-2.480273420\n",
      "[Iter.  510]  loss:4.137692  pct:-2.374382834\n",
      "[Iter.  520]  loss:4.043746  pct:-2.270503620\n",
      "[Iter.  530]  loss:3.956044  pct:-2.168825579\n",
      "[Iter.  540]  loss:3.874161  pct:-2.069837060\n",
      "[Iter.  550]  loss:3.797703  pct:-1.973518172\n",
      "[Iter.  560]  loss:3.726325  pct:-1.879504206\n",
      "[Iter.  570]  loss:3.659690  pct:-1.788232786\n",
      "[Iter.  580]  loss:3.597474  pct:-1.700022906\n",
      "[Iter.  590]  loss:3.539391  pct:-1.614564242\n",
      "[Iter.  600]  loss:3.485173  pct:-1.531833584\n",
      "[Iter.  610]  loss:3.434555  pct:-1.452379265\n",
      "[Iter.  620]  loss:3.387294  pct:-1.376044223\n",
      "[Iter.  630]  loss:3.343176  pct:-1.302474453\n",
      "[Iter.  640]  loss:3.301989  pct:-1.231952402\n",
      "[Iter.  650]  loss:3.263541  pct:-1.164384655\n",
      "[Iter.  660]  loss:3.227648  pct:-1.099823681\n",
      "[Iter.  670]  loss:3.194142  pct:-1.038105480\n",
      "[Iter.  680]  loss:3.162861  pct:-0.979331356\n",
      "[Iter.  690]  loss:3.133660  pct:-0.923224855\n",
      "[Iter.  700]  loss:3.106393  pct:-0.870154762\n",
      "[Iter.  710]  loss:3.080945  pct:-0.819208920\n",
      "[Iter.  720]  loss:3.057189  pct:-0.771047588\n",
      "[Iter.  730]  loss:3.035007  pct:-0.725568034\n",
      "[Iter.  740]  loss:3.014295  pct:-0.682433259\n",
      "[Iter.  750]  loss:2.994957  pct:-0.641539410\n",
      "[Iter.  760]  loss:2.976900  pct:-0.602940893\n",
      "[Iter.  770]  loss:2.960036  pct:-0.566473453\n",
      "[Iter.  780]  loss:2.944294  pct:-0.531811898\n",
      "[Iter.  790]  loss:2.929593  pct:-0.499325223\n",
      "[Iter.  800]  loss:2.915863  pct:-0.468651209\n",
      "[Iter.  810]  loss:2.903049  pct:-0.439467909\n",
      "[Iter.  820]  loss:2.891081  pct:-0.412244472\n",
      "[Iter.  830]  loss:2.879902  pct:-0.386695644\n",
      "[Iter.  840]  loss:2.869464  pct:-0.362416895\n",
      "[Iter.  850]  loss:2.859715  pct:-0.339780810\n",
      "[Iter.  860]  loss:2.850611  pct:-0.318337253\n",
      "[Iter.  870]  loss:2.842107  pct:-0.298327354\n",
      "[Iter.  880]  loss:2.834166  pct:-0.279397282\n",
      "[Iter.  890]  loss:2.826748  pct:-0.261723509\n",
      "[Iter.  900]  loss:2.819817  pct:-0.245195737\n",
      "[Iter.  910]  loss:2.813339  pct:-0.229725266\n",
      "[Iter.  920]  loss:2.807282  pct:-0.215305066\n",
      "[Iter.  930]  loss:2.801625  pct:-0.201527101\n",
      "[Iter.  940]  loss:2.796345  pct:-0.188454196\n",
      "[Iter.  950]  loss:2.791407  pct:-0.176583619\n",
      "[Iter.  960]  loss:2.786785  pct:-0.165570409\n",
      "[Iter.  970]  loss:2.782462  pct:-0.155133730\n",
      "[Iter.  980]  loss:2.778421  pct:-0.145238093\n",
      "[Iter.  990]  loss:2.774646  pct:-0.135864272\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.716587  pct:100.000000000\n",
      "[Iter.    2]  loss:2.716592  pct:0.000184304\n",
      "[Iter.    4]  loss:2.716592  pct:0.000008776\n",
      "[Iter.    6]  loss:2.716591  pct:-0.000043882\n",
      "[Iter.    8]  loss:2.716592  pct:0.000026329\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.716592\n",
      "Best loss: 2.716592 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 51%|█████     | 5120/10000 [01:14<01:10, 68.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:4046.176025  pct:100.000000000\n",
      "[Iter.   10]  loss:709.553894  pct:-82.463591534\n",
      "[Iter.   20]  loss:683.983459  pct:-3.603733950\n",
      "[Iter.   30]  loss:661.678528  pct:-3.261033777\n",
      "[Iter.   40]  loss:641.060669  pct:-3.115993344\n",
      "[Iter.   50]  loss:621.863037  pct:-2.994666927\n",
      "[Iter.   60]  loss:603.927490  pct:-2.884163522\n",
      "[Iter.   70]  loss:587.121887  pct:-2.782718671\n",
      "[Iter.   80]  loss:571.332153  pct:-2.689345131\n",
      "[Iter.   90]  loss:556.458557  pct:-2.603318596\n",
      "[Iter.  100]  loss:542.413452  pct:-2.524016353\n",
      "[Iter.  110]  loss:529.120239  pct:-2.450752805\n",
      "[Iter.  120]  loss:516.512512  pct:-2.382771649\n",
      "[Iter.  130]  loss:504.531433  pct:-2.319610623\n",
      "[Iter.  140]  loss:493.125519  pct:-2.260694490\n",
      "[Iter.  150]  loss:482.247894  pct:-2.205853094\n",
      "[Iter.  160]  loss:471.858398  pct:-2.154389054\n",
      "[Iter.  170]  loss:461.920746  pct:-2.106066697\n",
      "[Iter.  180]  loss:452.401947  pct:-2.060699571\n",
      "[Iter.  190]  loss:443.273041  pct:-2.017875102\n",
      "[Iter.  200]  loss:434.507416  pct:-1.977477580\n",
      "[Iter.  210]  loss:426.080505  pct:-1.939416934\n",
      "[Iter.  220]  loss:417.970428  pct:-1.903414214\n",
      "[Iter.  230]  loss:410.157257  pct:-1.869312003\n",
      "[Iter.  240]  loss:402.622467  pct:-1.837049061\n",
      "[Iter.  250]  loss:395.349548  pct:-1.806386701\n",
      "[Iter.  260]  loss:388.323090  pct:-1.777277543\n",
      "[Iter.  270]  loss:381.529510  pct:-1.749465660\n",
      "[Iter.  280]  loss:374.955597  pct:-1.723041965\n",
      "[Iter.  290]  loss:368.589752  pct:-1.697759622\n",
      "[Iter.  300]  loss:362.420776  pct:-1.673669925\n",
      "[Iter.  310]  loss:356.438324  pct:-1.650692450\n",
      "[Iter.  320]  loss:350.633148  pct:-1.628662069\n",
      "[Iter.  330]  loss:344.996552  pct:-1.607548148\n",
      "[Iter.  340]  loss:339.520355  pct:-1.587319138\n",
      "[Iter.  350]  loss:334.197144  pct:-1.567862306\n",
      "[Iter.  360]  loss:329.019653  pct:-1.549232342\n",
      "[Iter.  370]  loss:323.981506  pct:-1.531260191\n",
      "[Iter.  380]  loss:319.076538  pct:-1.513965509\n",
      "[Iter.  390]  loss:314.299103  pct:-1.497269380\n",
      "[Iter.  400]  loss:309.644043  pct:-1.481092301\n",
      "[Iter.  410]  loss:305.105896  pct:-1.465601253\n",
      "[Iter.  420]  loss:300.679626  pct:-1.450732218\n",
      "[Iter.  430]  loss:296.360870  pct:-1.436331472\n",
      "[Iter.  440]  loss:292.145355  pct:-1.422426359\n",
      "[Iter.  450]  loss:288.029266  pct:-1.408918127\n",
      "[Iter.  460]  loss:284.008667  pct:-1.395899596\n",
      "[Iter.  470]  loss:280.079926  pct:-1.383317452\n",
      "[Iter.  480]  loss:276.239502  pct:-1.371188448\n",
      "[Iter.  490]  loss:272.484283  pct:-1.359406775\n",
      "[Iter.  500]  loss:268.810944  pct:-1.348092373\n",
      "[Iter.  510]  loss:265.216705  pct:-1.337087781\n",
      "[Iter.  520]  loss:261.698914  pct:-1.326383926\n",
      "[Iter.  530]  loss:258.254852  pct:-1.316039579\n",
      "[Iter.  540]  loss:254.882141  pct:-1.305962367\n",
      "[Iter.  550]  loss:251.578537  pct:-1.296130090\n",
      "[Iter.  560]  loss:248.341385  pct:-1.286736197\n",
      "[Iter.  570]  loss:245.169006  pct:-1.277426451\n",
      "[Iter.  580]  loss:242.059036  pct:-1.268500509\n",
      "[Iter.  590]  loss:239.009552  pct:-1.259810127\n",
      "[Iter.  600]  loss:236.018692  pct:-1.251355839\n",
      "[Iter.  610]  loss:233.084732  pct:-1.243104915\n",
      "[Iter.  620]  loss:230.206009  pct:-1.235054359\n",
      "[Iter.  630]  loss:227.380676  pct:-1.227306209\n",
      "[Iter.  640]  loss:224.607193  pct:-1.219753289\n",
      "[Iter.  650]  loss:221.884109  pct:-1.212375908\n",
      "[Iter.  660]  loss:219.209946  pct:-1.205207450\n",
      "[Iter.  670]  loss:216.583450  pct:-1.198164323\n",
      "[Iter.  680]  loss:214.003098  pct:-1.191389637\n",
      "[Iter.  690]  loss:211.467667  pct:-1.184763649\n",
      "[Iter.  700]  loss:208.976120  pct:-1.178216354\n",
      "[Iter.  710]  loss:206.527191  pct:-1.171870180\n",
      "[Iter.  720]  loss:204.119644  pct:-1.165728824\n",
      "[Iter.  730]  loss:201.752518  pct:-1.159675971\n",
      "[Iter.  740]  loss:199.424622  pct:-1.153837456\n",
      "[Iter.  750]  loss:197.134781  pct:-1.148223665\n",
      "[Iter.  760]  loss:194.882507  pct:-1.142504407\n",
      "[Iter.  770]  loss:192.666534  pct:-1.137081481\n",
      "[Iter.  780]  loss:190.485977  pct:-1.131777897\n",
      "[Iter.  790]  loss:188.340042  pct:-1.126558023\n",
      "[Iter.  800]  loss:186.227875  pct:-1.121464843\n",
      "[Iter.  810]  loss:184.148682  pct:-1.116477927\n",
      "[Iter.  820]  loss:182.101700  pct:-1.111591890\n",
      "[Iter.  830]  loss:180.086151  pct:-1.106825860\n",
      "[Iter.  840]  loss:178.101135  pct:-1.102259034\n",
      "[Iter.  850]  loss:176.145844  pct:-1.097854736\n",
      "[Iter.  860]  loss:174.219788  pct:-1.093443859\n",
      "[Iter.  870]  loss:172.322281  pct:-1.089145349\n",
      "[Iter.  880]  loss:170.452927  pct:-1.084801245\n",
      "[Iter.  890]  loss:168.610733  pct:-1.080763845\n",
      "[Iter.  900]  loss:166.795395  pct:-1.076644471\n",
      "[Iter.  910]  loss:165.006348  pct:-1.072599901\n",
      "[Iter.  920]  loss:163.242981  pct:-1.068665978\n",
      "[Iter.  930]  loss:161.504654  pct:-1.064870916\n",
      "[Iter.  940]  loss:159.790833  pct:-1.061159149\n",
      "[Iter.  950]  loss:158.101151  pct:-1.057433634\n",
      "[Iter.  960]  loss:156.435120  pct:-1.053775307\n",
      "[Iter.  970]  loss:154.792160  pct:-1.050249841\n",
      "[Iter.  980]  loss:153.171799  pct:-1.046798060\n",
      "[Iter.  990]  loss:151.573669  pct:-1.043357384\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:4.496773  pct:100.000000000\n",
      "[Iter.    2]  loss:4.494607  pct:-0.048173904\n",
      "[Iter.    4]  loss:4.493598  pct:-0.022448847\n",
      "[Iter.    6]  loss:4.493018  pct:-0.012914169\n",
      "[Iter.    8]  loss:4.492612  pct:-0.009031534\n",
      "[Iter.   10]  loss:4.492309  pct:-0.006750381\n",
      "[Iter.   12]  loss:4.492072  pct:-0.005275418\n",
      "[Iter.   14]  loss:4.491786  pct:-0.006347820\n",
      "[Iter.   16]  loss:4.491569  pct:-0.004851401\n",
      "[Iter.   18]  loss:4.491339  pct:-0.005106427\n",
      "[Iter.   20]  loss:4.491092  pct:-0.005510127\n",
      "[Iter.   22]  loss:4.490856  pct:-0.005244995\n",
      "[Iter.   24]  loss:4.490652  pct:-0.004555103\n",
      "[Iter.   26]  loss:4.490449  pct:-0.004512837\n",
      "[Iter.   28]  loss:4.490232  pct:-0.004831608\n",
      "[Iter.   30]  loss:4.490018  pct:-0.004757506\n",
      "[Iter.   32]  loss:4.489784  pct:-0.005225009\n",
      "[Iter.   34]  loss:4.489589  pct:-0.004333161\n",
      "[Iter.   36]  loss:4.489349  pct:-0.005342339\n",
      "[Iter.   38]  loss:4.489166  pct:-0.004078664\n",
      "[Iter.   40]  loss:4.488937  pct:-0.005109160\n",
      "[Iter.   42]  loss:4.488728  pct:-0.004663276\n",
      "[Iter.   44]  loss:4.488530  pct:-0.004408542\n",
      "[Iter.   46]  loss:4.488332  pct:-0.004398112\n",
      "[Iter.   48]  loss:4.488139  pct:-0.004313314\n",
      "New best!\n",
      "Trial 0 loss: 4.488139\n",
      "Best loss: 4.488139 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 82%|████████▏ | 8192/10000 [01:25<00:18, 96.08it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:330.104645  pct:100.000000000\n",
      "[Iter.   10]  loss:75.954613  pct:-76.990744734\n",
      "[Iter.   20]  loss:61.428406  pct:-19.124851602\n",
      "[Iter.   30]  loss:53.018814  pct:-13.690069880\n",
      "[Iter.   40]  loss:46.657047  pct:-11.999074149\n",
      "[Iter.   50]  loss:41.599060  pct:-10.840778637\n",
      "[Iter.   60]  loss:37.442638  pct:-9.991623983\n",
      "[Iter.   70]  loss:33.945786  pct:-9.339226680\n",
      "[Iter.   80]  loss:30.951767  pct:-8.820000800\n",
      "[Iter.   90]  loss:28.353415  pct:-8.394843613\n",
      "[Iter.  100]  loss:26.074177  pct:-8.038671125\n",
      "[Iter.  110]  loss:24.057646  pct:-7.733824185\n",
      "[Iter.  120]  loss:22.260857  pct:-7.468682449\n",
      "[Iter.  130]  loss:20.650513  pct:-7.233971091\n",
      "[Iter.  140]  loss:19.200212  pct:-7.023071234\n",
      "[Iter.  150]  loss:17.888594  pct:-6.831272343\n",
      "[Iter.  160]  loss:16.698225  pct:-6.654344517\n",
      "[Iter.  170]  loss:15.614615  pct:-6.489369856\n",
      "[Iter.  180]  loss:14.625673  pct:-6.333439015\n",
      "[Iter.  190]  loss:13.721072  pct:-6.185021906\n",
      "[Iter.  200]  loss:12.892000  pct:-6.042326625\n",
      "[Iter.  210]  loss:12.130852  pct:-5.904036930\n",
      "[Iter.  220]  loss:11.431006  pct:-5.769135826\n",
      "[Iter.  230]  loss:10.786744  pct:-5.636094404\n",
      "[Iter.  240]  loss:10.192930  pct:-5.505033676\n",
      "[Iter.  250]  loss:9.645093  pct:-5.374678777\n",
      "[Iter.  260]  loss:9.139250  pct:-5.244564925\n",
      "[Iter.  270]  loss:8.671791  pct:-5.114847883\n",
      "[Iter.  280]  loss:8.239501  pct:-4.985014899\n",
      "[Iter.  290]  loss:7.839538  pct:-4.854218465\n",
      "[Iter.  300]  loss:7.469291  pct:-4.722815364\n",
      "[Iter.  310]  loss:7.126380  pct:-4.590935097\n",
      "[Iter.  320]  loss:6.808640  pct:-4.458657796\n",
      "[Iter.  330]  loss:6.514133  pct:-4.325476064\n",
      "[Iter.  340]  loss:6.241088  pct:-4.191586522\n",
      "[Iter.  350]  loss:5.987870  pct:-4.057268551\n",
      "[Iter.  360]  loss:5.753006  pct:-3.922341388\n",
      "[Iter.  370]  loss:5.535110  pct:-3.787507376\n",
      "[Iter.  380]  loss:5.332911  pct:-3.653016932\n",
      "[Iter.  390]  loss:5.145278  pct:-3.518397743\n",
      "[Iter.  400]  loss:4.971146  pct:-3.384313379\n",
      "[Iter.  410]  loss:4.809504  pct:-3.251605789\n",
      "[Iter.  420]  loss:4.659461  pct:-3.119728906\n",
      "[Iter.  430]  loss:4.520178  pct:-2.989235025\n",
      "[Iter.  440]  loss:4.390848  pct:-2.861184360\n",
      "[Iter.  450]  loss:4.270778  pct:-2.734551260\n",
      "[Iter.  460]  loss:4.159301  pct:-2.610213622\n",
      "[Iter.  470]  loss:4.055781  pct:-2.488877567\n",
      "[Iter.  480]  loss:3.959670  pct:-2.369729789\n",
      "[Iter.  490]  loss:3.870422  pct:-2.253941625\n",
      "[Iter.  500]  loss:3.787557  pct:-2.140975038\n",
      "[Iter.  510]  loss:3.710623  pct:-2.031232837\n",
      "[Iter.  520]  loss:3.639201  pct:-1.924788029\n",
      "[Iter.  530]  loss:3.572883  pct:-1.822330521\n",
      "[Iter.  540]  loss:3.511305  pct:-1.723476498\n",
      "[Iter.  550]  loss:3.454131  pct:-1.628282528\n",
      "[Iter.  560]  loss:3.401040  pct:-1.537037410\n",
      "[Iter.  570]  loss:3.351742  pct:-1.449499281\n",
      "[Iter.  580]  loss:3.305974  pct:-1.365499653\n",
      "[Iter.  590]  loss:3.263474  pct:-1.285552217\n",
      "[Iter.  600]  loss:3.224031  pct:-1.208620601\n",
      "[Iter.  610]  loss:3.187415  pct:-1.135716486\n",
      "[Iter.  620]  loss:3.153408  pct:-1.066909557\n",
      "[Iter.  630]  loss:3.121838  pct:-1.001145289\n",
      "[Iter.  640]  loss:3.092523  pct:-0.939022389\n",
      "[Iter.  650]  loss:3.065297  pct:-0.880372820\n",
      "[Iter.  660]  loss:3.040019  pct:-0.824677141\n",
      "[Iter.  670]  loss:3.016553  pct:-0.771891123\n",
      "[Iter.  680]  loss:2.994773  pct:-0.722024542\n",
      "[Iter.  690]  loss:2.974554  pct:-0.675122106\n",
      "[Iter.  700]  loss:2.955781  pct:-0.631138445\n",
      "[Iter.  710]  loss:2.938347  pct:-0.589815174\n",
      "[Iter.  720]  loss:2.922164  pct:-0.550764622\n",
      "[Iter.  730]  loss:2.907126  pct:-0.514594650\n",
      "[Iter.  740]  loss:2.893165  pct:-0.480260916\n",
      "[Iter.  750]  loss:2.880201  pct:-0.448090996\n",
      "[Iter.  760]  loss:2.868162  pct:-0.417965012\n",
      "[Iter.  770]  loss:2.856987  pct:-0.389636029\n",
      "[Iter.  780]  loss:2.846611  pct:-0.363162301\n",
      "[Iter.  790]  loss:2.836983  pct:-0.338253841\n",
      "[Iter.  800]  loss:2.828047  pct:-0.314980005\n",
      "[Iter.  810]  loss:2.819749  pct:-0.293415237\n",
      "[Iter.  820]  loss:2.812042  pct:-0.273301066\n",
      "[Iter.  830]  loss:2.804887  pct:-0.254447792\n",
      "[Iter.  840]  loss:2.798240  pct:-0.236974640\n",
      "[Iter.  850]  loss:2.792054  pct:-0.221067797\n",
      "[Iter.  860]  loss:2.786303  pct:-0.205999202\n",
      "[Iter.  870]  loss:2.780964  pct:-0.191595486\n",
      "[Iter.  880]  loss:2.775987  pct:-0.178966257\n",
      "[Iter.  890]  loss:2.771360  pct:-0.166704814\n",
      "[Iter.  900]  loss:2.767066  pct:-0.154913250\n",
      "[Iter.  910]  loss:2.763083  pct:-0.143978270\n",
      "[Iter.  920]  loss:2.759388  pct:-0.133693346\n",
      "[Iter.  930]  loss:2.755949  pct:-0.124653158\n",
      "[Iter.  940]  loss:2.752731  pct:-0.116745955\n",
      "[Iter.  950]  loss:2.749733  pct:-0.108905478\n",
      "[Iter.  960]  loss:2.746935  pct:-0.101766914\n",
      "[Iter.  970]  loss:2.744331  pct:-0.094796841\n",
      "[Iter.  980]  loss:2.741907  pct:-0.088318908\n",
      "[Iter.  990]  loss:2.739654  pct:-0.082171105\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.706746  pct:100.000000000\n",
      "[Iter.    2]  loss:2.706748  pct:0.000096891\n",
      "[Iter.    4]  loss:2.706748  pct:-0.000026425\n",
      "[Iter.    6]  loss:2.706748  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.706748\n",
      "Best loss: 2.706748 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▌        | 1598/10000 [00:15<01:20, 104.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:281.361755  pct:100.000000000\n",
      "[Iter.   10]  loss:63.547733  pct:-77.414226248\n",
      "[Iter.   20]  loss:50.492653  pct:-20.543738910\n",
      "[Iter.   30]  loss:43.312988  pct:-14.219226364\n",
      "[Iter.   40]  loss:37.910770  pct:-12.472512471\n",
      "[Iter.   50]  loss:33.628605  pct:-11.295379863\n",
      "[Iter.   60]  loss:30.120920  pct:-10.430657826\n",
      "[Iter.   70]  loss:27.179983  pct:-9.763768917\n",
      "[Iter.   80]  loss:24.671177  pct:-9.230345051\n",
      "[Iter.   90]  loss:22.502445  pct:-8.790548166\n",
      "[Iter.  100]  loss:20.608038  pct:-8.418672965\n",
      "[Iter.  110]  loss:18.939373  pct:-8.097155762\n",
      "[Iter.  120]  loss:17.459497  pct:-7.813751613\n",
      "[Iter.  130]  loss:16.139681  pct:-7.559304573\n",
      "[Iter.  140]  loss:14.957137  pct:-7.326933938\n",
      "[Iter.  150]  loss:13.893435  pct:-7.111672345\n",
      "[Iter.  160]  loss:12.933424  pct:-6.909814322\n",
      "[Iter.  170]  loss:12.064588  pct:-6.717760147\n",
      "[Iter.  180]  loss:11.276391  pct:-6.533141375\n",
      "[Iter.  190]  loss:10.559875  pct:-6.354121094\n",
      "[Iter.  200]  loss:9.907364  pct:-6.179159948\n",
      "[Iter.  210]  loss:9.312252  pct:-6.006762782\n",
      "[Iter.  220]  loss:8.768750  pct:-5.836416920\n",
      "[Iter.  230]  loss:8.271861  pct:-5.666589920\n",
      "[Iter.  240]  loss:7.817107  pct:-5.497600498\n",
      "[Iter.  250]  loss:7.400565  pct:-5.328595893\n",
      "[Iter.  260]  loss:7.018755  pct:-5.159196681\n",
      "[Iter.  270]  loss:6.668573  pct:-4.989239709\n",
      "[Iter.  280]  loss:6.347220  pct:-4.818909331\n",
      "[Iter.  290]  loss:6.052189  pct:-4.648200755\n",
      "[Iter.  300]  loss:5.781217  pct:-4.477252457\n",
      "[Iter.  310]  loss:5.532237  pct:-4.306706375\n",
      "[Iter.  320]  loss:5.303409  pct:-4.136264429\n",
      "[Iter.  330]  loss:5.093052  pct:-3.966452243\n",
      "[Iter.  340]  loss:4.899636  pct:-3.797637354\n",
      "[Iter.  350]  loss:4.721774  pct:-3.630109616\n",
      "[Iter.  360]  loss:4.558197  pct:-3.464324068\n",
      "[Iter.  370]  loss:4.407706  pct:-3.301531263\n",
      "[Iter.  380]  loss:4.269264  pct:-3.140908929\n",
      "[Iter.  390]  loss:4.141891  pct:-2.983504622\n",
      "[Iter.  400]  loss:4.024697  pct:-2.829473598\n",
      "[Iter.  410]  loss:3.916864  pct:-2.679268438\n",
      "[Iter.  420]  loss:3.817640  pct:-2.533253148\n",
      "[Iter.  430]  loss:3.726329  pct:-2.391817213\n",
      "[Iter.  440]  loss:3.642316  pct:-2.254583969\n",
      "[Iter.  450]  loss:3.565004  pct:-2.122618337\n",
      "[Iter.  460]  loss:3.493857  pct:-1.995705260\n",
      "[Iter.  470]  loss:3.428386  pct:-1.873866757\n",
      "[Iter.  480]  loss:3.368137  pct:-1.757380807\n",
      "[Iter.  490]  loss:3.312698  pct:-1.645969994\n",
      "[Iter.  500]  loss:3.261688  pct:-1.539850802\n",
      "[Iter.  510]  loss:3.214739  pct:-1.439383578\n",
      "[Iter.  520]  loss:3.171546  pct:-1.343595780\n",
      "[Iter.  530]  loss:3.131798  pct:-1.253289907\n",
      "[Iter.  540]  loss:3.095221  pct:-1.167923076\n",
      "[Iter.  550]  loss:3.061560  pct:-1.087496387\n",
      "[Iter.  560]  loss:3.030582  pct:-1.011843676\n",
      "[Iter.  570]  loss:3.002079  pct:-0.940502622\n",
      "[Iter.  580]  loss:2.975850  pct:-0.873691284\n",
      "[Iter.  590]  loss:2.951715  pct:-0.811056513\n",
      "[Iter.  600]  loss:2.929506  pct:-0.752383528\n",
      "[Iter.  610]  loss:2.909073  pct:-0.697495939\n",
      "[Iter.  620]  loss:2.890266  pct:-0.646508714\n",
      "[Iter.  630]  loss:2.872954  pct:-0.598977798\n",
      "[Iter.  640]  loss:2.857021  pct:-0.554562428\n",
      "[Iter.  650]  loss:2.842358  pct:-0.513234511\n",
      "[Iter.  660]  loss:2.828851  pct:-0.475225291\n",
      "[Iter.  670]  loss:2.816417  pct:-0.439508982\n",
      "[Iter.  680]  loss:2.804968  pct:-0.406529771\n",
      "[Iter.  690]  loss:2.794429  pct:-0.375719684\n",
      "[Iter.  700]  loss:2.784721  pct:-0.347394372\n",
      "[Iter.  710]  loss:2.775781  pct:-0.321053962\n",
      "[Iter.  720]  loss:2.767546  pct:-0.296681056\n",
      "[Iter.  730]  loss:2.759959  pct:-0.274148840\n",
      "[Iter.  740]  loss:2.752970  pct:-0.253228525\n",
      "[Iter.  750]  loss:2.746532  pct:-0.233848521\n",
      "[Iter.  760]  loss:2.740597  pct:-0.216071666\n",
      "[Iter.  770]  loss:2.735127  pct:-0.199584198\n",
      "[Iter.  780]  loss:2.730086  pct:-0.184336481\n",
      "[Iter.  790]  loss:2.725436  pct:-0.170302378\n",
      "[Iter.  800]  loss:2.721151  pct:-0.157217354\n",
      "[Iter.  810]  loss:2.717198  pct:-0.145268658\n",
      "[Iter.  820]  loss:2.713552  pct:-0.134204856\n",
      "[Iter.  830]  loss:2.710188  pct:-0.123973539\n",
      "[Iter.  840]  loss:2.707082  pct:-0.114591341\n",
      "[Iter.  850]  loss:2.704215  pct:-0.105897973\n",
      "[Iter.  860]  loss:2.701569  pct:-0.097846107\n",
      "[Iter.  870]  loss:2.699127  pct:-0.090414054\n",
      "[Iter.  880]  loss:2.696871  pct:-0.083579499\n",
      "[Iter.  890]  loss:2.694786  pct:-0.077310729\n",
      "[Iter.  900]  loss:2.692858  pct:-0.071540106\n",
      "[Iter.  910]  loss:2.691077  pct:-0.066155127\n",
      "[Iter.  920]  loss:2.689430  pct:-0.061202108\n",
      "[Iter.  930]  loss:2.687907  pct:-0.056612045\n",
      "[Iter.  940]  loss:2.686500  pct:-0.052351010\n",
      "[Iter.  950]  loss:2.685200  pct:-0.048393694\n",
      "[Iter.  960]  loss:2.683999  pct:-0.044714587\n",
      "[Iter.  970]  loss:2.682879  pct:-0.041714383\n",
      "[Iter.  980]  loss:2.681842  pct:-0.038683664\n",
      "[Iter.  990]  loss:2.680881  pct:-0.035827130\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.667318  pct:100.000000000\n",
      "[Iter.    2]  loss:2.667318  pct:0.000017877\n",
      "[Iter.    4]  loss:2.667318  pct:-0.000026816\n",
      "[Iter.    6]  loss:2.667318  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.667318\n",
      "Best loss: 2.667318 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 299/10000 [00:02<01:37, 99.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:413.769562  pct:100.000000000\n",
      "[Iter.   10]  loss:63.444294  pct:-84.666753845\n",
      "[Iter.   20]  loss:54.802547  pct:-13.620998800\n",
      "[Iter.   30]  loss:48.739159  pct:-11.064063818\n",
      "[Iter.   40]  loss:43.777100  pct:-10.180846696\n",
      "[Iter.   50]  loss:39.613380  pct:-9.511180993\n",
      "[Iter.   60]  loss:36.057198  pct:-8.977226438\n",
      "[Iter.   70]  loss:32.977779  pct:-8.540370272\n",
      "[Iter.   80]  loss:30.281805  pct:-8.175123977\n",
      "[Iter.   90]  loss:27.900347  pct:-7.864320768\n",
      "[Iter.  100]  loss:25.781210  pct:-7.595378039\n",
      "[Iter.  110]  loss:23.883913  pct:-7.359223673\n",
      "[Iter.  120]  loss:22.176472  pct:-7.148917881\n",
      "[Iter.  130]  loss:20.633118  pct:-6.959421023\n",
      "[Iter.  140]  loss:19.232912  pct:-6.786204752\n",
      "[Iter.  150]  loss:17.958496  pct:-6.626224701\n",
      "[Iter.  160]  loss:16.795353  pct:-6.476840554\n",
      "[Iter.  170]  loss:15.731200  pct:-6.335994972\n",
      "[Iter.  180]  loss:14.755582  pct:-6.201805005\n",
      "[Iter.  190]  loss:13.859522  pct:-6.072684891\n",
      "[Iter.  200]  loss:13.035202  pct:-5.947678769\n",
      "[Iter.  210]  loss:12.275851  pct:-5.825385561\n",
      "[Iter.  220]  loss:11.575494  pct:-5.705163926\n",
      "[Iter.  230]  loss:10.928793  pct:-5.586810114\n",
      "[Iter.  240]  loss:10.331093  pct:-5.469040557\n",
      "[Iter.  250]  loss:9.778245  pct:-5.351300885\n",
      "[Iter.  260]  loss:9.266487  pct:-5.233637039\n",
      "[Iter.  270]  loss:8.792437  pct:-5.115752233\n",
      "[Iter.  280]  loss:8.353079  pct:-4.996996596\n",
      "[Iter.  290]  loss:7.945688  pct:-4.877130962\n",
      "[Iter.  300]  loss:7.567771  pct:-4.756250197\n",
      "[Iter.  310]  loss:7.217078  pct:-4.634041946\n",
      "[Iter.  320]  loss:6.891533  pct:-4.510756933\n",
      "[Iter.  330]  loss:6.589229  pct:-4.386604147\n",
      "[Iter.  340]  loss:6.308421  pct:-4.261621317\n",
      "[Iter.  350]  loss:6.047534  pct:-4.135538198\n",
      "[Iter.  360]  loss:5.805118  pct:-4.008500783\n",
      "[Iter.  370]  loss:5.579839  pct:-3.880693745\n",
      "[Iter.  380]  loss:5.370424  pct:-3.753072932\n",
      "[Iter.  390]  loss:5.175756  pct:-3.624804053\n",
      "[Iter.  400]  loss:4.994776  pct:-3.496690911\n",
      "[Iter.  410]  loss:4.826505  pct:-3.368940997\n",
      "[Iter.  420]  loss:4.670060  pct:-3.241372803\n",
      "[Iter.  430]  loss:4.524609  pct:-3.114554006\n",
      "[Iter.  440]  loss:4.389341  pct:-2.989600784\n",
      "[Iter.  450]  loss:4.263519  pct:-2.866536703\n",
      "[Iter.  460]  loss:4.146497  pct:-2.744729007\n",
      "[Iter.  470]  loss:4.037660  pct:-2.624797115\n",
      "[Iter.  480]  loss:3.936425  pct:-2.507267124\n",
      "[Iter.  490]  loss:3.842264  pct:-2.392032446\n",
      "[Iter.  500]  loss:3.754693  pct:-2.279167099\n",
      "[Iter.  510]  loss:3.673235  pct:-2.169494629\n",
      "[Iter.  520]  loss:3.597481  pct:-2.062315506\n",
      "[Iter.  530]  loss:3.527025  pct:-1.958482275\n",
      "[Iter.  540]  loss:3.461487  pct:-1.858171355\n",
      "[Iter.  550]  loss:3.400534  pct:-1.760887881\n",
      "[Iter.  560]  loss:3.343840  pct:-1.667209685\n",
      "[Iter.  570]  loss:3.291091  pct:-1.577502476\n",
      "[Iter.  580]  loss:3.242039  pct:-1.490440881\n",
      "[Iter.  590]  loss:3.196418  pct:-1.407174842\n",
      "[Iter.  600]  loss:3.153979  pct:-1.327697067\n",
      "[Iter.  610]  loss:3.114518  pct:-1.251161580\n",
      "[Iter.  620]  loss:3.077818  pct:-1.178345270\n",
      "[Iter.  630]  loss:3.043702  pct:-1.108433111\n",
      "[Iter.  640]  loss:3.011984  pct:-1.042094472\n",
      "[Iter.  650]  loss:2.982473  pct:-0.979785212\n",
      "[Iter.  660]  loss:2.954996  pct:-0.921283269\n",
      "[Iter.  670]  loss:2.929416  pct:-0.865666324\n",
      "[Iter.  680]  loss:2.905629  pct:-0.811989394\n",
      "[Iter.  690]  loss:2.883507  pct:-0.761362924\n",
      "[Iter.  700]  loss:2.862927  pct:-0.713716143\n",
      "[Iter.  710]  loss:2.843781  pct:-0.668763327\n",
      "[Iter.  720]  loss:2.825982  pct:-0.625863538\n",
      "[Iter.  730]  loss:2.809433  pct:-0.585622460\n",
      "[Iter.  740]  loss:2.794021  pct:-0.548575289\n",
      "[Iter.  750]  loss:2.779665  pct:-0.513799289\n",
      "[Iter.  760]  loss:2.766307  pct:-0.480582799\n",
      "[Iter.  770]  loss:2.753885  pct:-0.449049452\n",
      "[Iter.  780]  loss:2.742331  pct:-0.419544252\n",
      "[Iter.  790]  loss:2.731568  pct:-0.392456414\n",
      "[Iter.  800]  loss:2.721547  pct:-0.366857792\n",
      "[Iter.  810]  loss:2.712233  pct:-0.342260274\n",
      "[Iter.  820]  loss:2.703565  pct:-0.319569533\n",
      "[Iter.  830]  loss:2.695488  pct:-0.298750200\n",
      "[Iter.  840]  loss:2.687959  pct:-0.279319356\n",
      "[Iter.  850]  loss:2.680963  pct:-0.260295306\n",
      "[Iter.  860]  loss:2.674462  pct:-0.242468233\n",
      "[Iter.  870]  loss:2.668376  pct:-0.227546103\n",
      "[Iter.  880]  loss:2.662705  pct:-0.212527147\n",
      "[Iter.  890]  loss:2.657422  pct:-0.198429540\n",
      "[Iter.  900]  loss:2.652499  pct:-0.185231789\n",
      "[Iter.  910]  loss:2.647923  pct:-0.172515316\n",
      "[Iter.  920]  loss:2.643664  pct:-0.160865160\n",
      "[Iter.  930]  loss:2.639689  pct:-0.150338238\n",
      "[Iter.  940]  loss:2.635986  pct:-0.140304164\n",
      "[Iter.  950]  loss:2.632518  pct:-0.131556026\n",
      "[Iter.  960]  loss:2.629278  pct:-0.123062087\n",
      "[Iter.  970]  loss:2.626259  pct:-0.114835038\n",
      "[Iter.  980]  loss:2.623441  pct:-0.107305011\n",
      "[Iter.  990]  loss:2.620819  pct:-0.099959022\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.580227  pct:100.000000000\n",
      "[Iter.    2]  loss:2.580198  pct:-0.001099586\n",
      "[Iter.    4]  loss:2.580193  pct:-0.000194047\n",
      "[Iter.    6]  loss:2.580191  pct:-0.000101644\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.580191\n",
      "Best loss: 2.580191 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 26%|██▋       | 2647/10000 [00:32<01:30, 81.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:307.905121  pct:100.000000000\n",
      "[Iter.   10]  loss:69.487282  pct:-77.432244719\n",
      "[Iter.   20]  loss:57.387890  pct:-17.412383423\n",
      "[Iter.   30]  loss:49.188313  pct:-14.287992382\n",
      "[Iter.   40]  loss:43.060574  pct:-12.457713301\n",
      "[Iter.   50]  loss:38.221313  pct:-11.238262056\n",
      "[Iter.   60]  loss:34.265102  pct:-10.350798364\n",
      "[Iter.   70]  loss:30.951530  pct:-9.670398450\n",
      "[Iter.   80]  loss:28.125967  pct:-9.128994234\n",
      "[Iter.   90]  loss:25.683046  pct:-8.685641573\n",
      "[Iter.  100]  loss:23.547871  pct:-8.313560925\n",
      "[Iter.  110]  loss:21.665314  pct:-7.994595114\n",
      "[Iter.  120]  loss:19.993637  pct:-7.715912436\n",
      "[Iter.  130]  loss:18.500439  pct:-7.468368003\n",
      "[Iter.  140]  loss:17.160110  pct:-7.244845590\n",
      "[Iter.  150]  loss:15.952004  pct:-7.040199670\n",
      "[Iter.  160]  loss:14.859234  pct:-6.850365301\n",
      "[Iter.  170]  loss:13.867824  pct:-6.672014621\n",
      "[Iter.  180]  loss:12.966054  pct:-6.502603898\n",
      "[Iter.  190]  loss:12.144001  pct:-6.340039599\n",
      "[Iter.  200]  loss:11.393204  pct:-6.182453965\n",
      "[Iter.  210]  loss:10.706355  pct:-6.028582095\n",
      "[Iter.  220]  loss:10.077107  pct:-5.877328557\n",
      "[Iter.  230]  loss:9.499900  pct:-5.727909217\n",
      "[Iter.  240]  loss:8.969889  pct:-5.579123829\n",
      "[Iter.  250]  loss:8.482715  pct:-5.431216051\n",
      "[Iter.  260]  loss:8.034552  pct:-5.283250125\n",
      "[Iter.  270]  loss:7.621999  pct:-5.134727813\n",
      "[Iter.  280]  loss:7.241994  pct:-4.985638892\n",
      "[Iter.  290]  loss:6.891761  pct:-4.836134990\n",
      "[Iter.  300]  loss:6.568827  pct:-4.685799994\n",
      "[Iter.  310]  loss:6.270957  pct:-4.534601874\n",
      "[Iter.  320]  loss:5.996094  pct:-4.383114785\n",
      "[Iter.  330]  loss:5.742393  pct:-4.231108128\n",
      "[Iter.  340]  loss:5.508169  pct:-4.078846302\n",
      "[Iter.  350]  loss:5.291877  pct:-3.926747665\n",
      "[Iter.  360]  loss:5.092119  pct:-3.774805095\n",
      "[Iter.  370]  loss:4.907573  pct:-3.624149120\n",
      "[Iter.  380]  loss:4.737092  pct:-3.473848969\n",
      "[Iter.  390]  loss:4.579595  pct:-3.324760094\n",
      "[Iter.  400]  loss:4.434043  pct:-3.178265629\n",
      "[Iter.  410]  loss:4.299520  pct:-3.033865864\n",
      "[Iter.  420]  loss:4.175217  pct:-2.891086996\n",
      "[Iter.  430]  loss:4.060339  pct:-2.751418588\n",
      "[Iter.  440]  loss:3.954181  pct:-2.614522658\n",
      "[Iter.  450]  loss:3.856067  pct:-2.481266698\n",
      "[Iter.  460]  loss:3.765368  pct:-2.352116590\n",
      "[Iter.  470]  loss:3.681531  pct:-2.226523257\n",
      "[Iter.  480]  loss:3.604046  pct:-2.104703694\n",
      "[Iter.  490]  loss:3.532422  pct:-1.987316611\n",
      "[Iter.  500]  loss:3.466227  pct:-1.873940507\n",
      "[Iter.  510]  loss:3.405045  pct:-1.765076341\n",
      "[Iter.  520]  loss:3.348507  pct:-1.660407649\n",
      "[Iter.  530]  loss:3.296259  pct:-1.560337033\n",
      "[Iter.  540]  loss:3.247966  pct:-1.465110086\n",
      "[Iter.  550]  loss:3.203337  pct:-1.374041095\n",
      "[Iter.  560]  loss:3.162091  pct:-1.287592747\n",
      "[Iter.  570]  loss:3.123970  pct:-1.205562457\n",
      "[Iter.  580]  loss:3.088738  pct:-1.127797772\n",
      "[Iter.  590]  loss:3.056182  pct:-1.054016767\n",
      "[Iter.  600]  loss:3.026099  pct:-0.984338488\n",
      "[Iter.  610]  loss:2.998297  pct:-0.918732342\n",
      "[Iter.  620]  loss:2.972607  pct:-0.856838121\n",
      "[Iter.  630]  loss:2.948869  pct:-0.798547195\n",
      "[Iter.  640]  loss:2.926920  pct:-0.744312911\n",
      "[Iter.  650]  loss:2.906629  pct:-0.693273611\n",
      "[Iter.  660]  loss:2.887880  pct:-0.645026345\n",
      "[Iter.  670]  loss:2.870556  pct:-0.599901855\n",
      "[Iter.  680]  loss:2.854542  pct:-0.557866190\n",
      "[Iter.  690]  loss:2.839747  pct:-0.518290725\n",
      "[Iter.  700]  loss:2.826079  pct:-0.481320970\n",
      "[Iter.  710]  loss:2.813457  pct:-0.446621629\n",
      "[Iter.  720]  loss:2.801788  pct:-0.414745349\n",
      "[Iter.  730]  loss:2.790997  pct:-0.385174656\n",
      "[Iter.  740]  loss:2.781030  pct:-0.357090138\n",
      "[Iter.  750]  loss:2.771813  pct:-0.331433378\n",
      "[Iter.  760]  loss:2.763289  pct:-0.307513634\n",
      "[Iter.  770]  loss:2.755400  pct:-0.285511526\n",
      "[Iter.  780]  loss:2.748102  pct:-0.264844184\n",
      "[Iter.  790]  loss:2.741352  pct:-0.245645292\n",
      "[Iter.  800]  loss:2.735110  pct:-0.227690545\n",
      "[Iter.  810]  loss:2.729338  pct:-0.211011585\n",
      "[Iter.  820]  loss:2.724004  pct:-0.195454536\n",
      "[Iter.  830]  loss:2.719068  pct:-0.181203119\n",
      "[Iter.  840]  loss:2.714500  pct:-0.168002429\n",
      "[Iter.  850]  loss:2.710276  pct:-0.155602284\n",
      "[Iter.  860]  loss:2.706363  pct:-0.144364908\n",
      "[Iter.  870]  loss:2.702741  pct:-0.133843583\n",
      "[Iter.  880]  loss:2.699383  pct:-0.124222430\n",
      "[Iter.  890]  loss:2.696276  pct:-0.115102983\n",
      "[Iter.  900]  loss:2.693403  pct:-0.106569960\n",
      "[Iter.  910]  loss:2.690745  pct:-0.098699198\n",
      "[Iter.  920]  loss:2.688286  pct:-0.091380311\n",
      "[Iter.  930]  loss:2.686011  pct:-0.084608311\n",
      "[Iter.  940]  loss:2.683908  pct:-0.078324523\n",
      "[Iter.  950]  loss:2.681958  pct:-0.072647330\n",
      "[Iter.  960]  loss:2.680152  pct:-0.067339642\n",
      "[Iter.  970]  loss:2.678485  pct:-0.062189923\n",
      "[Iter.  980]  loss:2.676940  pct:-0.057662283\n",
      "[Iter.  990]  loss:2.675521  pct:-0.053028607\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.654781  pct:100.000000000\n",
      "[Iter.    2]  loss:2.654780  pct:-0.000026942\n",
      "[Iter.    4]  loss:2.654780  pct:0.000008981\n",
      "[Iter.    6]  loss:2.654780  pct:-0.000008981\n",
      "[Iter.    8]  loss:2.654781  pct:0.000026942\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.654781\n",
      "Best loss: 2.654781 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 21%|██        | 2111/10000 [00:27<01:41, 77.42it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:429.928467  pct:100.000000000\n",
      "[Iter.   10]  loss:93.951912  pct:-78.147084561\n",
      "[Iter.   20]  loss:79.531860  pct:-15.348332226\n",
      "[Iter.   30]  loss:69.598686  pct:-12.489553356\n",
      "[Iter.   40]  loss:61.927479  pct:-11.022057807\n",
      "[Iter.   50]  loss:55.744953  pct:-9.983493201\n",
      "[Iter.   60]  loss:50.612534  pct:-9.206967260\n",
      "[Iter.   70]  loss:46.257927  pct:-8.603810798\n",
      "[Iter.   80]  loss:42.501278  pct:-8.121092461\n",
      "[Iter.   90]  loss:39.217838  pct:-7.725508024\n",
      "[Iter.  100]  loss:36.317795  pct:-7.394705099\n",
      "[Iter.  110]  loss:33.734394  pct:-7.113319354\n",
      "[Iter.  120]  loss:31.416758  pct:-6.870247869\n",
      "[Iter.  130]  loss:29.325140  pct:-6.657649436\n",
      "[Iter.  140]  loss:27.427944  pct:-6.469520064\n",
      "[Iter.  150]  loss:25.699749  pct:-6.300855722\n",
      "[Iter.  160]  loss:24.119587  pct:-6.148550512\n",
      "[Iter.  170]  loss:22.670170  pct:-6.009294925\n",
      "[Iter.  180]  loss:21.336950  pct:-5.880941952\n",
      "[Iter.  190]  loss:20.107590  pct:-5.761650859\n",
      "[Iter.  200]  loss:18.971558  pct:-5.649767676\n",
      "[Iter.  210]  loss:17.919769  pct:-5.544027282\n",
      "[Iter.  220]  loss:16.944340  pct:-5.443315253\n",
      "[Iter.  230]  loss:16.038326  pct:-5.346997888\n",
      "[Iter.  240]  loss:15.195736  pct:-5.253605134\n",
      "[Iter.  250]  loss:14.411160  pct:-5.163129090\n",
      "[Iter.  260]  loss:13.679817  pct:-5.074839538\n",
      "[Iter.  270]  loss:12.997407  pct:-4.988445607\n",
      "[Iter.  280]  loss:12.360188  pct:-4.902658487\n",
      "[Iter.  290]  loss:11.764683  pct:-4.817933927\n",
      "[Iter.  300]  loss:11.207807  pct:-4.733456851\n",
      "[Iter.  310]  loss:10.686713  pct:-4.649378667\n",
      "[Iter.  320]  loss:10.198863  pct:-4.565016196\n",
      "[Iter.  330]  loss:9.741863  pct:-4.480889462\n",
      "[Iter.  340]  loss:9.313609  pct:-4.396018672\n",
      "[Iter.  350]  loss:8.912122  pct:-4.310760148\n",
      "[Iter.  360]  loss:8.535592  pct:-4.224916391\n",
      "[Iter.  370]  loss:8.182399  pct:-4.137888500\n",
      "[Iter.  380]  loss:7.850958  pct:-4.050657196\n",
      "[Iter.  390]  loss:7.539873  pct:-3.962379527\n",
      "[Iter.  400]  loss:7.247830  pct:-3.873317287\n",
      "[Iter.  410]  loss:6.973606  pct:-3.783529798\n",
      "[Iter.  420]  loss:6.716081  pct:-3.692859046\n",
      "[Iter.  430]  loss:6.474174  pct:-3.601909087\n",
      "[Iter.  440]  loss:6.246918  pct:-3.510190445\n",
      "[Iter.  450]  loss:6.033410  pct:-3.417807977\n",
      "[Iter.  460]  loss:5.832777  pct:-3.325375259\n",
      "[Iter.  470]  loss:5.644234  pct:-3.232463382\n",
      "[Iter.  480]  loss:5.467060  pct:-3.139036449\n",
      "[Iter.  490]  loss:5.300564  pct:-3.045435972\n",
      "[Iter.  500]  loss:5.144043  pct:-2.952900337\n",
      "[Iter.  510]  loss:4.996941  pct:-2.859663949\n",
      "[Iter.  520]  loss:4.858665  pct:-2.767214936\n",
      "[Iter.  530]  loss:4.728695  pct:-2.675016161\n",
      "[Iter.  540]  loss:4.606514  pct:-2.583809361\n",
      "[Iter.  550]  loss:4.491664  pct:-2.493219597\n",
      "[Iter.  560]  loss:4.383687  pct:-2.403940167\n",
      "[Iter.  570]  loss:4.282177  pct:-2.315620860\n",
      "[Iter.  580]  loss:4.186759  pct:-2.228258346\n",
      "[Iter.  590]  loss:4.097070  pct:-2.142211784\n",
      "[Iter.  600]  loss:4.012760  pct:-2.057824913\n",
      "[Iter.  610]  loss:3.933499  pct:-1.975225798\n",
      "[Iter.  620]  loss:3.858983  pct:-1.894378259\n",
      "[Iter.  630]  loss:3.788943  pct:-1.815004051\n",
      "[Iter.  640]  loss:3.723103  pct:-1.737681850\n",
      "[Iter.  650]  loss:3.661209  pct:-1.662435382\n",
      "[Iter.  660]  loss:3.603020  pct:-1.589316695\n",
      "[Iter.  670]  loss:3.548335  pct:-1.517764201\n",
      "[Iter.  680]  loss:3.496924  pct:-1.448867538\n",
      "[Iter.  690]  loss:3.448606  pct:-1.381732692\n",
      "[Iter.  700]  loss:3.403189  pct:-1.316961945\n",
      "[Iter.  710]  loss:3.360502  pct:-1.254328584\n",
      "[Iter.  720]  loss:3.320372  pct:-1.194170856\n",
      "[Iter.  730]  loss:3.282664  pct:-1.135671536\n",
      "[Iter.  740]  loss:3.247225  pct:-1.079566538\n",
      "[Iter.  750]  loss:3.213910  pct:-1.025958511\n",
      "[Iter.  760]  loss:3.182599  pct:-0.974220175\n",
      "[Iter.  770]  loss:3.153175  pct:-0.924525813\n",
      "[Iter.  780]  loss:3.125509  pct:-0.877426850\n",
      "[Iter.  790]  loss:3.099511  pct:-0.831781449\n",
      "[Iter.  800]  loss:3.075075  pct:-0.788382292\n",
      "[Iter.  810]  loss:3.052115  pct:-0.746654468\n",
      "[Iter.  820]  loss:3.030534  pct:-0.707081312\n",
      "[Iter.  830]  loss:3.010251  pct:-0.669287445\n",
      "[Iter.  840]  loss:2.991180  pct:-0.633538594\n",
      "[Iter.  850]  loss:2.973262  pct:-0.599039464\n",
      "[Iter.  860]  loss:2.956425  pct:-0.566252442\n",
      "[Iter.  870]  loss:2.940599  pct:-0.535324299\n",
      "[Iter.  880]  loss:2.925721  pct:-0.505944413\n",
      "[Iter.  890]  loss:2.911750  pct:-0.477526378\n",
      "[Iter.  900]  loss:2.898610  pct:-0.451290105\n",
      "[Iter.  910]  loss:2.886263  pct:-0.425954029\n",
      "[Iter.  920]  loss:2.874651  pct:-0.402300901\n",
      "[Iter.  930]  loss:2.863742  pct:-0.379517170\n",
      "[Iter.  940]  loss:2.853484  pct:-0.358176310\n",
      "[Iter.  950]  loss:2.843841  pct:-0.337948800\n",
      "[Iter.  960]  loss:2.834779  pct:-0.318655351\n",
      "[Iter.  970]  loss:2.826255  pct:-0.300699962\n",
      "[Iter.  980]  loss:2.818243  pct:-0.283486691\n",
      "[Iter.  990]  loss:2.810707  pct:-0.267381421\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.688649  pct:100.000000000\n",
      "[Iter.    2]  loss:2.688647  pct:-0.000070941\n",
      "[Iter.    4]  loss:2.688646  pct:-0.000035470\n",
      "[Iter.    6]  loss:2.688645  pct:-0.000026603\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.688645\n",
      "Best loss: 2.688645 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  6%|▌         | 552/10000 [00:07<02:06, 74.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:864.879333  pct:100.000000000\n",
      "[Iter.   10]  loss:155.500168  pct:-82.020592112\n",
      "[Iter.   20]  loss:139.365845  pct:-10.375759296\n",
      "[Iter.   30]  loss:127.732933  pct:-8.347032018\n",
      "[Iter.   40]  loss:117.956993  pct:-7.653421642\n",
      "[Iter.   50]  loss:109.573402  pct:-7.107328254\n",
      "[Iter.   60]  loss:102.270592  pct:-6.664765818\n",
      "[Iter.   70]  loss:95.828445  pct:-6.299119025\n",
      "[Iter.   80]  loss:90.086624  pct:-5.991771298\n",
      "[Iter.   90]  loss:84.924614  pct:-5.730051761\n",
      "[Iter.  100]  loss:80.249901  pct:-5.504544463\n",
      "[Iter.  110]  loss:75.990112  pct:-5.308154240\n",
      "[Iter.  120]  loss:72.087402  pct:-5.135812861\n",
      "[Iter.  130]  loss:68.494995  pct:-4.983405019\n",
      "[Iter.  140]  loss:65.174622  pct:-4.847614821\n",
      "[Iter.  150]  loss:62.094448  pct:-4.726032032\n",
      "[Iter.  160]  loss:59.228062  pct:-4.616171819\n",
      "[Iter.  170]  loss:56.552883  pct:-4.516741646\n",
      "[Iter.  180]  loss:54.049732  pct:-4.426212777\n",
      "[Iter.  190]  loss:51.702183  pct:-4.343313727\n",
      "[Iter.  200]  loss:49.495892  pct:-4.267307646\n",
      "[Iter.  210]  loss:47.418499  pct:-4.197101036\n",
      "[Iter.  220]  loss:45.459145  pct:-4.132046442\n",
      "[Iter.  230]  loss:43.608181  pct:-4.071707924\n",
      "[Iter.  240]  loss:41.857059  pct:-4.015580290\n",
      "[Iter.  250]  loss:40.198383  pct:-3.962715413\n",
      "[Iter.  260]  loss:38.625259  pct:-3.913400992\n",
      "[Iter.  270]  loss:37.131649  pct:-3.866926476\n",
      "[Iter.  280]  loss:35.712116  pct:-3.822972622\n",
      "[Iter.  290]  loss:34.361664  pct:-3.781496493\n",
      "[Iter.  300]  loss:33.075806  pct:-3.742130070\n",
      "[Iter.  310]  loss:31.850498  pct:-3.704543064\n",
      "[Iter.  320]  loss:30.682018  pct:-3.668639379\n",
      "[Iter.  330]  loss:29.566978  pct:-3.634180174\n",
      "[Iter.  340]  loss:28.502277  pct:-3.600980337\n",
      "[Iter.  350]  loss:27.484962  pct:-3.569240793\n",
      "[Iter.  360]  loss:26.512428  pct:-3.538422805\n",
      "[Iter.  370]  loss:25.582157  pct:-3.508811561\n",
      "[Iter.  380]  loss:24.691950  pct:-3.479797602\n",
      "[Iter.  390]  loss:23.839649  pct:-3.451734874\n",
      "[Iter.  400]  loss:23.023302  pct:-3.424325229\n",
      "[Iter.  410]  loss:22.241079  pct:-3.397526320\n",
      "[Iter.  420]  loss:21.491343  pct:-3.370955046\n",
      "[Iter.  430]  loss:20.772467  pct:-3.344955689\n",
      "[Iter.  440]  loss:20.082943  pct:-3.319411740\n",
      "[Iter.  450]  loss:19.421352  pct:-3.294290968\n",
      "[Iter.  460]  loss:18.786440  pct:-3.269146650\n",
      "[Iter.  470]  loss:18.176941  pct:-3.244355935\n",
      "[Iter.  480]  loss:17.591677  pct:-3.219816847\n",
      "[Iter.  490]  loss:17.029594  pct:-3.195160415\n",
      "[Iter.  500]  loss:16.489616  pct:-3.170821418\n",
      "[Iter.  510]  loss:15.970763  pct:-3.146544924\n",
      "[Iter.  520]  loss:15.472153  pct:-3.122020470\n",
      "[Iter.  530]  loss:14.992922  pct:-3.097376879\n",
      "[Iter.  540]  loss:14.532219  pct:-3.072802629\n",
      "[Iter.  550]  loss:14.089263  pct:-3.048095909\n",
      "[Iter.  560]  loss:13.663265  pct:-3.023562944\n",
      "[Iter.  570]  loss:13.253635  pct:-2.998037548\n",
      "[Iter.  580]  loss:12.859590  pct:-2.973115056\n",
      "[Iter.  590]  loss:12.480522  pct:-2.947741207\n",
      "[Iter.  600]  loss:12.115790  pct:-2.922408086\n",
      "[Iter.  610]  loss:11.764891  pct:-2.896217958\n",
      "[Iter.  620]  loss:11.427231  pct:-2.870063524\n",
      "[Iter.  630]  loss:11.102265  pct:-2.843781505\n",
      "[Iter.  640]  loss:10.789477  pct:-2.817335017\n",
      "[Iter.  650]  loss:10.488418  pct:-2.790308679\n",
      "[Iter.  660]  loss:10.198645  pct:-2.762790325\n",
      "[Iter.  670]  loss:9.919682  pct:-2.735295707\n",
      "[Iter.  680]  loss:9.651139  pct:-2.707166439\n",
      "[Iter.  690]  loss:9.392605  pct:-2.678797026\n",
      "[Iter.  700]  loss:9.143693  pct:-2.650083360\n",
      "[Iter.  710]  loss:8.904029  pct:-2.621086234\n",
      "[Iter.  720]  loss:8.673221  pct:-2.592177775\n",
      "[Iter.  730]  loss:8.450993  pct:-2.562232181\n",
      "[Iter.  740]  loss:8.237006  pct:-2.532085961\n",
      "[Iter.  750]  loss:8.030905  pct:-2.502139890\n",
      "[Iter.  760]  loss:7.832446  pct:-2.471186962\n",
      "[Iter.  770]  loss:7.641318  pct:-2.440211545\n",
      "[Iter.  780]  loss:7.457240  pct:-2.408985208\n",
      "[Iter.  790]  loss:7.279951  pct:-2.377401574\n",
      "[Iter.  800]  loss:7.109217  pct:-2.345268578\n",
      "[Iter.  810]  loss:6.944750  pct:-2.313424787\n",
      "[Iter.  820]  loss:6.786369  pct:-2.280592607\n",
      "[Iter.  830]  loss:6.633830  pct:-2.247730070\n",
      "[Iter.  840]  loss:6.486907  pct:-2.214754888\n",
      "[Iter.  850]  loss:6.345359  pct:-2.182052096\n",
      "[Iter.  860]  loss:6.209029  pct:-2.148501463\n",
      "[Iter.  870]  loss:6.077703  pct:-2.115084411\n",
      "[Iter.  880]  loss:5.951237  pct:-2.080815850\n",
      "[Iter.  890]  loss:5.829422  pct:-2.046888912\n",
      "[Iter.  900]  loss:5.712061  pct:-2.013237757\n",
      "[Iter.  910]  loss:5.599056  pct:-1.978368755\n",
      "[Iter.  920]  loss:5.490182  pct:-1.944503657\n",
      "[Iter.  930]  loss:5.385330  pct:-1.909812845\n",
      "[Iter.  940]  loss:5.284326  pct:-1.875533199\n",
      "[Iter.  950]  loss:5.187034  pct:-1.841141992\n",
      "[Iter.  960]  loss:5.093338  pct:-1.806352437\n",
      "[Iter.  970]  loss:5.003073  pct:-1.772222339\n",
      "[Iter.  980]  loss:4.916135  pct:-1.737680663\n",
      "[Iter.  990]  loss:4.832393  pct:-1.703423792\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.649655  pct:100.000000000\n",
      "[Iter.    2]  loss:2.649656  pct:0.000026994\n",
      "[Iter.    4]  loss:2.649651  pct:-0.000170964\n",
      "[Iter.    6]  loss:2.649650  pct:-0.000044991\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.649650\n",
      "Best loss: 2.649650 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1813/10000 [00:25<01:56, 70.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:188.257370  pct:100.000000000\n",
      "[Iter.   10]  loss:37.326588  pct:-80.172577646\n",
      "[Iter.   20]  loss:29.520464  pct:-20.913038719\n",
      "[Iter.   30]  loss:24.383234  pct:-17.402266879\n",
      "[Iter.   40]  loss:20.651522  pct:-15.304419166\n",
      "[Iter.   50]  loss:17.775778  pct:-13.925094287\n",
      "[Iter.   60]  loss:15.482913  pct:-12.898815586\n",
      "[Iter.   70]  loss:13.612751  pct:-12.078876941\n",
      "[Iter.   80]  loss:12.062237  pct:-11.390160743\n",
      "[Iter.   90]  loss:10.761194  pct:-10.786080392\n",
      "[Iter.  100]  loss:9.659737  pct:-10.235458745\n",
      "[Iter.  110]  loss:8.720506  pct:-9.723152448\n",
      "[Iter.  120]  loss:7.915195  pct:-9.234673718\n",
      "[Iter.  130]  loss:7.221748  pct:-8.760966080\n",
      "[Iter.  140]  loss:6.622589  pct:-8.296595936\n",
      "[Iter.  150]  loss:6.103506  pct:-7.838068340\n",
      "[Iter.  160]  loss:5.652788  pct:-7.384566803\n",
      "[Iter.  170]  loss:5.260784  pct:-6.934710423\n",
      "[Iter.  180]  loss:4.919398  pct:-6.489258305\n",
      "[Iter.  190]  loss:4.621759  pct:-6.050311509\n",
      "[Iter.  200]  loss:4.362010  pct:-5.620131625\n",
      "[Iter.  210]  loss:4.135192  pct:-5.199841536\n",
      "[Iter.  220]  loss:3.937043  pct:-4.791777150\n",
      "[Iter.  230]  loss:3.763823  pct:-4.399764648\n",
      "[Iter.  240]  loss:3.612349  pct:-4.024453372\n",
      "[Iter.  250]  loss:3.479876  pct:-3.667238653\n",
      "[Iter.  260]  loss:3.363976  pct:-3.330566072\n",
      "[Iter.  270]  loss:3.262525  pct:-3.015824968\n",
      "[Iter.  280]  loss:3.173713  pct:-2.722160593\n",
      "[Iter.  290]  loss:3.095960  pct:-2.449930770\n",
      "[Iter.  300]  loss:3.027867  pct:-2.199401543\n",
      "[Iter.  310]  loss:2.968219  pct:-1.969984555\n",
      "[Iter.  320]  loss:2.915957  pct:-1.760713648\n",
      "[Iter.  330]  loss:2.870155  pct:-1.570716073\n",
      "[Iter.  340]  loss:2.830002  pct:-1.399001055\n",
      "[Iter.  350]  loss:2.794793  pct:-1.244122944\n",
      "[Iter.  360]  loss:2.763921  pct:-1.104637930\n",
      "[Iter.  370]  loss:2.736833  pct:-0.980054386\n",
      "[Iter.  380]  loss:2.713049  pct:-0.869039574\n",
      "[Iter.  390]  loss:2.692155  pct:-0.770123006\n",
      "[Iter.  400]  loss:2.673789  pct:-0.682199233\n",
      "[Iter.  410]  loss:2.657640  pct:-0.603993835\n",
      "[Iter.  420]  loss:2.643432  pct:-0.534585790\n",
      "[Iter.  430]  loss:2.630914  pct:-0.473566381\n",
      "[Iter.  440]  loss:2.619872  pct:-0.419697518\n",
      "[Iter.  450]  loss:2.610136  pct:-0.371623562\n",
      "[Iter.  460]  loss:2.601543  pct:-0.329192343\n",
      "[Iter.  470]  loss:2.593956  pct:-0.291642045\n",
      "[Iter.  480]  loss:2.587254  pct:-0.258367746\n",
      "[Iter.  490]  loss:2.581324  pct:-0.229216887\n",
      "[Iter.  500]  loss:2.576066  pct:-0.203706394\n",
      "[Iter.  510]  loss:2.571405  pct:-0.180910303\n",
      "[Iter.  520]  loss:2.567273  pct:-0.160700883\n",
      "[Iter.  530]  loss:2.563598  pct:-0.143128810\n",
      "[Iter.  540]  loss:2.560332  pct:-0.127402799\n",
      "[Iter.  550]  loss:2.557427  pct:-0.113476239\n",
      "[Iter.  560]  loss:2.554842  pct:-0.101066263\n",
      "[Iter.  570]  loss:2.552539  pct:-0.090166049\n",
      "[Iter.  580]  loss:2.550472  pct:-0.080953675\n",
      "[Iter.  590]  loss:2.548626  pct:-0.072391043\n",
      "[Iter.  600]  loss:2.546978  pct:-0.064641592\n",
      "[Iter.  610]  loss:2.545511  pct:-0.057615970\n",
      "[Iter.  620]  loss:2.544198  pct:-0.051598596\n",
      "[Iter.  630]  loss:2.543006  pct:-0.046845988\n",
      "[Iter.  640]  loss:2.541936  pct:-0.042067706\n",
      "[Iter.  650]  loss:2.540972  pct:-0.037939711\n",
      "[Iter.  660]  loss:2.540098  pct:-0.034360434\n",
      "[Iter.  670]  loss:2.539315  pct:-0.030843035\n",
      "[Iter.  680]  loss:2.538611  pct:-0.027735373\n",
      "[Iter.  690]  loss:2.537963  pct:-0.025526628\n",
      "[Iter.  700]  loss:2.537371  pct:-0.023297351\n",
      "[Iter.  710]  loss:2.536836  pct:-0.021094654\n",
      "[Iter.  720]  loss:2.536352  pct:-0.019087876\n",
      "[Iter.  730]  loss:2.535912  pct:-0.017343109\n",
      "[Iter.  740]  loss:2.535517  pct:-0.015569198\n",
      "[Iter.  750]  loss:2.535153  pct:-0.014368019\n",
      "[Iter.  760]  loss:2.534819  pct:-0.013156902\n",
      "[Iter.  770]  loss:2.534517  pct:-0.011926481\n",
      "[Iter.  780]  loss:2.534246  pct:-0.010695605\n",
      "[Iter.  790]  loss:2.534005  pct:-0.009520765\n",
      "[Iter.  800]  loss:2.533787  pct:-0.008599612\n",
      "[Iter.  810]  loss:2.533580  pct:-0.008167512\n",
      "[Iter.  820]  loss:2.533377  pct:-0.008017613\n",
      "[Iter.  830]  loss:2.533189  pct:-0.007406535\n",
      "[Iter.  840]  loss:2.533016  pct:-0.006842375\n",
      "[Iter.  850]  loss:2.532857  pct:-0.006278097\n",
      "[Iter.  860]  loss:2.532710  pct:-0.005807840\n",
      "[Iter.  870]  loss:2.532575  pct:-0.005299844\n",
      "[Iter.  880]  loss:2.532451  pct:-0.004904734\n",
      "[Iter.  890]  loss:2.532337  pct:-0.004518978\n",
      "[Iter.  900]  loss:2.532231  pct:-0.004161414\n",
      "[Iter.  910]  loss:2.532135  pct:-0.003794388\n",
      "[Iter.  920]  loss:2.532047  pct:-0.003502645\n",
      "[Iter.  930]  loss:2.531964  pct:-0.003267367\n",
      "[Iter.  940]  loss:2.531886  pct:-0.003069730\n",
      "[Iter.  950]  loss:2.531815  pct:-0.002806158\n",
      "[Iter.  960]  loss:2.531750  pct:-0.002561398\n",
      "[Iter.  970]  loss:2.531693  pct:-0.002278949\n",
      "[Iter.  980]  loss:2.531642  pct:-0.001996480\n",
      "[Iter.  990]  loss:2.531594  pct:-0.001902345\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.530169  pct:100.000000000\n",
      "[Iter.    2]  loss:2.530164  pct:-0.000160192\n",
      "[Iter.    4]  loss:2.530163  pct:-0.000065961\n",
      "[Iter.    6]  loss:2.530162  pct:-0.000028269\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.530162\n",
      "Best loss: 2.530162 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  4%|▍         | 409/10000 [00:04<01:55, 82.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:2714.304932  pct:100.000000000\n",
      "[Iter.   10]  loss:333.568085  pct:-87.710735395\n",
      "[Iter.   20]  loss:321.635956  pct:-3.577119471\n",
      "[Iter.   30]  loss:311.279480  pct:-3.219937213\n",
      "[Iter.   40]  loss:301.592316  pct:-3.112047189\n",
      "[Iter.   50]  loss:292.667725  pct:-2.959157313\n",
      "[Iter.   60]  loss:284.344757  pct:-2.843828284\n",
      "[Iter.   70]  loss:276.472473  pct:-2.768570104\n",
      "[Iter.   80]  loss:268.984955  pct:-2.708232840\n",
      "[Iter.   90]  loss:261.848114  pct:-2.653249073\n",
      "[Iter.  100]  loss:255.036972  pct:-2.601180457\n",
      "[Iter.  110]  loss:248.527634  pct:-2.552311662\n",
      "[Iter.  120]  loss:242.299057  pct:-2.506190788\n",
      "[Iter.  130]  loss:236.332489  pct:-2.462480897\n",
      "[Iter.  140]  loss:230.610092  pct:-2.421333129\n",
      "[Iter.  150]  loss:225.115921  pct:-2.382450434\n",
      "[Iter.  160]  loss:219.835403  pct:-2.345688192\n",
      "[Iter.  170]  loss:214.755325  pct:-2.310855324\n",
      "[Iter.  180]  loss:209.863449  pct:-2.277883547\n",
      "[Iter.  190]  loss:205.148758  pct:-2.246551833\n",
      "[Iter.  200]  loss:200.600937  pct:-2.216840643\n",
      "[Iter.  210]  loss:196.210800  pct:-2.188492630\n",
      "[Iter.  220]  loss:191.969681  pct:-2.161511691\n",
      "[Iter.  230]  loss:187.869370  pct:-2.135916079\n",
      "[Iter.  240]  loss:183.902618  pct:-2.111441109\n",
      "[Iter.  250]  loss:180.062363  pct:-2.088200685\n",
      "[Iter.  260]  loss:176.342346  pct:-2.065960051\n",
      "[Iter.  270]  loss:172.736893  pct:-2.044576115\n",
      "[Iter.  280]  loss:169.240356  pct:-2.024197727\n",
      "[Iter.  290]  loss:165.847519  pct:-2.004744965\n",
      "[Iter.  300]  loss:162.553574  pct:-1.986128785\n",
      "[Iter.  310]  loss:159.354172  pct:-1.968213792\n",
      "[Iter.  320]  loss:156.245056  pct:-1.951072612\n",
      "[Iter.  330]  loss:153.222198  pct:-1.934690121\n",
      "[Iter.  340]  loss:150.282166  pct:-1.918803534\n",
      "[Iter.  350]  loss:147.421371  pct:-1.903615148\n",
      "[Iter.  360]  loss:144.636627  pct:-1.888969174\n",
      "[Iter.  370]  loss:141.924789  pct:-1.874931559\n",
      "[Iter.  380]  loss:139.283035  pct:-1.861376128\n",
      "[Iter.  390]  loss:136.708603  pct:-1.848345973\n",
      "[Iter.  400]  loss:134.198883  pct:-1.835817056\n",
      "[Iter.  410]  loss:131.751480  pct:-1.823713356\n",
      "[Iter.  420]  loss:129.364136  pct:-1.812005724\n",
      "[Iter.  430]  loss:127.034698  pct:-1.800682424\n",
      "[Iter.  440]  loss:124.760986  pct:-1.789835522\n",
      "[Iter.  450]  loss:122.541222  pct:-1.779213819\n",
      "[Iter.  460]  loss:120.373367  pct:-1.769081686\n",
      "[Iter.  470]  loss:118.255722  pct:-1.759230726\n",
      "[Iter.  480]  loss:116.186577  pct:-1.749721000\n",
      "[Iter.  490]  loss:114.164330  pct:-1.740517166\n",
      "[Iter.  500]  loss:112.187462  pct:-1.731598376\n",
      "[Iter.  510]  loss:110.254578  pct:-1.722905737\n",
      "[Iter.  520]  loss:108.364182  pct:-1.714573815\n",
      "[Iter.  530]  loss:106.514877  pct:-1.706564082\n",
      "[Iter.  540]  loss:104.705513  pct:-1.698696336\n",
      "[Iter.  550]  loss:102.934883  pct:-1.691056977\n",
      "[Iter.  560]  loss:101.201958  pct:-1.683516183\n",
      "[Iter.  570]  loss:99.505463  pct:-1.676346085\n",
      "[Iter.  580]  loss:97.844322  pct:-1.669396230\n",
      "[Iter.  590]  loss:96.217674  pct:-1.662485786\n",
      "[Iter.  600]  loss:94.624458  pct:-1.655845410\n",
      "[Iter.  610]  loss:93.063713  pct:-1.649409959\n",
      "[Iter.  620]  loss:91.534386  pct:-1.643312245\n",
      "[Iter.  630]  loss:90.035828  pct:-1.637153113\n",
      "[Iter.  640]  loss:88.567245  pct:-1.631108628\n",
      "[Iter.  650]  loss:87.127640  pct:-1.625438056\n",
      "[Iter.  660]  loss:85.716400  pct:-1.619738154\n",
      "[Iter.  670]  loss:84.332664  pct:-1.614318444\n",
      "[Iter.  680]  loss:82.975822  pct:-1.608916366\n",
      "[Iter.  690]  loss:81.645073  pct:-1.603779839\n",
      "[Iter.  700]  loss:80.339928  pct:-1.598559738\n",
      "[Iter.  710]  loss:79.059532  pct:-1.593722505\n",
      "[Iter.  720]  loss:77.803452  pct:-1.588778219\n",
      "[Iter.  730]  loss:76.571053  pct:-1.583990122\n",
      "[Iter.  740]  loss:75.361679  pct:-1.579413413\n",
      "[Iter.  750]  loss:74.174805  pct:-1.574904387\n",
      "[Iter.  760]  loss:73.009850  pct:-1.570553700\n",
      "[Iter.  770]  loss:71.866325  pct:-1.566260137\n",
      "[Iter.  780]  loss:70.743843  pct:-1.561903011\n",
      "[Iter.  790]  loss:69.641815  pct:-1.557772161\n",
      "[Iter.  800]  loss:68.559731  pct:-1.553785830\n",
      "[Iter.  810]  loss:67.497162  pct:-1.549843700\n",
      "[Iter.  820]  loss:66.453720  pct:-1.545904663\n",
      "[Iter.  830]  loss:65.428970  pct:-1.542050249\n",
      "[Iter.  840]  loss:64.422462  pct:-1.538321432\n",
      "[Iter.  850]  loss:63.433815  pct:-1.534631591\n",
      "[Iter.  860]  loss:62.462524  pct:-1.531187409\n",
      "[Iter.  870]  loss:61.508446  pct:-1.527441747\n",
      "[Iter.  880]  loss:60.571018  pct:-1.524063093\n",
      "[Iter.  890]  loss:59.649982  pct:-1.520588218\n",
      "[Iter.  900]  loss:58.744919  pct:-1.517290688\n",
      "[Iter.  910]  loss:57.855461  pct:-1.514101509\n",
      "[Iter.  920]  loss:56.981407  pct:-1.510754453\n",
      "[Iter.  930]  loss:56.122387  pct:-1.507544787\n",
      "[Iter.  940]  loss:55.278088  pct:-1.504389536\n",
      "[Iter.  950]  loss:54.448147  pct:-1.501392019\n",
      "[Iter.  960]  loss:53.632309  pct:-1.498375808\n",
      "[Iter.  970]  loss:52.830379  pct:-1.495235781\n",
      "[Iter.  980]  loss:52.041931  pct:-1.492414670\n",
      "[Iter.  990]  loss:51.266788  pct:-1.489457928\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.650975  pct:100.000000000\n",
      "[Iter.    2]  loss:2.650604  pct:-0.013958099\n",
      "[Iter.    4]  loss:2.650513  pct:-0.003445037\n",
      "[Iter.    6]  loss:2.650493  pct:-0.000773586\n",
      "[Iter.    8]  loss:2.650468  pct:-0.000935507\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.650468\n",
      "Best loss: 2.650468 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 21%|██▏       | 2130/10000 [00:23<01:26, 91.02it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:414.174194  pct:100.000000000\n",
      "[Iter.   10]  loss:80.184708  pct:-80.639862773\n",
      "[Iter.   20]  loss:68.331734  pct:-14.782087865\n",
      "[Iter.   30]  loss:59.786644  pct:-12.505302088\n",
      "[Iter.   40]  loss:53.244591  pct:-10.942332245\n",
      "[Iter.   50]  loss:47.946648  pct:-9.950199710\n",
      "[Iter.   60]  loss:43.523808  pct:-9.224503351\n",
      "[Iter.   70]  loss:39.754326  pct:-8.660735063\n",
      "[Iter.   80]  loss:36.491196  pct:-8.208239272\n",
      "[Iter.   90]  loss:33.632210  pct:-7.834727933\n",
      "[Iter.  100]  loss:31.103127  pct:-7.519824801\n",
      "[Iter.  110]  loss:28.848230  pct:-7.249741154\n",
      "[Iter.  120]  loss:26.824657  pct:-7.014547847\n",
      "[Iter.  130]  loss:24.998760  pct:-6.806786707\n",
      "[Iter.  140]  loss:23.343582  pct:-6.621040625\n",
      "[Iter.  150]  loss:21.837225  pct:-6.452982165\n",
      "[Iter.  160]  loss:20.461607  pct:-6.299417547\n",
      "[Iter.  170]  loss:19.201763  pct:-6.157110864\n",
      "[Iter.  180]  loss:18.044987  pct:-6.024324011\n",
      "[Iter.  190]  loss:16.980469  pct:-5.899244987\n",
      "[Iter.  200]  loss:15.998949  pct:-5.780286243\n",
      "[Iter.  210]  loss:15.092420  pct:-5.666181095\n",
      "[Iter.  220]  loss:14.253871  pct:-5.556091609\n",
      "[Iter.  230]  loss:13.477163  pct:-5.449099765\n",
      "[Iter.  240]  loss:12.756863  pct:-5.344601513\n",
      "[Iter.  250]  loss:12.088223  pct:-5.241407718\n",
      "[Iter.  260]  loss:11.466944  pct:-5.139545266\n",
      "[Iter.  270]  loss:10.889172  pct:-5.038588778\n",
      "[Iter.  280]  loss:10.351483  pct:-4.937825163\n",
      "[Iter.  290]  loss:9.850734  pct:-4.837466973\n",
      "[Iter.  300]  loss:9.384161  pct:-4.736426474\n",
      "[Iter.  310]  loss:8.949182  pct:-4.635251239\n",
      "[Iter.  320]  loss:8.543503  pct:-4.533137992\n",
      "[Iter.  330]  loss:8.164998  pct:-4.430322804\n",
      "[Iter.  340]  loss:7.811702  pct:-4.326955141\n",
      "[Iter.  350]  loss:7.481826  pct:-4.222843312\n",
      "[Iter.  360]  loss:7.173728  pct:-4.117949639\n",
      "[Iter.  370]  loss:6.885894  pct:-4.012337083\n",
      "[Iter.  380]  loss:6.616940  pct:-3.905872867\n",
      "[Iter.  390]  loss:6.365578  pct:-3.798762662\n",
      "[Iter.  400]  loss:6.130615  pct:-3.691155942\n",
      "[Iter.  410]  loss:5.910963  pct:-3.582873571\n",
      "[Iter.  420]  loss:5.705573  pct:-3.474721703\n",
      "[Iter.  430]  loss:5.513515  pct:-3.366157272\n",
      "[Iter.  440]  loss:5.333904  pct:-3.257644985\n",
      "[Iter.  450]  loss:5.165932  pct:-3.149130874\n",
      "[Iter.  460]  loss:5.008808  pct:-3.041552113\n",
      "[Iter.  470]  loss:4.861850  pct:-2.933989171\n",
      "[Iter.  480]  loss:4.724369  pct:-2.827745444\n",
      "[Iter.  490]  loss:4.595771  pct:-2.722008696\n",
      "[Iter.  500]  loss:4.475463  pct:-2.617806479\n",
      "[Iter.  510]  loss:4.362903  pct:-2.515053178\n",
      "[Iter.  520]  loss:4.257612  pct:-2.413322011\n",
      "[Iter.  530]  loss:4.159107  pct:-2.313621483\n",
      "[Iter.  540]  loss:4.066956  pct:-2.215647998\n",
      "[Iter.  550]  loss:3.980742  pct:-2.119855683\n",
      "[Iter.  560]  loss:3.900082  pct:-2.026246071\n",
      "[Iter.  570]  loss:3.824620  pct:-1.934890973\n",
      "[Iter.  580]  loss:3.754019  pct:-1.845967044\n",
      "[Iter.  590]  loss:3.687965  pct:-1.759557146\n",
      "[Iter.  600]  loss:3.626176  pct:-1.675417277\n",
      "[Iter.  610]  loss:3.568370  pct:-1.594138159\n",
      "[Iter.  620]  loss:3.514281  pct:-1.515772181\n",
      "[Iter.  630]  loss:3.463685  pct:-1.439738930\n",
      "[Iter.  640]  loss:3.416341  pct:-1.366853374\n",
      "[Iter.  650]  loss:3.372056  pct:-1.296292445\n",
      "[Iter.  660]  loss:3.330632  pct:-1.228422307\n",
      "[Iter.  670]  loss:3.291872  pct:-1.163748503\n",
      "[Iter.  680]  loss:3.255614  pct:-1.101454011\n",
      "[Iter.  690]  loss:3.221695  pct:-1.041842884\n",
      "[Iter.  700]  loss:3.189963  pct:-0.984957192\n",
      "[Iter.  710]  loss:3.160273  pct:-0.930747570\n",
      "[Iter.  720]  loss:3.132499  pct:-0.878836171\n",
      "[Iter.  730]  loss:3.106520  pct:-0.829339202\n",
      "[Iter.  740]  loss:3.082214  pct:-0.782420824\n",
      "[Iter.  750]  loss:3.059467  pct:-0.738002042\n",
      "[Iter.  760]  loss:3.038188  pct:-0.695516443\n",
      "[Iter.  770]  loss:3.018281  pct:-0.655241698\n",
      "[Iter.  780]  loss:2.999654  pct:-0.617113387\n",
      "[Iter.  790]  loss:2.982226  pct:-0.581005610\n",
      "[Iter.  800]  loss:2.965920  pct:-0.546786178\n",
      "[Iter.  810]  loss:2.950660  pct:-0.514510928\n",
      "[Iter.  820]  loss:2.936383  pct:-0.483857258\n",
      "[Iter.  830]  loss:2.923020  pct:-0.455071670\n",
      "[Iter.  840]  loss:2.910519  pct:-0.427690459\n",
      "[Iter.  850]  loss:2.898813  pct:-0.402175694\n",
      "[Iter.  860]  loss:2.887861  pct:-0.377801414\n",
      "[Iter.  870]  loss:2.877613  pct:-0.354879299\n",
      "[Iter.  880]  loss:2.868018  pct:-0.333449769\n",
      "[Iter.  890]  loss:2.859032  pct:-0.313309012\n",
      "[Iter.  900]  loss:2.850627  pct:-0.293962907\n",
      "[Iter.  910]  loss:2.842757  pct:-0.276111585\n",
      "[Iter.  920]  loss:2.835388  pct:-0.259196529\n",
      "[Iter.  930]  loss:2.828491  pct:-0.243246152\n",
      "[Iter.  940]  loss:2.822033  pct:-0.228338022\n",
      "[Iter.  950]  loss:2.815981  pct:-0.214447509\n",
      "[Iter.  960]  loss:2.810315  pct:-0.201209373\n",
      "[Iter.  970]  loss:2.805007  pct:-0.188881013\n",
      "[Iter.  980]  loss:2.800033  pct:-0.177304799\n",
      "[Iter.  990]  loss:2.795375  pct:-0.166380128\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.722488  pct:100.000000000\n",
      "[Iter.    2]  loss:2.722495  pct:0.000262721\n",
      "[Iter.    4]  loss:2.722497  pct:0.000052544\n",
      "[Iter.    6]  loss:2.722497  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.722497\n",
      "Best loss: 2.722497 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 19%|█▉        | 1885/10000 [00:30<02:12, 61.08it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:4241.005371  pct:100.000000000\n",
      "[Iter.   10]  loss:760.906921  pct:-82.058334856\n",
      "[Iter.   20]  loss:733.191528  pct:-3.642415687\n",
      "[Iter.   30]  loss:708.829956  pct:-3.322675089\n",
      "[Iter.   40]  loss:686.628357  pct:-3.132147412\n",
      "[Iter.   50]  loss:665.975037  pct:-3.007932909\n",
      "[Iter.   60]  loss:646.659729  pct:-2.900305050\n",
      "[Iter.   70]  loss:628.547668  pct:-2.800864154\n",
      "[Iter.   80]  loss:611.525085  pct:-2.708240578\n",
      "[Iter.   90]  loss:595.492615  pct:-2.621719220\n",
      "[Iter.  100]  loss:580.361938  pct:-2.540867157\n",
      "[Iter.  110]  loss:566.053833  pct:-2.465376263\n",
      "[Iter.  120]  loss:552.497253  pct:-2.394927620\n",
      "[Iter.  130]  loss:539.628235  pct:-2.329245707\n",
      "[Iter.  140]  loss:527.389526  pct:-2.267988905\n",
      "[Iter.  150]  loss:515.730164  pct:-2.210768741\n",
      "[Iter.  160]  loss:504.604675  pct:-2.157230480\n",
      "[Iter.  170]  loss:493.971558  pct:-2.107217431\n",
      "[Iter.  180]  loss:483.794312  pct:-2.060289897\n",
      "[Iter.  190]  loss:474.040161  pct:-2.016177156\n",
      "[Iter.  200]  loss:464.679199  pct:-1.974719166\n",
      "[Iter.  210]  loss:455.684631  pct:-1.935651066\n",
      "[Iter.  220]  loss:447.031921  pct:-1.898837346\n",
      "[Iter.  230]  loss:438.698761  pct:-1.864108580\n",
      "[Iter.  240]  loss:430.665710  pct:-1.831108554\n",
      "[Iter.  250]  loss:422.914612  pct:-1.799794700\n",
      "[Iter.  260]  loss:415.429169  pct:-1.769965593\n",
      "[Iter.  270]  loss:408.194122  pct:-1.741583628\n",
      "[Iter.  280]  loss:401.195435  pct:-1.714548878\n",
      "[Iter.  290]  loss:394.419708  pct:-1.688884203\n",
      "[Iter.  300]  loss:387.855164  pct:-1.664355138\n",
      "[Iter.  310]  loss:381.490387  pct:-1.641018919\n",
      "[Iter.  320]  loss:375.314728  pct:-1.618824324\n",
      "[Iter.  330]  loss:369.317993  pct:-1.597788249\n",
      "[Iter.  340]  loss:363.491302  pct:-1.577689358\n",
      "[Iter.  350]  loss:357.824921  pct:-1.558876869\n",
      "[Iter.  360]  loss:352.313293  pct:-1.540313958\n",
      "[Iter.  370]  loss:346.948669  pct:-1.522685667\n",
      "[Iter.  380]  loss:341.724213  pct:-1.505829896\n",
      "[Iter.  390]  loss:336.633514  pct:-1.489709553\n",
      "[Iter.  400]  loss:331.669830  pct:-1.474506806\n",
      "[Iter.  410]  loss:326.826935  pct:-1.460155572\n",
      "[Iter.  420]  loss:322.102600  pct:-1.445515719\n",
      "[Iter.  430]  loss:317.490204  pct:-1.431964920\n",
      "[Iter.  440]  loss:312.983063  pct:-1.419615805\n",
      "[Iter.  450]  loss:308.582001  pct:-1.406166191\n",
      "[Iter.  460]  loss:304.280914  pct:-1.393822846\n",
      "[Iter.  470]  loss:300.074799  pct:-1.382313357\n",
      "[Iter.  480]  loss:295.960724  pct:-1.371016402\n",
      "[Iter.  490]  loss:291.929260  pct:-1.362161698\n",
      "[Iter.  500]  loss:287.988495  pct:-1.349904212\n",
      "[Iter.  510]  loss:284.132904  pct:-1.338800295\n",
      "[Iter.  520]  loss:280.355286  pct:-1.329525146\n",
      "[Iter.  530]  loss:276.656128  pct:-1.319453531\n",
      "[Iter.  540]  loss:273.031677  pct:-1.310092320\n",
      "[Iter.  550]  loss:269.479431  pct:-1.301038081\n",
      "[Iter.  560]  loss:265.997314  pct:-1.292164186\n",
      "[Iter.  570]  loss:262.583008  pct:-1.283586884\n",
      "[Iter.  580]  loss:259.234344  pct:-1.275278000\n",
      "[Iter.  590]  loss:255.949661  pct:-1.267071010\n",
      "[Iter.  600]  loss:252.726746  pct:-1.259199029\n",
      "[Iter.  610]  loss:249.563797  pct:-1.251529038\n",
      "[Iter.  620]  loss:246.458984  pct:-1.244095762\n",
      "[Iter.  630]  loss:243.410446  pct:-1.236935312\n",
      "[Iter.  640]  loss:240.416656  pct:-1.229934754\n",
      "[Iter.  650]  loss:237.476440  pct:-1.222966872\n",
      "[Iter.  660]  loss:234.587906  pct:-1.216345731\n",
      "[Iter.  670]  loss:231.749527  pct:-1.209942557\n",
      "[Iter.  680]  loss:228.959793  pct:-1.203771124\n",
      "[Iter.  690]  loss:226.216858  pct:-1.197998628\n",
      "[Iter.  700]  loss:223.520630  pct:-1.191877587\n",
      "[Iter.  710]  loss:220.869827  pct:-1.185931971\n",
      "[Iter.  720]  loss:218.263000  pct:-1.180254820\n",
      "[Iter.  730]  loss:215.698975  pct:-1.174741423\n",
      "[Iter.  740]  loss:213.176529  pct:-1.169428683\n",
      "[Iter.  750]  loss:210.694794  pct:-1.164169077\n",
      "[Iter.  760]  loss:208.252579  pct:-1.159124496\n",
      "[Iter.  770]  loss:205.848999  pct:-1.154165642\n",
      "[Iter.  780]  loss:203.483109  pct:-1.149333013\n",
      "[Iter.  790]  loss:201.153885  pct:-1.144676651\n",
      "[Iter.  800]  loss:198.860519  pct:-1.140104990\n",
      "[Iter.  810]  loss:196.602158  pct:-1.135651171\n",
      "[Iter.  820]  loss:194.377762  pct:-1.131419807\n",
      "[Iter.  830]  loss:192.186584  pct:-1.127277806\n",
      "[Iter.  840]  loss:190.028091  pct:-1.123123681\n",
      "[Iter.  850]  loss:187.899582  pct:-1.120102562\n",
      "[Iter.  860]  loss:185.803391  pct:-1.115591309\n",
      "[Iter.  870]  loss:183.738480  pct:-1.111341878\n",
      "[Iter.  880]  loss:181.703293  pct:-1.107654081\n",
      "[Iter.  890]  loss:179.697220  pct:-1.104037779\n",
      "[Iter.  900]  loss:177.719589  pct:-1.100534898\n",
      "[Iter.  910]  loss:175.769699  pct:-1.097172318\n",
      "[Iter.  920]  loss:173.847198  pct:-1.093761109\n",
      "[Iter.  930]  loss:171.951263  pct:-1.090575560\n",
      "[Iter.  940]  loss:170.081482  pct:-1.087390378\n",
      "[Iter.  950]  loss:168.237335  pct:-1.084272495\n",
      "[Iter.  960]  loss:166.418228  pct:-1.081274292\n",
      "[Iter.  970]  loss:164.623779  pct:-1.078276624\n",
      "[Iter.  980]  loss:162.853455  pct:-1.075376057\n",
      "[Iter.  990]  loss:161.106781  pct:-1.072543158\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.998802  pct:100.000000000\n",
      "[Iter.    2]  loss:2.996719  pct:-0.069471117\n",
      "[Iter.    4]  loss:2.995712  pct:-0.033622000\n",
      "[Iter.    6]  loss:2.995095  pct:-0.020589060\n",
      "[Iter.    8]  loss:2.994514  pct:-0.019383335\n",
      "[Iter.   10]  loss:2.994079  pct:-0.014546291\n",
      "[Iter.   12]  loss:2.993683  pct:-0.013218585\n",
      "[Iter.   14]  loss:2.993279  pct:-0.013475183\n",
      "[Iter.   16]  loss:2.992898  pct:-0.012728277\n",
      "[Iter.   18]  loss:2.992576  pct:-0.010762260\n",
      "[Iter.   20]  loss:2.992288  pct:-0.009640071\n",
      "[Iter.   22]  loss:2.991979  pct:-0.010334196\n",
      "[Iter.   24]  loss:2.991698  pct:-0.009394970\n",
      "[Iter.   26]  loss:2.991440  pct:-0.008598919\n",
      "[Iter.   28]  loss:2.991209  pct:-0.007738896\n",
      "[Iter.   30]  loss:2.991023  pct:-0.006201160\n",
      "[Iter.   32]  loss:2.990766  pct:-0.008600857\n",
      "[Iter.   34]  loss:2.990583  pct:-0.006114388\n",
      "[Iter.   36]  loss:2.990413  pct:-0.005700202\n",
      "[Iter.   38]  loss:2.990207  pct:-0.006880496\n",
      "[Iter.   40]  loss:2.990069  pct:-0.004608575\n",
      "[Iter.   42]  loss:2.989932  pct:-0.004584867\n",
      "[Iter.   44]  loss:2.989768  pct:-0.005486144\n",
      "[Iter.   46]  loss:2.989631  pct:-0.004569379\n",
      "[Iter.   48]  loss:2.989530  pct:-0.003381336\n",
      "New best!\n",
      "Trial 0 loss: 2.989530\n",
      "Best loss: 2.989530 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:39<00:00, 62.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:329.083984  pct:100.000000000\n",
      "[Iter.   10]  loss:73.604317  pct:-77.633579093\n",
      "[Iter.   20]  loss:60.727413  pct:-17.494766760\n",
      "[Iter.   30]  loss:52.413395  pct:-13.690716951\n",
      "[Iter.   40]  loss:46.124863  pct:-11.997948741\n",
      "[Iter.   50]  loss:41.122444  pct:-10.845384091\n",
      "[Iter.   60]  loss:37.010262  pct:-9.999849722\n",
      "[Iter.   70]  loss:33.549934  pct:-9.349642518\n",
      "[Iter.   80]  loss:30.586906  pct:-8.831695227\n",
      "[Iter.   90]  loss:28.015354  pct:-8.407363073\n",
      "[Iter.  100]  loss:25.759737  pct:-8.051360440\n",
      "[Iter.  110]  loss:23.764221  pct:-7.746646723\n",
      "[Iter.  120]  loss:21.986443  pct:-7.480904218\n",
      "[Iter.  130]  loss:20.393398  pct:-7.245575432\n",
      "[Iter.  140]  loss:18.958902  pct:-7.034119110\n",
      "[Iter.  150]  loss:17.661890  pct:-6.841178379\n",
      "[Iter.  160]  loss:16.485048  pct:-6.663169875\n",
      "[Iter.  170]  loss:15.414094  pct:-6.496519171\n",
      "[Iter.  180]  loss:14.436947  pct:-6.339309363\n",
      "[Iter.  190]  loss:13.543410  pct:-6.189234994\n",
      "[Iter.  200]  loss:12.724757  pct:-6.044660012\n",
      "[Iter.  210]  loss:11.973424  pct:-5.904499592\n",
      "[Iter.  220]  loss:11.282901  pct:-5.767131858\n",
      "[Iter.  230]  loss:10.647433  pct:-5.632129006\n",
      "[Iter.  240]  loss:10.061954  pct:-5.498778600\n",
      "[Iter.  250]  loss:9.522004  pct:-5.366257330\n",
      "[Iter.  260]  loss:9.023587  pct:-5.234369718\n",
      "[Iter.  270]  loss:8.563202  pct:-5.102021081\n",
      "[Iter.  280]  loss:8.137648  pct:-4.969569564\n",
      "[Iter.  290]  loss:7.744070  pct:-4.836503049\n",
      "[Iter.  300]  loss:7.379893  pct:-4.702659254\n",
      "[Iter.  310]  loss:7.042765  pct:-4.568192161\n",
      "[Iter.  320]  loss:6.730557  pct:-4.433034035\n",
      "[Iter.  330]  loss:6.441316  pct:-4.297435048\n",
      "[Iter.  340]  loss:6.173271  pct:-4.161338508\n",
      "[Iter.  350]  loss:5.924825  pct:-4.024543871\n",
      "[Iter.  360]  loss:5.694489  pct:-3.887637585\n",
      "[Iter.  370]  loss:5.480882  pct:-3.751123431\n",
      "[Iter.  380]  loss:5.282783  pct:-3.614357519\n",
      "[Iter.  390]  loss:5.099030  pct:-3.478346344\n",
      "[Iter.  400]  loss:4.928577  pct:-3.342843837\n",
      "[Iter.  410]  loss:4.770447  pct:-3.208434618\n",
      "[Iter.  420]  loss:4.623711  pct:-3.075941350\n",
      "[Iter.  430]  loss:4.487561  pct:-2.944602163\n",
      "[Iter.  440]  loss:4.361210  pct:-2.815569747\n",
      "[Iter.  450]  loss:4.243968  pct:-2.688298132\n",
      "[Iter.  460]  loss:4.135167  pct:-2.563659476\n",
      "[Iter.  470]  loss:4.034208  pct:-2.441469019\n",
      "[Iter.  480]  loss:3.940516  pct:-2.322445838\n",
      "[Iter.  490]  loss:3.853569  pct:-2.206492825\n",
      "[Iter.  500]  loss:3.772864  pct:-2.094290560\n",
      "[Iter.  510]  loss:3.697968  pct:-1.985119566\n",
      "[Iter.  520]  loss:3.628464  pct:-1.879525753\n",
      "[Iter.  530]  loss:3.563961  pct:-1.777680092\n",
      "[Iter.  540]  loss:3.504094  pct:-1.679798897\n",
      "[Iter.  550]  loss:3.448532  pct:-1.585631583\n",
      "[Iter.  560]  loss:3.396971  pct:-1.495140655\n",
      "[Iter.  570]  loss:3.349134  pct:-1.408239554\n",
      "[Iter.  580]  loss:3.304739  pct:-1.325579902\n",
      "[Iter.  590]  loss:3.263546  pct:-1.246483138\n",
      "[Iter.  600]  loss:3.225329  pct:-1.171021797\n",
      "[Iter.  610]  loss:3.189844  pct:-1.100198854\n",
      "[Iter.  620]  loss:3.156912  pct:-1.032380624\n",
      "[Iter.  630]  loss:3.126349  pct:-0.968125640\n",
      "[Iter.  640]  loss:3.097994  pct:-0.906979866\n",
      "[Iter.  650]  loss:3.071683  pct:-0.849304317\n",
      "[Iter.  660]  loss:3.047274  pct:-0.794639933\n",
      "[Iter.  670]  loss:3.024610  pct:-0.743741214\n",
      "[Iter.  680]  loss:3.003579  pct:-0.695341859\n",
      "[Iter.  690]  loss:2.984060  pct:-0.649853206\n",
      "[Iter.  700]  loss:2.965950  pct:-0.606892556\n",
      "[Iter.  710]  loss:2.949148  pct:-0.566498889\n",
      "[Iter.  720]  loss:2.933548  pct:-0.528965234\n",
      "[Iter.  730]  loss:2.919065  pct:-0.493677317\n",
      "[Iter.  740]  loss:2.905622  pct:-0.460532033\n",
      "[Iter.  750]  loss:2.893149  pct:-0.429291290\n",
      "[Iter.  760]  loss:2.881569  pct:-0.400239148\n",
      "[Iter.  770]  loss:2.870821  pct:-0.373012906\n",
      "[Iter.  780]  loss:2.860843  pct:-0.347559782\n",
      "[Iter.  790]  loss:2.851585  pct:-0.323587053\n",
      "[Iter.  800]  loss:2.842995  pct:-0.301235342\n",
      "[Iter.  810]  loss:2.835025  pct:-0.280341455\n",
      "[Iter.  820]  loss:2.827612  pct:-0.261492808\n",
      "[Iter.  830]  loss:2.820720  pct:-0.243738039\n",
      "[Iter.  840]  loss:2.814318  pct:-0.226955436\n",
      "[Iter.  850]  loss:2.808367  pct:-0.211451846\n",
      "[Iter.  860]  loss:2.802839  pct:-0.196856377\n",
      "[Iter.  870]  loss:2.797700  pct:-0.183353836\n",
      "[Iter.  880]  loss:2.792921  pct:-0.170822495\n",
      "[Iter.  890]  loss:2.788479  pct:-0.159035604\n",
      "[Iter.  900]  loss:2.784353  pct:-0.147960007\n",
      "[Iter.  910]  loss:2.780516  pct:-0.137801140\n",
      "[Iter.  920]  loss:2.776947  pct:-0.128353425\n",
      "[Iter.  930]  loss:2.773631  pct:-0.119434780\n",
      "[Iter.  940]  loss:2.770543  pct:-0.111334127\n",
      "[Iter.  950]  loss:2.767671  pct:-0.103635834\n",
      "[Iter.  960]  loss:2.765000  pct:-0.096507244\n",
      "[Iter.  970]  loss:2.762514  pct:-0.089909230\n",
      "[Iter.  980]  loss:2.760199  pct:-0.083802077\n",
      "[Iter.  990]  loss:2.758043  pct:-0.078128273\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.726691  pct:100.000000000\n",
      "[Iter.    2]  loss:2.726694  pct:0.000096183\n",
      "[Iter.    4]  loss:2.726695  pct:0.000026232\n",
      "[Iter.    6]  loss:2.726695  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.726695\n",
      "Best loss: 2.726695 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▌        | 1556/10000 [00:20<01:50, 76.25it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:276.480896  pct:100.000000000\n",
      "[Iter.   10]  loss:58.504448  pct:-78.839602875\n",
      "[Iter.   20]  loss:47.859421  pct:-18.195244184\n",
      "[Iter.   30]  loss:41.102364  pct:-14.118551960\n",
      "[Iter.   40]  loss:36.001980  pct:-12.408979225\n",
      "[Iter.   50]  loss:31.942713  pct:-11.275121711\n",
      "[Iter.   60]  loss:28.608976  pct:-10.436610197\n",
      "[Iter.   70]  loss:25.809278  pct:-9.786081964\n",
      "[Iter.   80]  loss:23.418604  pct:-9.262849375\n",
      "[Iter.   90]  loss:21.350903  pct:-8.829310871\n",
      "[Iter.  100]  loss:19.544420  pct:-8.460917801\n",
      "[Iter.  110]  loss:17.953398  pct:-8.140545853\n",
      "[Iter.  120]  loss:16.542826  pct:-7.856852901\n",
      "[Iter.  130]  loss:15.285466  pct:-7.600633215\n",
      "[Iter.  140]  loss:14.159612  pct:-7.365522764\n",
      "[Iter.  150]  loss:13.147701  pct:-7.146456131\n",
      "[Iter.  160]  loss:12.235300  pct:-6.939625270\n",
      "[Iter.  170]  loss:11.410390  pct:-6.742050947\n",
      "[Iter.  180]  loss:10.662841  pct:-6.551476887\n",
      "[Iter.  190]  loss:9.984085  pct:-6.365618414\n",
      "[Iter.  200]  loss:9.366728  pct:-6.183413391\n",
      "[Iter.  210]  loss:8.804408  pct:-6.003374560\n",
      "[Iter.  220]  loss:8.291579  pct:-5.824682621\n",
      "[Iter.  230]  loss:7.823391  pct:-5.646545671\n",
      "[Iter.  240]  loss:7.395551  pct:-5.468731001\n",
      "[Iter.  250]  loss:7.004268  pct:-5.290789347\n",
      "[Iter.  260]  loss:6.646194  pct:-5.112221616\n",
      "[Iter.  270]  loss:6.318303  pct:-4.933520255\n",
      "[Iter.  280]  loss:6.017915  pct:-4.754248953\n",
      "[Iter.  290]  loss:5.742601  pct:-4.574904146\n",
      "[Iter.  300]  loss:5.490179  pct:-4.395617809\n",
      "[Iter.  310]  loss:5.258642  pct:-4.217283369\n",
      "[Iter.  320]  loss:5.046260  pct:-4.038729175\n",
      "[Iter.  330]  loss:4.851381  pct:-3.861841895\n",
      "[Iter.  340]  loss:4.672513  pct:-3.686955995\n",
      "[Iter.  350]  loss:4.508333  pct:-3.513736648\n",
      "[Iter.  360]  loss:4.357605  pct:-3.343335456\n",
      "[Iter.  370]  loss:4.219223  pct:-3.175621014\n",
      "[Iter.  380]  loss:4.092153  pct:-3.011713027\n",
      "[Iter.  390]  loss:3.975464  pct:-2.851512713\n",
      "[Iter.  400]  loss:3.868309  pct:-2.695416529\n",
      "[Iter.  410]  loss:3.769898  pct:-2.544033651\n",
      "[Iter.  420]  loss:3.679524  pct:-2.397246716\n",
      "[Iter.  430]  loss:3.596540  pct:-2.255284327\n",
      "[Iter.  440]  loss:3.520336  pct:-2.118822267\n",
      "[Iter.  450]  loss:3.450354  pct:-1.987936630\n",
      "[Iter.  460]  loss:3.386073  pct:-1.863039612\n",
      "[Iter.  470]  loss:3.327023  pct:-1.743889581\n",
      "[Iter.  480]  loss:3.272803  pct:-1.629691025\n",
      "[Iter.  490]  loss:3.223021  pct:-1.521082592\n",
      "[Iter.  500]  loss:3.177296  pct:-1.418710743\n",
      "[Iter.  510]  loss:3.135298  pct:-1.321813067\n",
      "[Iter.  520]  loss:3.096724  pct:-1.230297926\n",
      "[Iter.  530]  loss:3.061306  pct:-1.143718073\n",
      "[Iter.  540]  loss:3.028786  pct:-1.062308947\n",
      "[Iter.  550]  loss:2.998913  pct:-0.986291419\n",
      "[Iter.  560]  loss:2.971477  pct:-0.914865331\n",
      "[Iter.  570]  loss:2.946276  pct:-0.848091421\n",
      "[Iter.  580]  loss:2.923125  pct:-0.785784983\n",
      "[Iter.  590]  loss:2.901845  pct:-0.727973427\n",
      "[Iter.  600]  loss:2.882294  pct:-0.673745003\n",
      "[Iter.  610]  loss:2.864339  pct:-0.622951739\n",
      "[Iter.  620]  loss:2.847847  pct:-0.575765973\n",
      "[Iter.  630]  loss:2.832697  pct:-0.531991621\n",
      "[Iter.  640]  loss:2.818782  pct:-0.491221756\n",
      "[Iter.  650]  loss:2.806004  pct:-0.453334815\n",
      "[Iter.  660]  loss:2.794263  pct:-0.418396035\n",
      "[Iter.  670]  loss:2.783486  pct:-0.385699988\n",
      "[Iter.  680]  loss:2.773580  pct:-0.355869478\n",
      "[Iter.  690]  loss:2.764471  pct:-0.328429597\n",
      "[Iter.  700]  loss:2.756097  pct:-0.302922838\n",
      "[Iter.  710]  loss:2.748399  pct:-0.279310181\n",
      "[Iter.  720]  loss:2.741323  pct:-0.257468584\n",
      "[Iter.  730]  loss:2.734817  pct:-0.237303378\n",
      "[Iter.  740]  loss:2.728840  pct:-0.218548984\n",
      "[Iter.  750]  loss:2.723352  pct:-0.201125569\n",
      "[Iter.  760]  loss:2.718280  pct:-0.186245367\n",
      "[Iter.  770]  loss:2.713608  pct:-0.171884029\n",
      "[Iter.  780]  loss:2.709311  pct:-0.158315611\n",
      "[Iter.  790]  loss:2.705354  pct:-0.146079490\n",
      "[Iter.  800]  loss:2.701712  pct:-0.134607365\n",
      "[Iter.  810]  loss:2.698361  pct:-0.124049114\n",
      "[Iter.  820]  loss:2.695277  pct:-0.114289551\n",
      "[Iter.  830]  loss:2.692435  pct:-0.105424151\n",
      "[Iter.  840]  loss:2.689815  pct:-0.097326708\n",
      "[Iter.  850]  loss:2.687398  pct:-0.089860742\n",
      "[Iter.  860]  loss:2.685171  pct:-0.082844183\n",
      "[Iter.  870]  loss:2.683126  pct:-0.076173648\n",
      "[Iter.  880]  loss:2.681241  pct:-0.070242653\n",
      "[Iter.  890]  loss:2.679492  pct:-0.065232425\n",
      "[Iter.  900]  loss:2.677879  pct:-0.060203201\n",
      "[Iter.  910]  loss:2.676384  pct:-0.055832353\n",
      "[Iter.  920]  loss:2.675004  pct:-0.051569774\n",
      "[Iter.  930]  loss:2.673730  pct:-0.047621259\n",
      "[Iter.  940]  loss:2.672552  pct:-0.044050365\n",
      "[Iter.  950]  loss:2.671459  pct:-0.040893899\n",
      "[Iter.  960]  loss:2.670449  pct:-0.037813773\n",
      "[Iter.  970]  loss:2.669515  pct:-0.034988963\n",
      "[Iter.  980]  loss:2.668651  pct:-0.032366518\n",
      "[Iter.  990]  loss:2.667853  pct:-0.029884397\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.656522  pct:100.000000000\n",
      "[Iter.    2]  loss:2.656521  pct:-0.000026925\n",
      "[Iter.    4]  loss:2.656520  pct:-0.000035899\n",
      "[Iter.    6]  loss:2.656520  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.656520\n",
      "Best loss: 2.656520 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 15%|█▌        | 1518/10000 [00:23<02:08, 65.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:414.972687  pct:100.000000000\n",
      "[Iter.   10]  loss:61.566666  pct:-85.163678750\n",
      "[Iter.   20]  loss:54.051960  pct:-12.205802570\n",
      "[Iter.   30]  loss:48.164982  pct:-10.891331508\n",
      "[Iter.   40]  loss:43.311275  pct:-10.077251510\n",
      "[Iter.   50]  loss:39.168709  pct:-9.564637926\n",
      "[Iter.   60]  loss:35.558434  pct:-9.217243506\n",
      "[Iter.   70]  loss:32.466587  pct:-8.695114376\n",
      "[Iter.   80]  loss:29.802343  pct:-8.206109539\n",
      "[Iter.   90]  loss:27.458168  pct:-7.865741663\n",
      "[Iter.  100]  loss:25.372019  pct:-7.597554263\n",
      "[Iter.  110]  loss:23.505362  pct:-7.357149113\n",
      "[Iter.  120]  loss:21.826305  pct:-7.143290111\n",
      "[Iter.  130]  loss:20.309336  pct:-6.950189937\n",
      "[Iter.  140]  loss:18.932621  pct:-6.778728395\n",
      "[Iter.  150]  loss:17.679382  pct:-6.619467415\n",
      "[Iter.  160]  loss:16.535719  pct:-6.468910426\n",
      "[Iter.  170]  loss:15.489403  pct:-6.327612074\n",
      "[Iter.  180]  loss:14.530220  pct:-6.192509508\n",
      "[Iter.  190]  loss:13.649303  pct:-6.062651450\n",
      "[Iter.  200]  loss:12.838921  pct:-5.937173621\n",
      "[Iter.  210]  loss:12.092147  pct:-5.816483671\n",
      "[Iter.  220]  loss:11.403144  pct:-5.697937661\n",
      "[Iter.  230]  loss:10.767692  pct:-5.572605915\n",
      "[Iter.  240]  loss:10.180571  pct:-5.452617246\n",
      "[Iter.  250]  loss:9.637621  pct:-5.333194943\n",
      "[Iter.  260]  loss:9.135181  pct:-5.213314601\n",
      "[Iter.  270]  loss:8.669881  pct:-5.093501029\n",
      "[Iter.  280]  loss:8.238603  pct:-4.974442387\n",
      "[Iter.  290]  loss:7.838923  pct:-4.851309806\n",
      "[Iter.  300]  loss:7.468389  pct:-4.726841812\n",
      "[Iter.  310]  loss:7.124644  pct:-4.602662679\n",
      "[Iter.  320]  loss:6.805657  pct:-4.477239242\n",
      "[Iter.  330]  loss:6.509539  pct:-4.351053638\n",
      "[Iter.  340]  loss:6.234591  pct:-4.223772447\n",
      "[Iter.  350]  loss:5.979259  pct:-4.095401222\n",
      "[Iter.  360]  loss:5.742079  pct:-3.966723809\n",
      "[Iter.  370]  loss:5.521766  pct:-3.836808998\n",
      "[Iter.  380]  loss:5.317070  pct:-3.707069345\n",
      "[Iter.  390]  loss:5.126855  pct:-3.577450932\n",
      "[Iter.  400]  loss:4.949969  pct:-3.450177710\n",
      "[Iter.  410]  loss:4.785440  pct:-3.323845339\n",
      "[Iter.  420]  loss:4.632738  pct:-3.190977895\n",
      "[Iter.  430]  loss:4.490828  pct:-3.063190935\n",
      "[Iter.  440]  loss:4.358913  pct:-2.937433617\n",
      "[Iter.  450]  loss:4.236286  pct:-2.813253209\n",
      "[Iter.  460]  loss:4.122271  pct:-2.691381885\n",
      "[Iter.  470]  loss:4.016300  pct:-2.570691203\n",
      "[Iter.  480]  loss:3.917805  pct:-2.452381586\n",
      "[Iter.  490]  loss:3.826233  pct:-2.337336344\n",
      "[Iter.  500]  loss:3.741107  pct:-2.224790982\n",
      "[Iter.  510]  loss:3.661957  pct:-2.115689156\n",
      "[Iter.  520]  loss:3.588387  pct:-2.009022381\n",
      "[Iter.  530]  loss:3.519985  pct:-1.906212485\n",
      "[Iter.  540]  loss:3.456394  pct:-1.806576967\n",
      "[Iter.  550]  loss:3.397292  pct:-1.709920290\n",
      "[Iter.  560]  loss:3.342354  pct:-1.617120644\n",
      "[Iter.  570]  loss:3.291254  pct:-1.528877299\n",
      "[Iter.  580]  loss:3.243760  pct:-1.443013077\n",
      "[Iter.  590]  loss:3.199611  pct:-1.361056122\n",
      "[Iter.  600]  loss:3.158571  pct:-1.282661635\n",
      "[Iter.  610]  loss:3.120419  pct:-1.207894984\n",
      "[Iter.  620]  loss:3.084939  pct:-1.137027674\n",
      "[Iter.  630]  loss:3.051964  pct:-1.068893909\n",
      "[Iter.  640]  loss:3.021304  pct:-1.004580628\n",
      "[Iter.  650]  loss:2.992800  pct:-0.943437990\n",
      "[Iter.  660]  loss:2.966297  pct:-0.885569447\n",
      "[Iter.  670]  loss:2.941646  pct:-0.831029829\n",
      "[Iter.  680]  loss:2.918725  pct:-0.779200591\n",
      "[Iter.  690]  loss:2.897411  pct:-0.730247192\n",
      "[Iter.  700]  loss:2.877597  pct:-0.683852423\n",
      "[Iter.  710]  loss:2.859173  pct:-0.640249369\n",
      "[Iter.  720]  loss:2.842036  pct:-0.599387554\n",
      "[Iter.  730]  loss:2.826105  pct:-0.560519937\n",
      "[Iter.  740]  loss:2.811289  pct:-0.524273540\n",
      "[Iter.  750]  loss:2.797503  pct:-0.490357379\n",
      "[Iter.  760]  loss:2.784681  pct:-0.458368205\n",
      "[Iter.  770]  loss:2.772745  pct:-0.428603340\n",
      "[Iter.  780]  loss:2.761644  pct:-0.400378788\n",
      "[Iter.  790]  loss:2.751316  pct:-0.373982123\n",
      "[Iter.  800]  loss:2.741702  pct:-0.349415136\n",
      "[Iter.  810]  loss:2.732756  pct:-0.326300207\n",
      "[Iter.  820]  loss:2.724437  pct:-0.304414322\n",
      "[Iter.  830]  loss:2.716689  pct:-0.284384917\n",
      "[Iter.  840]  loss:2.709477  pct:-0.265493692\n",
      "[Iter.  850]  loss:2.702763  pct:-0.247792024\n",
      "[Iter.  860]  loss:2.696507  pct:-0.231470679\n",
      "[Iter.  870]  loss:2.690686  pct:-0.215844977\n",
      "[Iter.  880]  loss:2.685261  pct:-0.201647114\n",
      "[Iter.  890]  loss:2.680209  pct:-0.188141492\n",
      "[Iter.  900]  loss:2.675504  pct:-0.175526459\n",
      "[Iter.  910]  loss:2.671118  pct:-0.163920870\n",
      "[Iter.  920]  loss:2.667034  pct:-0.152916731\n",
      "[Iter.  930]  loss:2.663232  pct:-0.142557658\n",
      "[Iter.  940]  loss:2.659682  pct:-0.133280766\n",
      "[Iter.  950]  loss:2.656368  pct:-0.124593071\n",
      "[Iter.  960]  loss:2.653280  pct:-0.116266786\n",
      "[Iter.  970]  loss:2.650396  pct:-0.108683316\n",
      "[Iter.  980]  loss:2.647706  pct:-0.101497153\n",
      "[Iter.  990]  loss:2.645199  pct:-0.094711662\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.606923  pct:100.000000000\n",
      "[Iter.    2]  loss:2.606900  pct:-0.000868832\n",
      "[Iter.    4]  loss:2.606895  pct:-0.000173768\n",
      "[Iter.    6]  loss:2.606891  pct:-0.000173768\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.606891\n",
      "Best loss: 2.606891 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1702/10000 [00:18<01:31, 90.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:317.453552  pct:100.000000000\n",
      "[Iter.   10]  loss:70.587578  pct:-77.764436734\n",
      "[Iter.   20]  loss:58.240047  pct:-17.492497613\n",
      "[Iter.   30]  loss:50.097275  pct:-13.981397733\n",
      "[Iter.   40]  loss:43.924759  pct:-12.321061168\n",
      "[Iter.   50]  loss:39.031086  pct:-11.141035408\n",
      "[Iter.   60]  loss:35.029728  pct:-10.251720989\n",
      "[Iter.   70]  loss:31.678511  pct:-9.566780753\n",
      "[Iter.   80]  loss:28.819828  pct:-9.024043657\n",
      "[Iter.   90]  loss:26.346319  pct:-8.582663408\n",
      "[Iter.  100]  loss:24.181635  pct:-8.216268388\n",
      "[Iter.  110]  loss:22.271122  pct:-7.900677237\n",
      "[Iter.  120]  loss:20.572348  pct:-7.627699850\n",
      "[Iter.  130]  loss:19.052471  pct:-7.387958373\n",
      "[Iter.  140]  loss:17.685644  pct:-7.174014329\n",
      "[Iter.  150]  loss:16.450037  pct:-6.986497844\n",
      "[Iter.  160]  loss:15.327125  pct:-6.826199885\n",
      "[Iter.  170]  loss:14.302514  pct:-6.684949372\n",
      "[Iter.  180]  loss:13.368752  pct:-6.528660244\n",
      "[Iter.  190]  loss:12.520761  pct:-6.343082883\n",
      "[Iter.  200]  loss:11.749081  pct:-6.163202914\n",
      "[Iter.  210]  loss:11.044065  pct:-6.000606828\n",
      "[Iter.  220]  loss:10.398184  pct:-5.848215554\n",
      "[Iter.  230]  loss:9.805404  pct:-5.700804326\n",
      "[Iter.  240]  loss:9.260662  pct:-5.555524757\n",
      "[Iter.  250]  loss:8.759546  pct:-5.411230803\n",
      "[Iter.  260]  loss:8.298159  pct:-5.267254941\n",
      "[Iter.  270]  loss:7.873024  pct:-5.123235167\n",
      "[Iter.  280]  loss:7.481019  pct:-4.979095967\n",
      "[Iter.  290]  loss:7.119381  pct:-4.834068612\n",
      "[Iter.  300]  loss:6.785587  pct:-4.688526951\n",
      "[Iter.  310]  loss:6.477360  pct:-4.542385573\n",
      "[Iter.  320]  loss:6.192659  pct:-4.395315433\n",
      "[Iter.  330]  loss:5.929582  pct:-4.248211358\n",
      "[Iter.  340]  loss:5.686443  pct:-4.100437185\n",
      "[Iter.  350]  loss:5.461684  pct:-3.952542721\n",
      "[Iter.  360]  loss:5.253856  pct:-3.805200672\n",
      "[Iter.  370]  loss:5.061645  pct:-3.658477592\n",
      "[Iter.  380]  loss:4.883871  pct:-3.512177393\n",
      "[Iter.  390]  loss:4.719432  pct:-3.366975128\n",
      "[Iter.  400]  loss:4.567299  pct:-3.223554305\n",
      "[Iter.  410]  loss:4.426558  pct:-3.081490282\n",
      "[Iter.  420]  loss:4.296348  pct:-2.941572190\n",
      "[Iter.  430]  loss:4.175867  pct:-2.804265971\n",
      "[Iter.  440]  loss:4.064382  pct:-2.669745349\n",
      "[Iter.  450]  loss:3.961232  pct:-2.537881179\n",
      "[Iter.  460]  loss:3.865773  pct:-2.409842468\n",
      "[Iter.  470]  loss:3.777434  pct:-2.285159817\n",
      "[Iter.  480]  loss:3.695686  pct:-2.164108574\n",
      "[Iter.  490]  loss:3.620046  pct:-2.046703174\n",
      "[Iter.  500]  loss:3.550049  pct:-1.933602594\n",
      "[Iter.  510]  loss:3.485281  pct:-1.824441059\n",
      "[Iter.  520]  loss:3.425353  pct:-1.719451324\n",
      "[Iter.  530]  loss:3.369891  pct:-1.619165237\n",
      "[Iter.  540]  loss:3.318573  pct:-1.522822492\n",
      "[Iter.  550]  loss:3.271087  pct:-1.430932520\n",
      "[Iter.  560]  loss:3.227147  pct:-1.343264675\n",
      "[Iter.  570]  loss:3.186492  pct:-1.259778169\n",
      "[Iter.  580]  loss:3.148872  pct:-1.180610601\n",
      "[Iter.  590]  loss:3.114070  pct:-1.105242484\n",
      "[Iter.  600]  loss:3.081861  pct:-1.034310759\n",
      "[Iter.  610]  loss:3.052060  pct:-0.966961828\n",
      "[Iter.  620]  loss:3.024491  pct:-0.903301142\n",
      "[Iter.  630]  loss:2.998973  pct:-0.843718178\n",
      "[Iter.  640]  loss:2.975370  pct:-0.787034935\n",
      "[Iter.  650]  loss:2.953525  pct:-0.734197917\n",
      "[Iter.  660]  loss:2.933306  pct:-0.684542597\n",
      "[Iter.  670]  loss:2.914605  pct:-0.637550669\n",
      "[Iter.  680]  loss:2.897294  pct:-0.593951282\n",
      "[Iter.  690]  loss:2.881278  pct:-0.552791934\n",
      "[Iter.  700]  loss:2.866453  pct:-0.514515776\n",
      "[Iter.  710]  loss:2.852734  pct:-0.478608268\n",
      "[Iter.  720]  loss:2.840037  pct:-0.445072783\n",
      "[Iter.  730]  loss:2.828280  pct:-0.413978197\n",
      "[Iter.  740]  loss:2.817407  pct:-0.384432944\n",
      "[Iter.  750]  loss:2.807334  pct:-0.357533847\n",
      "[Iter.  760]  loss:2.798010  pct:-0.332141156\n",
      "[Iter.  770]  loss:2.789381  pct:-0.308400757\n",
      "[Iter.  780]  loss:2.781386  pct:-0.286601722\n",
      "[Iter.  790]  loss:2.773983  pct:-0.266192810\n",
      "[Iter.  800]  loss:2.767130  pct:-0.247023488\n",
      "[Iter.  810]  loss:2.760784  pct:-0.229351850\n",
      "[Iter.  820]  loss:2.754905  pct:-0.212952713\n",
      "[Iter.  830]  loss:2.749457  pct:-0.197725520\n",
      "[Iter.  840]  loss:2.744411  pct:-0.183531823\n",
      "[Iter.  850]  loss:2.739736  pct:-0.170369029\n",
      "[Iter.  860]  loss:2.735401  pct:-0.158215566\n",
      "[Iter.  870]  loss:2.731383  pct:-0.146873954\n",
      "[Iter.  880]  loss:2.727659  pct:-0.136370953\n",
      "[Iter.  890]  loss:2.724205  pct:-0.126610172\n",
      "[Iter.  900]  loss:2.721002  pct:-0.117572472\n",
      "[Iter.  910]  loss:2.718033  pct:-0.109123950\n",
      "[Iter.  920]  loss:2.715277  pct:-0.101383688\n",
      "[Iter.  930]  loss:2.712721  pct:-0.094128407\n",
      "[Iter.  940]  loss:2.710351  pct:-0.087396900\n",
      "[Iter.  950]  loss:2.708149  pct:-0.081218969\n",
      "[Iter.  960]  loss:2.706105  pct:-0.075465712\n",
      "[Iter.  970]  loss:2.704208  pct:-0.070121933\n",
      "[Iter.  980]  loss:2.702446  pct:-0.065172139\n",
      "[Iter.  990]  loss:2.700808  pct:-0.060609386\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.677527  pct:100.000000000\n",
      "[Iter.    2]  loss:2.677528  pct:0.000044522\n",
      "[Iter.    4]  loss:2.677528  pct:0.000000000\n",
      "[Iter.    6]  loss:2.677528  pct:-0.000008904\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.677528\n",
      "Best loss: 2.677528 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  8%|▊         | 807/10000 [00:08<01:35, 95.96it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:439.645447  pct:100.000000000\n",
      "[Iter.   10]  loss:92.554146  pct:-78.948004838\n",
      "[Iter.   20]  loss:78.522034  pct:-15.160976311\n",
      "[Iter.   30]  loss:68.802368  pct:-12.378265145\n",
      "[Iter.   40]  loss:61.361256  pct:-10.815198251\n",
      "[Iter.   50]  loss:55.332443  pct:-9.825112516\n",
      "[Iter.   60]  loss:50.305714  pct:-9.084597190\n",
      "[Iter.   70]  loss:46.026863  pct:-8.505694969\n",
      "[Iter.   80]  loss:42.326157  pct:-8.040318703\n",
      "[Iter.   90]  loss:39.084984  pct:-7.657611864\n",
      "[Iter.  100]  loss:36.217537  pct:-7.336441310\n",
      "[Iter.  110]  loss:33.659691  pct:-7.062451747\n",
      "[Iter.  120]  loss:31.362261  pct:-6.825463871\n",
      "[Iter.  130]  loss:29.286869  pct:-6.617481378\n",
      "[Iter.  140]  loss:27.402805  pct:-6.433134650\n",
      "[Iter.  150]  loss:25.685228  pct:-6.267887393\n",
      "[Iter.  160]  loss:24.113754  pct:-6.118201692\n",
      "[Iter.  170]  loss:22.671461  pct:-5.981205377\n",
      "[Iter.  180]  loss:21.344046  pct:-5.855006257\n",
      "[Iter.  190]  loss:20.119429  pct:-5.737511178\n",
      "[Iter.  200]  loss:18.987280  pct:-5.627141621\n",
      "[Iter.  210]  loss:17.938631  pct:-5.522901860\n",
      "[Iter.  220]  loss:16.965698  pct:-5.423673704\n",
      "[Iter.  230]  loss:16.061722  pct:-5.328259571\n",
      "[Iter.  240]  loss:15.220683  pct:-5.236292312\n",
      "[Iter.  250]  loss:14.437296  pct:-5.146859567\n",
      "[Iter.  260]  loss:13.706824  pct:-5.059615148\n",
      "[Iter.  270]  loss:13.025118  pct:-4.973481920\n",
      "[Iter.  280]  loss:12.388342  pct:-4.888830770\n",
      "[Iter.  290]  loss:11.793111  pct:-4.804767747\n",
      "[Iter.  300]  loss:11.236326  pct:-4.721270215\n",
      "[Iter.  310]  loss:10.715183  pct:-4.638019131\n",
      "[Iter.  320]  loss:10.227130  pct:-4.554782780\n",
      "[Iter.  330]  loss:9.769876  pct:-4.470994430\n",
      "[Iter.  340]  loss:9.341237  pct:-4.387348202\n",
      "[Iter.  350]  loss:8.939309  pct:-4.302727198\n",
      "[Iter.  360]  loss:8.562305  pct:-4.217369202\n",
      "[Iter.  370]  loss:8.208551  pct:-4.131527959\n",
      "[Iter.  380]  loss:7.876536  pct:-4.044745791\n",
      "[Iter.  390]  loss:7.564825  pct:-3.957472848\n",
      "[Iter.  400]  loss:7.272107  pct:-3.869454654\n",
      "[Iter.  410]  loss:6.997200  pct:-3.780288582\n",
      "[Iter.  420]  loss:6.738947  pct:-3.690812844\n",
      "[Iter.  430]  loss:6.496334  pct:-3.600166594\n",
      "[Iter.  440]  loss:6.268352  pct:-3.509395580\n",
      "[Iter.  450]  loss:6.054103  pct:-3.417942586\n",
      "[Iter.  460]  loss:5.852739  pct:-3.326075618\n",
      "[Iter.  470]  loss:5.663502  pct:-3.233308753\n",
      "[Iter.  480]  loss:5.485603  pct:-3.141138030\n",
      "[Iter.  490]  loss:5.318353  pct:-3.048892643\n",
      "[Iter.  500]  loss:5.161123  pct:-2.956373373\n",
      "[Iter.  510]  loss:5.013312  pct:-2.863920602\n",
      "[Iter.  520]  loss:4.874335  pct:-2.772160228\n",
      "[Iter.  530]  loss:4.743661  pct:-2.680855493\n",
      "[Iter.  540]  loss:4.620787  pct:-2.590283106\n",
      "[Iter.  550]  loss:4.505249  pct:-2.500399103\n",
      "[Iter.  560]  loss:4.396625  pct:-2.411064466\n",
      "[Iter.  570]  loss:4.294491  pct:-2.322992845\n",
      "[Iter.  580]  loss:4.198465  pct:-2.236026045\n",
      "[Iter.  590]  loss:4.108173  pct:-2.150594788\n",
      "[Iter.  600]  loss:4.023263  pct:-2.066864884\n",
      "[Iter.  610]  loss:3.943422  pct:-1.984481227\n",
      "[Iter.  620]  loss:3.868346  pct:-1.903837330\n",
      "[Iter.  630]  loss:3.797755  pct:-1.824830025\n",
      "[Iter.  640]  loss:3.731390  pct:-1.747473576\n",
      "[Iter.  650]  loss:3.668994  pct:-1.672186695\n",
      "[Iter.  660]  loss:3.610321  pct:-1.599161420\n",
      "[Iter.  670]  loss:3.555155  pct:-1.528028034\n",
      "[Iter.  680]  loss:3.503287  pct:-1.458944261\n",
      "[Iter.  690]  loss:3.454518  pct:-1.392099416\n",
      "[Iter.  700]  loss:3.408680  pct:-1.326896674\n",
      "[Iter.  710]  loss:3.365573  pct:-1.264618499\n",
      "[Iter.  720]  loss:3.325047  pct:-1.204117018\n",
      "[Iter.  730]  loss:3.286950  pct:-1.145776717\n",
      "[Iter.  740]  loss:3.251131  pct:-1.089735318\n",
      "[Iter.  750]  loss:3.217460  pct:-1.035660074\n",
      "[Iter.  760]  loss:3.185802  pct:-0.983934353\n",
      "[Iter.  770]  loss:3.156046  pct:-0.934021163\n",
      "[Iter.  780]  loss:3.128070  pct:-0.886449306\n",
      "[Iter.  790]  loss:3.101776  pct:-0.840582085\n",
      "[Iter.  800]  loss:3.077055  pct:-0.796976738\n",
      "[Iter.  810]  loss:3.053820  pct:-0.755100009\n",
      "[Iter.  820]  loss:3.031974  pct:-0.715375882\n",
      "[Iter.  830]  loss:3.011435  pct:-0.677407075\n",
      "[Iter.  840]  loss:2.992131  pct:-0.641040308\n",
      "[Iter.  850]  loss:2.973981  pct:-0.606570280\n",
      "[Iter.  860]  loss:2.956923  pct:-0.573579048\n",
      "[Iter.  870]  loss:2.940889  pct:-0.542257101\n",
      "[Iter.  880]  loss:2.925819  pct:-0.512420649\n",
      "[Iter.  890]  loss:2.911651  pct:-0.484257535\n",
      "[Iter.  900]  loss:2.898324  pct:-0.457708832\n",
      "[Iter.  910]  loss:2.885797  pct:-0.432214926\n",
      "[Iter.  920]  loss:2.874017  pct:-0.408198673\n",
      "[Iter.  930]  loss:2.862936  pct:-0.385557197\n",
      "[Iter.  940]  loss:2.852520  pct:-0.363823360\n",
      "[Iter.  950]  loss:2.842734  pct:-0.343077932\n",
      "[Iter.  960]  loss:2.833530  pct:-0.323761311\n",
      "[Iter.  970]  loss:2.824881  pct:-0.305233112\n",
      "[Iter.  980]  loss:2.816747  pct:-0.287962664\n",
      "[Iter.  990]  loss:2.809096  pct:-0.271603241\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.684356  pct:100.000000000\n",
      "[Iter.    2]  loss:2.684355  pct:-0.000035527\n",
      "[Iter.    4]  loss:2.684355  pct:-0.000026645\n",
      "[Iter.    6]  loss:2.684356  pct:0.000044409\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.684356\n",
      "Best loss: 2.684356 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 12%|█▏        | 1227/10000 [00:11<01:24, 103.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:824.815430  pct:100.000000000\n",
      "[Iter.   10]  loss:151.972534  pct:-81.574964688\n",
      "[Iter.   20]  loss:135.756439  pct:-10.670411636\n",
      "[Iter.   30]  loss:123.930443  pct:-8.711186348\n",
      "[Iter.   40]  loss:114.069954  pct:-7.956470314\n",
      "[Iter.   50]  loss:105.701241  pct:-7.336474761\n",
      "[Iter.   60]  loss:98.445213  pct:-6.864656635\n",
      "[Iter.   70]  loss:92.065376  pct:-6.480596487\n",
      "[Iter.   80]  loss:86.395103  pct:-6.158963398\n",
      "[Iter.   90]  loss:81.309990  pct:-5.885881632\n",
      "[Iter.  100]  loss:76.715042  pct:-5.651147933\n",
      "[Iter.  110]  loss:72.536171  pct:-5.447264369\n",
      "[Iter.  120]  loss:68.714417  pct:-5.268756822\n",
      "[Iter.  130]  loss:65.202446  pct:-5.110966081\n",
      "[Iter.  140]  loss:61.961361  pct:-4.970802864\n",
      "[Iter.  150]  loss:58.959244  pct:-4.845143993\n",
      "[Iter.  160]  loss:56.169243  pct:-4.732083957\n",
      "[Iter.  170]  loss:53.568787  pct:-4.629680062\n",
      "[Iter.  180]  loss:51.138680  pct:-4.536423671\n",
      "[Iter.  190]  loss:48.862392  pct:-4.451204257\n",
      "[Iter.  200]  loss:46.725616  pct:-4.373048196\n",
      "[Iter.  210]  loss:44.715996  pct:-4.300897065\n",
      "[Iter.  220]  loss:42.822655  pct:-4.234147157\n",
      "[Iter.  230]  loss:41.036030  pct:-4.172148878\n",
      "[Iter.  240]  loss:39.347713  pct:-4.114229259\n",
      "[Iter.  250]  loss:37.750179  pct:-4.060043237\n",
      "[Iter.  260]  loss:36.236668  pct:-4.009283363\n",
      "[Iter.  270]  loss:34.801128  pct:-3.961565286\n",
      "[Iter.  280]  loss:33.438221  pct:-3.916273618\n",
      "[Iter.  290]  loss:32.143093  pct:-3.873196094\n",
      "[Iter.  300]  loss:30.911249  pct:-3.832375261\n",
      "[Iter.  310]  loss:29.738661  pct:-3.793403309\n",
      "[Iter.  320]  loss:28.621624  pct:-3.756177275\n",
      "[Iter.  330]  loss:27.556826  pct:-3.720258345\n",
      "[Iter.  340]  loss:26.541027  pct:-3.686195871\n",
      "[Iter.  350]  loss:25.571543  pct:-3.652776235\n",
      "[Iter.  360]  loss:24.645725  pct:-3.620499158\n",
      "[Iter.  370]  loss:23.761148  pct:-3.589169272\n",
      "[Iter.  380]  loss:22.915571  pct:-3.558654758\n",
      "[Iter.  390]  loss:22.106947  pct:-3.528710937\n",
      "[Iter.  400]  loss:21.333256  pct:-3.499764935\n",
      "[Iter.  410]  loss:20.592777  pct:-3.471005662\n",
      "[Iter.  420]  loss:19.883810  pct:-3.442795501\n",
      "[Iter.  430]  loss:19.204781  pct:-3.414986681\n",
      "[Iter.  440]  loss:18.554173  pct:-3.387740152\n",
      "[Iter.  450]  loss:17.930656  pct:-3.360516791\n",
      "[Iter.  460]  loss:17.332920  pct:-3.333599977\n",
      "[Iter.  470]  loss:16.759745  pct:-3.306860170\n",
      "[Iter.  480]  loss:16.210072  pct:-3.279722288\n",
      "[Iter.  490]  loss:15.682766  pct:-3.252950494\n",
      "[Iter.  500]  loss:15.176802  pct:-3.226243894\n",
      "[Iter.  510]  loss:14.691209  pct:-3.199572955\n",
      "[Iter.  520]  loss:14.225111  pct:-3.172630903\n",
      "[Iter.  530]  loss:13.777632  pct:-3.145699515\n",
      "[Iter.  540]  loss:13.347942  pct:-3.118746493\n",
      "[Iter.  550]  loss:12.935287  pct:-3.091531410\n",
      "[Iter.  560]  loss:12.539001  pct:-3.063597056\n",
      "[Iter.  570]  loss:12.158303  pct:-3.036112605\n",
      "[Iter.  580]  loss:11.792594  pct:-3.007897541\n",
      "[Iter.  590]  loss:11.441239  pct:-2.979451343\n",
      "[Iter.  600]  loss:11.103626  pct:-2.950843831\n",
      "[Iter.  610]  loss:10.779208  pct:-2.921730798\n",
      "[Iter.  620]  loss:10.467379  pct:-2.892880086\n",
      "[Iter.  630]  loss:10.167702  pct:-2.862960309\n",
      "[Iter.  640]  loss:9.879663  pct:-2.832884120\n",
      "[Iter.  650]  loss:9.602747  pct:-2.802884712\n",
      "[Iter.  660]  loss:9.336563  pct:-2.771955297\n",
      "[Iter.  670]  loss:9.080623  pct:-2.741270361\n",
      "[Iter.  680]  loss:8.834554  pct:-2.709824682\n",
      "[Iter.  690]  loss:8.597965  pct:-2.677990147\n",
      "[Iter.  700]  loss:8.370478  pct:-2.645830237\n",
      "[Iter.  710]  loss:8.151725  pct:-2.613385633\n",
      "[Iter.  720]  loss:7.941387  pct:-2.580283849\n",
      "[Iter.  730]  loss:7.739109  pct:-2.547144593\n",
      "[Iter.  740]  loss:7.544580  pct:-2.513572478\n",
      "[Iter.  750]  loss:7.357460  pct:-2.480190407\n",
      "[Iter.  760]  loss:7.177502  pct:-2.445936616\n",
      "[Iter.  770]  loss:7.004470  pct:-2.410745937\n",
      "[Iter.  780]  loss:6.838037  pct:-2.376101633\n",
      "[Iter.  790]  loss:6.677930  pct:-2.341412581\n",
      "[Iter.  800]  loss:6.523971  pct:-2.305501000\n",
      "[Iter.  810]  loss:6.375890  pct:-2.269796031\n",
      "[Iter.  820]  loss:6.233492  pct:-2.233380524\n",
      "[Iter.  830]  loss:6.096486  pct:-2.197898196\n",
      "[Iter.  840]  loss:5.964745  pct:-2.160934101\n",
      "[Iter.  850]  loss:5.838008  pct:-2.124762081\n",
      "[Iter.  860]  loss:5.716102  pct:-2.088156593\n",
      "[Iter.  870]  loss:5.598873  pct:-2.050855497\n",
      "[Iter.  880]  loss:5.486126  pct:-2.013731076\n",
      "[Iter.  890]  loss:5.377652  pct:-1.977246717\n",
      "[Iter.  900]  loss:5.273337  pct:-1.939792267\n",
      "[Iter.  910]  loss:5.172969  pct:-1.903302372\n",
      "[Iter.  920]  loss:5.076437  pct:-1.866082353\n",
      "[Iter.  930]  loss:4.983599  pct:-1.828807857\n",
      "[Iter.  940]  loss:4.894305  pct:-1.791765956\n",
      "[Iter.  950]  loss:4.808418  pct:-1.754824899\n",
      "[Iter.  960]  loss:4.725798  pct:-1.718239537\n",
      "[Iter.  970]  loss:4.646329  pct:-1.681603864\n",
      "[Iter.  980]  loss:4.569915  pct:-1.644602280\n",
      "[Iter.  990]  loss:4.496415  pct:-1.608358766\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.650487  pct:100.000000000\n",
      "[Iter.    2]  loss:2.650479  pct:-0.000305839\n",
      "[Iter.    4]  loss:2.650481  pct:0.000062967\n",
      "[Iter.    6]  loss:2.650477  pct:-0.000125934\n",
      "[Iter.    8]  loss:2.650482  pct:0.000170911\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.650482\n",
      "Best loss: 2.650482 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 22%|██▏       | 2238/10000 [00:32<01:53, 68.34it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:183.918121  pct:100.000000000\n",
      "[Iter.   10]  loss:35.377087  pct:-80.764760864\n",
      "[Iter.   20]  loss:28.165487  pct:-20.384944140\n",
      "[Iter.   30]  loss:23.285145  pct:-17.327385226\n",
      "[Iter.   40]  loss:19.705427  pct:-15.373396498\n",
      "[Iter.   50]  loss:16.947008  pct:-13.998270695\n",
      "[Iter.   60]  loss:14.749156  pct:-12.968968431\n",
      "[Iter.   70]  loss:12.958217  pct:-12.142656375\n",
      "[Iter.   80]  loss:11.475307  pct:-11.443775333\n",
      "[Iter.   90]  loss:10.233217  pct:-10.824025666\n",
      "[Iter.  100]  loss:9.183760  pct:-10.255401850\n",
      "[Iter.  110]  loss:8.290356  pct:-9.728085634\n",
      "[Iter.  120]  loss:7.526989  pct:-9.207882649\n",
      "[Iter.  130]  loss:6.871076  pct:-8.714159005\n",
      "[Iter.  140]  loss:6.306113  pct:-8.222328169\n",
      "[Iter.  150]  loss:5.817892  pct:-7.742029832\n",
      "[Iter.  160]  loss:5.394959  pct:-7.269516509\n",
      "[Iter.  170]  loss:5.028416  pct:-6.794189525\n",
      "[Iter.  180]  loss:4.710376  pct:-6.324843400\n",
      "[Iter.  190]  loss:4.433685  pct:-5.874073418\n",
      "[Iter.  200]  loss:4.192957  pct:-5.429532361\n",
      "[Iter.  210]  loss:3.982806  pct:-5.012004638\n",
      "[Iter.  220]  loss:3.799959  pct:-4.590909913\n",
      "[Iter.  230]  loss:3.640646  pct:-4.192485693\n",
      "[Iter.  240]  loss:3.502007  pct:-3.808074093\n",
      "[Iter.  250]  loss:3.381462  pct:-3.442186409\n",
      "[Iter.  260]  loss:3.276003  pct:-3.118732099\n",
      "[Iter.  270]  loss:3.183541  pct:-2.822397323\n",
      "[Iter.  280]  loss:3.103465  pct:-2.515318953\n",
      "[Iter.  290]  loss:3.033627  pct:-2.250332568\n",
      "[Iter.  300]  loss:2.972749  pct:-2.006774153\n",
      "[Iter.  310]  loss:2.919643  pct:-1.786413756\n",
      "[Iter.  320]  loss:2.873275  pct:-1.588151631\n",
      "[Iter.  330]  loss:2.832708  pct:-1.411878864\n",
      "[Iter.  340]  loss:2.797272  pct:-1.250947187\n",
      "[Iter.  350]  loss:2.766364  pct:-1.104920483\n",
      "[Iter.  360]  loss:2.739331  pct:-0.977206445\n",
      "[Iter.  370]  loss:2.715674  pct:-0.863607990\n",
      "[Iter.  380]  loss:2.694951  pct:-0.763083670\n",
      "[Iter.  390]  loss:2.676754  pct:-0.675236620\n",
      "[Iter.  400]  loss:2.660759  pct:-0.597535254\n",
      "[Iter.  410]  loss:2.646802  pct:-0.524568294\n",
      "[Iter.  420]  loss:2.634551  pct:-0.462856704\n",
      "[Iter.  430]  loss:2.623795  pct:-0.408258453\n",
      "[Iter.  440]  loss:2.614350  pct:-0.360000164\n",
      "[Iter.  450]  loss:2.606033  pct:-0.318101238\n",
      "[Iter.  460]  loss:2.598699  pct:-0.281432791\n",
      "[Iter.  470]  loss:2.592236  pct:-0.248694142\n",
      "[Iter.  480]  loss:2.586534  pct:-0.219974437\n",
      "[Iter.  490]  loss:2.581499  pct:-0.194659088\n",
      "[Iter.  500]  loss:2.577048  pct:-0.172420608\n",
      "[Iter.  510]  loss:2.573112  pct:-0.152734921\n",
      "[Iter.  520]  loss:2.569630  pct:-0.135335802\n",
      "[Iter.  530]  loss:2.566542  pct:-0.120172859\n",
      "[Iter.  540]  loss:2.563808  pct:-0.106522558\n",
      "[Iter.  550]  loss:2.561377  pct:-0.094816620\n",
      "[Iter.  560]  loss:2.559191  pct:-0.085319141\n",
      "[Iter.  570]  loss:2.557240  pct:-0.076243520\n",
      "[Iter.  580]  loss:2.555499  pct:-0.068078565\n",
      "[Iter.  590]  loss:2.553943  pct:-0.060903811\n",
      "[Iter.  600]  loss:2.552547  pct:-0.054658261\n",
      "[Iter.  610]  loss:2.551295  pct:-0.049055880\n",
      "[Iter.  620]  loss:2.550172  pct:-0.044024310\n",
      "[Iter.  630]  loss:2.549161  pct:-0.039621566\n",
      "[Iter.  640]  loss:2.548253  pct:-0.035634262\n",
      "[Iter.  650]  loss:2.547434  pct:-0.032119693\n",
      "[Iter.  660]  loss:2.546697  pct:-0.028957256\n",
      "[Iter.  670]  loss:2.546031  pct:-0.026147719\n",
      "[Iter.  680]  loss:2.545428  pct:-0.023663648\n",
      "[Iter.  690]  loss:2.544884  pct:-0.021393179\n",
      "[Iter.  700]  loss:2.544392  pct:-0.019308571\n",
      "[Iter.  710]  loss:2.543949  pct:-0.017428859\n",
      "[Iter.  720]  loss:2.543551  pct:-0.015641848\n",
      "[Iter.  730]  loss:2.543197  pct:-0.013919579\n",
      "[Iter.  740]  loss:2.542877  pct:-0.012580926\n",
      "[Iter.  750]  loss:2.542567  pct:-0.012188720\n",
      "[Iter.  760]  loss:2.542283  pct:-0.011186858\n",
      "[Iter.  770]  loss:2.542022  pct:-0.010240919\n",
      "[Iter.  780]  loss:2.541785  pct:-0.009313437\n",
      "[Iter.  790]  loss:2.541568  pct:-0.008545148\n",
      "[Iter.  800]  loss:2.541370  pct:-0.007804798\n",
      "[Iter.  810]  loss:2.541189  pct:-0.007129939\n",
      "[Iter.  820]  loss:2.541023  pct:-0.006511224\n",
      "[Iter.  830]  loss:2.540874  pct:-0.005892385\n",
      "[Iter.  840]  loss:2.540739  pct:-0.005282815\n",
      "[Iter.  850]  loss:2.540620  pct:-0.004710681\n",
      "[Iter.  860]  loss:2.540520  pct:-0.003932009\n",
      "[Iter.  870]  loss:2.540431  pct:-0.003500470\n",
      "[Iter.  880]  loss:2.540335  pct:-0.003782142\n",
      "[Iter.  890]  loss:2.540244  pct:-0.003566422\n",
      "[Iter.  900]  loss:2.540162  pct:-0.003247437\n",
      "[Iter.  910]  loss:2.540088  pct:-0.002909648\n",
      "[Iter.  920]  loss:2.540023  pct:-0.002562442\n",
      "[Iter.  930]  loss:2.539967  pct:-0.002187049\n",
      "[Iter.  940]  loss:2.539921  pct:-0.001830403\n",
      "[Iter.  950]  loss:2.539865  pct:-0.002168363\n",
      "[Iter.  960]  loss:2.539812  pct:-0.002102700\n",
      "[Iter.  970]  loss:2.539763  pct:-0.001933774\n",
      "[Iter.  980]  loss:2.539720  pct:-0.001708513\n",
      "[Iter.  990]  loss:2.539680  pct:-0.001548953\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.538380  pct:100.000000000\n",
      "[Iter.    2]  loss:2.538379  pct:-0.000046963\n",
      "[Iter.    4]  loss:2.538378  pct:-0.000028178\n",
      "[Iter.    6]  loss:2.538378  pct:-0.000009393\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.538378\n",
      "Best loss: 2.538378 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 346/10000 [00:03<01:31, 105.43it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:2505.278076  pct:100.000000000\n",
      "[Iter.   10]  loss:399.369171  pct:-84.058889733\n",
      "[Iter.   20]  loss:380.242249  pct:-4.789283698\n",
      "[Iter.   30]  loss:364.973328  pct:-4.015577164\n",
      "[Iter.   40]  loss:351.011658  pct:-3.825394588\n",
      "[Iter.   50]  loss:338.102753  pct:-3.677628576\n",
      "[Iter.   60]  loss:326.121185  pct:-3.543765109\n",
      "[Iter.   70]  loss:314.985535  pct:-3.414574439\n",
      "[Iter.   80]  loss:304.597961  pct:-3.297793739\n",
      "[Iter.   90]  loss:294.858185  pct:-3.197584306\n",
      "[Iter.  100]  loss:285.692108  pct:-3.108639045\n",
      "[Iter.  110]  loss:277.043671  pct:-3.027188100\n",
      "[Iter.  120]  loss:268.866821  pct:-2.951465863\n",
      "[Iter.  130]  loss:261.121490  pct:-2.880731350\n",
      "[Iter.  140]  loss:253.771194  pct:-2.814895092\n",
      "[Iter.  150]  loss:246.782959  pct:-2.753754416\n",
      "[Iter.  160]  loss:240.127731  pct:-2.696793850\n",
      "[Iter.  170]  loss:233.779709  pct:-2.643602397\n",
      "[Iter.  180]  loss:227.716370  pct:-2.593612278\n",
      "[Iter.  190]  loss:221.916809  pct:-2.546835151\n",
      "[Iter.  200]  loss:216.362366  pct:-2.502939449\n",
      "[Iter.  210]  loss:211.036438  pct:-2.461577695\n",
      "[Iter.  220]  loss:205.923630  pct:-2.422713478\n",
      "[Iter.  230]  loss:201.010239  pct:-2.386025887\n",
      "[Iter.  240]  loss:196.283752  pct:-2.351365899\n",
      "[Iter.  250]  loss:191.732819  pct:-2.318548418\n",
      "[Iter.  260]  loss:187.347092  pct:-2.287415874\n",
      "[Iter.  270]  loss:183.116898  pct:-2.257944895\n",
      "[Iter.  280]  loss:179.033310  pct:-2.230044141\n",
      "[Iter.  290]  loss:175.088226  pct:-2.203547273\n",
      "[Iter.  300]  loss:171.274551  pct:-2.178144703\n",
      "[Iter.  310]  loss:167.585205  pct:-2.154053993\n",
      "[Iter.  320]  loss:164.013885  pct:-2.131047057\n",
      "[Iter.  330]  loss:160.554749  pct:-2.109051287\n",
      "[Iter.  340]  loss:157.202087  pct:-2.088173139\n",
      "[Iter.  350]  loss:153.950897  pct:-2.068159679\n",
      "[Iter.  360]  loss:150.796402  pct:-2.049026863\n",
      "[Iter.  370]  loss:147.734177  pct:-2.030701861\n",
      "[Iter.  380]  loss:144.760117  pct:-2.013115805\n",
      "[Iter.  390]  loss:141.870377  pct:-1.996226626\n",
      "[Iter.  400]  loss:139.061066  pct:-1.980195571\n",
      "[Iter.  410]  loss:136.328888  pct:-1.964732343\n",
      "[Iter.  420]  loss:133.670700  pct:-1.949834629\n",
      "[Iter.  430]  loss:131.083389  pct:-1.935585577\n",
      "[Iter.  440]  loss:128.564240  pct:-1.921791765\n",
      "[Iter.  450]  loss:126.110535  pct:-1.908543809\n",
      "[Iter.  460]  loss:123.719681  pct:-1.895839938\n",
      "[Iter.  470]  loss:121.389381  pct:-1.883531676\n",
      "[Iter.  480]  loss:119.117310  pct:-1.871722067\n",
      "[Iter.  490]  loss:116.901390  pct:-1.860283365\n",
      "[Iter.  500]  loss:114.739655  pct:-1.849195748\n",
      "[Iter.  510]  loss:112.630104  pct:-1.838553972\n",
      "[Iter.  520]  loss:110.570999  pct:-1.828201205\n",
      "[Iter.  530]  loss:108.560501  pct:-1.818286949\n",
      "[Iter.  540]  loss:106.597046  pct:-1.808627613\n",
      "[Iter.  550]  loss:104.679062  pct:-1.799284392\n",
      "[Iter.  560]  loss:102.805038  pct:-1.790256240\n",
      "[Iter.  570]  loss:100.973579  pct:-1.781487632\n",
      "[Iter.  580]  loss:99.183289  pct:-1.773028987\n",
      "[Iter.  590]  loss:97.432854  pct:-1.764848596\n",
      "[Iter.  600]  loss:95.721184  pct:-1.756768746\n",
      "[Iter.  610]  loss:94.047005  pct:-1.749016269\n",
      "[Iter.  620]  loss:92.409073  pct:-1.741609772\n",
      "[Iter.  630]  loss:90.806343  pct:-1.734385756\n",
      "[Iter.  640]  loss:89.237701  pct:-1.727458247\n",
      "[Iter.  650]  loss:87.702133  pct:-1.720761755\n",
      "[Iter.  660]  loss:86.199089  pct:-1.713805667\n",
      "[Iter.  670]  loss:84.727402  pct:-1.707311914\n",
      "[Iter.  680]  loss:83.286156  pct:-1.701038865\n",
      "[Iter.  690]  loss:81.874504  pct:-1.694941494\n",
      "[Iter.  700]  loss:80.491768  pct:-1.688848343\n",
      "[Iter.  710]  loss:79.137047  pct:-1.683055429\n",
      "[Iter.  720]  loss:77.809586  pct:-1.677420748\n",
      "[Iter.  730]  loss:76.508736  pct:-1.671837608\n",
      "[Iter.  740]  loss:75.233795  pct:-1.666398588\n",
      "[Iter.  750]  loss:73.984016  pct:-1.661193277\n",
      "[Iter.  760]  loss:72.758774  pct:-1.656090969\n",
      "[Iter.  770]  loss:71.557480  pct:-1.651064033\n",
      "[Iter.  780]  loss:70.379585  pct:-1.646081716\n",
      "[Iter.  790]  loss:69.224373  pct:-1.641402685\n",
      "[Iter.  800]  loss:68.091354  pct:-1.636733490\n",
      "[Iter.  810]  loss:66.979980  pct:-1.632180637\n",
      "[Iter.  820]  loss:65.889778  pct:-1.627654000\n",
      "[Iter.  830]  loss:64.820145  pct:-1.623367864\n",
      "[Iter.  840]  loss:63.770638  pct:-1.619106447\n",
      "[Iter.  850]  loss:62.740860  pct:-1.614814540\n",
      "[Iter.  860]  loss:61.730263  pct:-1.610748130\n",
      "[Iter.  870]  loss:60.738506  pct:-1.606596821\n",
      "[Iter.  880]  loss:59.765064  pct:-1.602677011\n",
      "[Iter.  890]  loss:58.809517  pct:-1.598839297\n",
      "[Iter.  900]  loss:57.871490  pct:-1.595024883\n",
      "[Iter.  910]  loss:56.950699  pct:-1.591097133\n",
      "[Iter.  920]  loss:56.046616  pct:-1.587484035\n",
      "[Iter.  930]  loss:55.158890  pct:-1.583906219\n",
      "[Iter.  940]  loss:54.287189  pct:-1.580344148\n",
      "[Iter.  950]  loss:53.431206  pct:-1.576769294\n",
      "[Iter.  960]  loss:52.590546  pct:-1.573350411\n",
      "[Iter.  970]  loss:51.764900  pct:-1.569950333\n",
      "[Iter.  980]  loss:50.953922  pct:-1.566656040\n",
      "[Iter.  990]  loss:50.157291  pct:-1.563433832\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.660293  pct:100.000000000\n",
      "[Iter.    2]  loss:2.659858  pct:-0.016337942\n",
      "[Iter.    4]  loss:2.659774  pct:-0.003164145\n",
      "[Iter.    6]  loss:2.659756  pct:-0.000654362\n",
      "[Iter.    8]  loss:2.659744  pct:-0.000484052\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.659744\n",
      "Best loss: 2.659744 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 344/10000 [00:04<02:09, 74.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:429.700073  pct:100.000000000\n",
      "[Iter.   10]  loss:82.563278  pct:-80.785832119\n",
      "[Iter.   20]  loss:70.733269  pct:-14.328415391\n",
      "[Iter.   30]  loss:62.101921  pct:-12.202670413\n",
      "[Iter.   40]  loss:55.401146  pct:-10.789964352\n",
      "[Iter.   50]  loss:49.953987  pct:-9.832213254\n",
      "[Iter.   60]  loss:45.398830  pct:-9.118704973\n",
      "[Iter.   70]  loss:41.511734  pct:-8.562106930\n",
      "[Iter.   80]  loss:38.143608  pct:-8.113671943\n",
      "[Iter.   90]  loss:35.189899  pct:-7.743652990\n",
      "[Iter.  100]  loss:32.574608  pct:-7.431938246\n",
      "[Iter.  110]  loss:30.240633  pct:-7.165012850\n",
      "[Iter.  120]  loss:28.144056  pct:-6.932978850\n",
      "[Iter.  130]  loss:26.250484  pct:-6.728141218\n",
      "[Iter.  140]  loss:24.532263  pct:-6.545485538\n",
      "[Iter.  150]  loss:22.966969  pct:-6.380553960\n",
      "[Iter.  160]  loss:21.536095  pct:-6.230138159\n",
      "[Iter.  170]  loss:20.224245  pct:-6.091399646\n",
      "[Iter.  180]  loss:19.018423  pct:-5.962259588\n",
      "[Iter.  190]  loss:17.907549  pct:-5.841042506\n",
      "[Iter.  200]  loss:16.882145  pct:-5.726098987\n",
      "[Iter.  210]  loss:15.934012  pct:-5.616185141\n",
      "[Iter.  220]  loss:15.055905  pct:-5.510897369\n",
      "[Iter.  230]  loss:14.241613  pct:-5.408455590\n",
      "[Iter.  240]  loss:13.485542  pct:-5.308886501\n",
      "[Iter.  250]  loss:12.782831  pct:-5.210847957\n",
      "[Iter.  260]  loss:12.129044  pct:-5.114575974\n",
      "[Iter.  270]  loss:11.520293  pct:-5.018947614\n",
      "[Iter.  280]  loss:10.953012  pct:-4.924186891\n",
      "[Iter.  290]  loss:10.424048  pct:-4.829393231\n",
      "[Iter.  300]  loss:9.930494  pct:-4.734764222\n",
      "[Iter.  310]  loss:9.469754  pct:-4.639649096\n",
      "[Iter.  320]  loss:9.039418  pct:-4.544320672\n",
      "[Iter.  330]  loss:8.637319  pct:-4.448290803\n",
      "[Iter.  340]  loss:8.261466  pct:-4.351496127\n",
      "[Iter.  350]  loss:7.910039  pct:-4.253804353\n",
      "[Iter.  360]  loss:7.581334  pct:-4.155545796\n",
      "[Iter.  370]  loss:7.273810  pct:-4.056333617\n",
      "[Iter.  380]  loss:6.986047  pct:-3.956147404\n",
      "[Iter.  390]  loss:6.716704  pct:-3.855440552\n",
      "[Iter.  400]  loss:6.464564  pct:-3.753932110\n",
      "[Iter.  410]  loss:6.228483  pct:-3.651919172\n",
      "[Iter.  420]  loss:6.007388  pct:-3.549749672\n",
      "[Iter.  430]  loss:5.800318  pct:-3.446912526\n",
      "[Iter.  440]  loss:5.606383  pct:-3.343530238\n",
      "[Iter.  450]  loss:5.424710  pct:-3.240459634\n",
      "[Iter.  460]  loss:5.254521  pct:-3.137290200\n",
      "[Iter.  470]  loss:5.095088  pct:-3.034221968\n",
      "[Iter.  480]  loss:4.945733  pct:-2.931342338\n",
      "[Iter.  490]  loss:4.805764  pct:-2.830103180\n",
      "[Iter.  500]  loss:4.674628  pct:-2.728722179\n",
      "[Iter.  510]  loss:4.551755  pct:-2.628494895\n",
      "[Iter.  520]  loss:4.436601  pct:-2.529886945\n",
      "[Iter.  530]  loss:4.328702  pct:-2.432023638\n",
      "[Iter.  540]  loss:4.227592  pct:-2.335814732\n",
      "[Iter.  550]  loss:4.132840  pct:-2.241260957\n",
      "[Iter.  560]  loss:4.044047  pct:-2.148480812\n",
      "[Iter.  570]  loss:3.960834  pct:-2.057668800\n",
      "[Iter.  580]  loss:3.882866  pct:-1.968471446\n",
      "[Iter.  590]  loss:3.809787  pct:-1.882098156\n",
      "[Iter.  600]  loss:3.741304  pct:-1.797538994\n",
      "[Iter.  610]  loss:3.677151  pct:-1.714734491\n",
      "[Iter.  620]  loss:3.617023  pct:-1.635165409\n",
      "[Iter.  630]  loss:3.560687  pct:-1.557528412\n",
      "[Iter.  640]  loss:3.507890  pct:-1.482777849\n",
      "[Iter.  650]  loss:3.458420  pct:-1.410254781\n",
      "[Iter.  660]  loss:3.412068  pct:-1.340269566\n",
      "[Iter.  670]  loss:3.368638  pct:-1.272823916\n",
      "[Iter.  680]  loss:3.327926  pct:-1.208548999\n",
      "[Iter.  690]  loss:3.289777  pct:-1.146347326\n",
      "[Iter.  700]  loss:3.254024  pct:-1.086798658\n",
      "[Iter.  710]  loss:3.220527  pct:-1.029376122\n",
      "[Iter.  720]  loss:3.189134  pct:-0.974802025\n",
      "[Iter.  730]  loss:3.159716  pct:-0.922429665\n",
      "[Iter.  740]  loss:3.132157  pct:-0.872192388\n",
      "[Iter.  750]  loss:3.106317  pct:-0.824991893\n",
      "[Iter.  760]  loss:3.082111  pct:-0.779255926\n",
      "[Iter.  770]  loss:3.059423  pct:-0.736108229\n",
      "[Iter.  780]  loss:3.038164  pct:-0.694879549\n",
      "[Iter.  790]  loss:3.018239  pct:-0.655843247\n",
      "[Iter.  800]  loss:2.999564  pct:-0.618709719\n",
      "[Iter.  810]  loss:2.982069  pct:-0.583264480\n",
      "[Iter.  820]  loss:2.965673  pct:-0.549821134\n",
      "[Iter.  830]  loss:2.950306  pct:-0.518163373\n",
      "[Iter.  840]  loss:2.935899  pct:-0.488335671\n",
      "[Iter.  850]  loss:2.922395  pct:-0.459962363\n",
      "[Iter.  860]  loss:2.909741  pct:-0.432978873\n",
      "[Iter.  870]  loss:2.897871  pct:-0.407953270\n",
      "[Iter.  880]  loss:2.886746  pct:-0.383905584\n",
      "[Iter.  890]  loss:2.876320  pct:-0.361152964\n",
      "[Iter.  900]  loss:2.866544  pct:-0.339899310\n",
      "[Iter.  910]  loss:2.857377  pct:-0.319791269\n",
      "[Iter.  920]  loss:2.848788  pct:-0.300558061\n",
      "[Iter.  930]  loss:2.840731  pct:-0.282826102\n",
      "[Iter.  940]  loss:2.833179  pct:-0.265868987\n",
      "[Iter.  950]  loss:2.826096  pct:-0.250008142\n",
      "[Iter.  960]  loss:2.819453  pct:-0.235061295\n",
      "[Iter.  970]  loss:2.813218  pct:-0.221138118\n",
      "[Iter.  980]  loss:2.807367  pct:-0.207966616\n",
      "[Iter.  990]  loss:2.801881  pct:-0.195414826\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.714365  pct:100.000000000\n",
      "[Iter.    2]  loss:2.714376  pct:0.000404045\n",
      "[Iter.    4]  loss:2.714376  pct:0.000008784\n",
      "[Iter.    6]  loss:2.714378  pct:0.000070268\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.714378\n",
      "Best loss: 2.714378 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  9%|▉         | 925/10000 [00:12<02:05, 72.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:4461.287109  pct:100.000000000\n",
      "[Iter.   10]  loss:794.447144  pct:-82.192420121\n",
      "[Iter.   20]  loss:767.900513  pct:-3.341522602\n",
      "[Iter.   30]  loss:743.641541  pct:-3.159129570\n",
      "[Iter.   40]  loss:721.029663  pct:-3.040695847\n",
      "[Iter.   50]  loss:699.913574  pct:-2.928601963\n",
      "[Iter.   60]  loss:680.163208  pct:-2.821829286\n",
      "[Iter.   70]  loss:661.660217  pct:-2.720375125\n",
      "[Iter.   80]  loss:644.292786  pct:-2.624826336\n",
      "[Iter.   90]  loss:627.955383  pct:-2.535710892\n",
      "[Iter.  100]  loss:612.549133  pct:-2.453398826\n",
      "[Iter.  110]  loss:597.985046  pct:-2.377619381\n",
      "[Iter.  120]  loss:584.183350  pct:-2.308033765\n",
      "[Iter.  130]  loss:571.075012  pct:-2.243873847\n",
      "[Iter.  140]  loss:558.585999  pct:-2.186930509\n",
      "[Iter.  150]  loss:546.695679  pct:-2.128646234\n",
      "[Iter.  160]  loss:535.351013  pct:-2.075133565\n",
      "[Iter.  170]  loss:524.500427  pct:-2.026817111\n",
      "[Iter.  180]  loss:514.104797  pct:-1.982005989\n",
      "[Iter.  190]  loss:504.130219  pct:-1.940183968\n",
      "[Iter.  200]  loss:494.546478  pct:-1.901044588\n",
      "[Iter.  210]  loss:485.326141  pct:-1.864402502\n",
      "[Iter.  220]  loss:476.444458  pct:-1.830044292\n",
      "[Iter.  230]  loss:467.878693  pct:-1.797851824\n",
      "[Iter.  240]  loss:459.608307  pct:-1.767634618\n",
      "[Iter.  250]  loss:451.614563  pct:-1.739251397\n",
      "[Iter.  260]  loss:443.881256  pct:-1.712368803\n",
      "[Iter.  270]  loss:436.392578  pct:-1.687090382\n",
      "[Iter.  280]  loss:429.134430  pct:-1.663215315\n",
      "[Iter.  290]  loss:422.093628  pct:-1.640698464\n",
      "[Iter.  300]  loss:415.258331  pct:-1.619379251\n",
      "[Iter.  310]  loss:408.617645  pct:-1.599169850\n",
      "[Iter.  320]  loss:402.160858  pct:-1.580153766\n",
      "[Iter.  330]  loss:395.877533  pct:-1.562391035\n",
      "[Iter.  340]  loss:389.758240  pct:-1.545754104\n",
      "[Iter.  350]  loss:383.794556  pct:-1.530098270\n",
      "[Iter.  360]  loss:377.978455  pct:-1.515420422\n",
      "[Iter.  370]  loss:372.305786  pct:-1.500791484\n",
      "[Iter.  380]  loss:366.776154  pct:-1.485239492\n",
      "[Iter.  390]  loss:361.390228  pct:-1.468450236\n",
      "[Iter.  400]  loss:356.144836  pct:-1.451448167\n",
      "[Iter.  410]  loss:351.035095  pct:-1.434736851\n",
      "[Iter.  420]  loss:346.052551  pct:-1.419386270\n",
      "[Iter.  430]  loss:341.194641  pct:-1.403807063\n",
      "[Iter.  440]  loss:336.453613  pct:-1.389537601\n",
      "[Iter.  450]  loss:331.821991  pct:-1.376600557\n",
      "[Iter.  460]  loss:327.296143  pct:-1.363938651\n",
      "[Iter.  470]  loss:322.871582  pct:-1.351852335\n",
      "[Iter.  480]  loss:318.544739  pct:-1.340112758\n",
      "[Iter.  490]  loss:314.311951  pct:-1.328789200\n",
      "[Iter.  500]  loss:310.169830  pct:-1.317837375\n",
      "[Iter.  510]  loss:306.115295  pct:-1.307198353\n",
      "[Iter.  520]  loss:302.144653  pct:-1.297106727\n",
      "[Iter.  530]  loss:298.254791  pct:-1.287417142\n",
      "[Iter.  540]  loss:294.443024  pct:-1.278023921\n",
      "[Iter.  550]  loss:290.707275  pct:-1.268750825\n",
      "[Iter.  560]  loss:287.045288  pct:-1.259682029\n",
      "[Iter.  570]  loss:283.454468  pct:-1.250959504\n",
      "[Iter.  580]  loss:279.932922  pct:-1.242367227\n",
      "[Iter.  590]  loss:276.478424  pct:-1.234045021\n",
      "[Iter.  600]  loss:273.088745  pct:-1.226019342\n",
      "[Iter.  610]  loss:269.762146  pct:-1.218138492\n",
      "[Iter.  620]  loss:266.496582  pct:-1.210534544\n",
      "[Iter.  630]  loss:263.290192  pct:-1.203163792\n",
      "[Iter.  640]  loss:260.141632  pct:-1.195851448\n",
      "[Iter.  650]  loss:257.049103  pct:-1.188786767\n",
      "[Iter.  660]  loss:254.010986  pct:-1.181920661\n",
      "[Iter.  670]  loss:251.026154  pct:-1.175080183\n",
      "[Iter.  680]  loss:248.092773  pct:-1.168555581\n",
      "[Iter.  690]  loss:245.209488  pct:-1.162180374\n",
      "[Iter.  700]  loss:242.375000  pct:-1.155945449\n",
      "[Iter.  710]  loss:239.588028  pct:-1.149859534\n",
      "[Iter.  720]  loss:236.847214  pct:-1.143969602\n",
      "[Iter.  730]  loss:234.151505  pct:-1.138163792\n",
      "[Iter.  740]  loss:231.499588  pct:-1.132564367\n",
      "[Iter.  750]  loss:228.890518  pct:-1.127030008\n",
      "[Iter.  760]  loss:226.323288  pct:-1.121597454\n",
      "[Iter.  770]  loss:223.796967  pct:-1.116244569\n",
      "[Iter.  780]  loss:221.310379  pct:-1.111090808\n",
      "[Iter.  790]  loss:218.862396  pct:-1.106131036\n",
      "[Iter.  800]  loss:216.452194  pct:-1.101240811\n",
      "[Iter.  810]  loss:214.078964  pct:-1.096422233\n",
      "[Iter.  820]  loss:211.741714  pct:-1.091769929\n",
      "[Iter.  830]  loss:209.439728  pct:-1.087167307\n",
      "[Iter.  840]  loss:207.172150  pct:-1.082687678\n",
      "[Iter.  850]  loss:204.938293  pct:-1.078260859\n",
      "[Iter.  860]  loss:202.737305  pct:-1.073976333\n",
      "[Iter.  870]  loss:200.568634  pct:-1.069694923\n",
      "[Iter.  880]  loss:198.431488  pct:-1.065543477\n",
      "[Iter.  890]  loss:196.325150  pct:-1.061494081\n",
      "[Iter.  900]  loss:194.248932  pct:-1.057540339\n",
      "[Iter.  910]  loss:192.202103  pct:-1.053714532\n",
      "[Iter.  920]  loss:190.184448  pct:-1.049756684\n",
      "[Iter.  930]  loss:188.195038  pct:-1.046042628\n",
      "[Iter.  940]  loss:186.233490  pct:-1.042295203\n",
      "[Iter.  950]  loss:184.299088  pct:-1.038697425\n",
      "[Iter.  960]  loss:182.391144  pct:-1.035243175\n",
      "[Iter.  970]  loss:180.509460  pct:-1.031674735\n",
      "[Iter.  980]  loss:178.653122  pct:-1.028388482\n",
      "[Iter.  990]  loss:176.821869  pct:-1.025032774\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.897526  pct:100.000000000\n",
      "[Iter.    2]  loss:2.895466  pct:-0.071092943\n",
      "[Iter.    4]  loss:2.894886  pct:-0.020050286\n",
      "[Iter.    6]  loss:2.894615  pct:-0.009355931\n",
      "[Iter.    8]  loss:2.894501  pct:-0.003912397\n",
      "[Iter.   10]  loss:2.894496  pct:-0.000181213\n",
      "[Iter.   12]  loss:2.894451  pct:-0.001565023\n",
      "[Iter.   14]  loss:2.894413  pct:-0.001309698\n",
      "[Iter.   16]  loss:2.894427  pct:0.000485995\n",
      "[Iter.   18]  loss:2.894381  pct:-0.001598009\n",
      "[Iter.   20]  loss:2.894352  pct:-0.000980238\n",
      "[Iter.   22]  loss:2.894326  pct:-0.000897874\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.894326\n",
      "Best loss: 2.894326 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 20%|█▉        | 1959/10000 [00:25<01:44, 76.92it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:332.280029  pct:100.000000000\n",
      "[Iter.   10]  loss:72.739754  pct:-78.108902227\n",
      "[Iter.   20]  loss:60.937599  pct:-16.225178031\n",
      "[Iter.   30]  loss:52.741753  pct:-13.449572460\n",
      "[Iter.   40]  loss:46.493813  pct:-11.846288287\n",
      "[Iter.   50]  loss:41.485699  pct:-10.771570635\n",
      "[Iter.   60]  loss:37.313370  pct:-10.057270529\n",
      "[Iter.   70]  loss:33.740635  pct:-9.574945540\n",
      "[Iter.   80]  loss:30.714371  pct:-8.969197521\n",
      "[Iter.   90]  loss:28.105555  pct:-8.493796503\n",
      "[Iter.  100]  loss:25.815355  pct:-8.148564631\n",
      "[Iter.  110]  loss:23.798769  pct:-7.811576793\n",
      "[Iter.  120]  loss:22.014214  pct:-7.498519925\n",
      "[Iter.  130]  loss:20.420172  pct:-7.240966478\n",
      "[Iter.  140]  loss:18.985294  pct:-7.026764584\n",
      "[Iter.  150]  loss:17.689152  pct:-6.827087085\n",
      "[Iter.  160]  loss:16.513372  pct:-6.646894992\n",
      "[Iter.  170]  loss:15.443386  pct:-6.479514397\n",
      "[Iter.  180]  loss:14.467168  pct:-6.321270599\n",
      "[Iter.  190]  loss:13.574552  pct:-6.169944809\n",
      "[Iter.  200]  loss:12.756751  pct:-6.024512242\n",
      "[Iter.  210]  loss:12.006276  pct:-5.882962882\n",
      "[Iter.  220]  loss:11.316550  pct:-5.744711086\n",
      "[Iter.  230]  loss:10.681844  pct:-5.608657081\n",
      "[Iter.  240]  loss:10.097142  pct:-5.473788527\n",
      "[Iter.  250]  loss:9.557946  pct:-5.340085370\n",
      "[Iter.  260]  loss:9.060309  pct:-5.206524334\n",
      "[Iter.  270]  loss:8.600673  pct:-5.073079378\n",
      "[Iter.  280]  loss:8.175836  pct:-4.939580032\n",
      "[Iter.  290]  loss:7.782932  pct:-4.805671537\n",
      "[Iter.  300]  loss:7.419394  pct:-4.670968144\n",
      "[Iter.  310]  loss:7.082895  pct:-4.535394107\n",
      "[Iter.  320]  loss:6.771278  pct:-4.399569756\n",
      "[Iter.  330]  loss:6.482601  pct:-4.263253448\n",
      "[Iter.  340]  loss:6.215113  pct:-4.126244943\n",
      "[Iter.  350]  loss:5.967185  pct:-3.989109437\n",
      "[Iter.  360]  loss:5.737348  pct:-3.851688836\n",
      "[Iter.  370]  loss:5.524243  pct:-3.714350232\n",
      "[Iter.  380]  loss:5.326626  pct:-3.577260839\n",
      "[Iter.  390]  loss:5.143318  pct:-3.441364027\n",
      "[Iter.  400]  loss:4.973291  pct:-3.305780220\n",
      "[Iter.  410]  loss:4.815552  pct:-3.171716438\n",
      "[Iter.  420]  loss:4.669210  pct:-3.038941196\n",
      "[Iter.  430]  loss:4.533429  pct:-2.908024106\n",
      "[Iter.  440]  loss:4.407432  pct:-2.779288607\n",
      "[Iter.  450]  loss:4.290522  pct:-2.652554015\n",
      "[Iter.  460]  loss:4.182042  pct:-2.528363079\n",
      "[Iter.  470]  loss:4.081376  pct:-2.407114038\n",
      "[Iter.  480]  loss:3.987953  pct:-2.288999114\n",
      "[Iter.  490]  loss:3.901250  pct:-2.174124499\n",
      "[Iter.  500]  loss:3.820796  pct:-2.062265004\n",
      "[Iter.  510]  loss:3.746127  pct:-1.954275724\n",
      "[Iter.  520]  loss:3.676839  pct:-1.849597086\n",
      "[Iter.  530]  loss:3.612541  pct:-1.748728211\n",
      "[Iter.  540]  loss:3.552866  pct:-1.651890491\n",
      "[Iter.  550]  loss:3.497488  pct:-1.558684395\n",
      "[Iter.  560]  loss:3.446103  pct:-1.469195392\n",
      "[Iter.  570]  loss:3.398422  pct:-1.383609882\n",
      "[Iter.  580]  loss:3.354169  pct:-1.302152411\n",
      "[Iter.  590]  loss:3.313095  pct:-1.224580809\n",
      "[Iter.  600]  loss:3.274988  pct:-1.150176592\n",
      "[Iter.  610]  loss:3.239622  pct:-1.079898025\n",
      "[Iter.  620]  loss:3.206797  pct:-1.013235321\n",
      "[Iter.  630]  loss:3.176342  pct:-0.949682761\n",
      "[Iter.  640]  loss:3.148085  pct:-0.889619747\n",
      "[Iter.  650]  loss:3.121855  pct:-0.833207993\n",
      "[Iter.  660]  loss:3.097512  pct:-0.779753550\n",
      "[Iter.  670]  loss:3.074916  pct:-0.729500240\n",
      "[Iter.  680]  loss:3.053944  pct:-0.682043108\n",
      "[Iter.  690]  loss:3.034481  pct:-0.637293539\n",
      "[Iter.  700]  loss:3.016419  pct:-0.595229116\n",
      "[Iter.  710]  loss:2.999651  pct:-0.555882343\n",
      "[Iter.  720]  loss:2.984085  pct:-0.518946586\n",
      "[Iter.  730]  loss:2.969636  pct:-0.484182127\n",
      "[Iter.  740]  loss:2.956224  pct:-0.451645800\n",
      "[Iter.  750]  loss:2.943771  pct:-0.421241454\n",
      "[Iter.  760]  loss:2.932210  pct:-0.392716621\n",
      "[Iter.  770]  loss:2.921477  pct:-0.366042199\n",
      "[Iter.  780]  loss:2.911506  pct:-0.341304783\n",
      "[Iter.  790]  loss:2.902253  pct:-0.317808876\n",
      "[Iter.  800]  loss:2.893664  pct:-0.295959922\n",
      "[Iter.  810]  loss:2.885686  pct:-0.275704556\n",
      "[Iter.  820]  loss:2.878281  pct:-0.256604670\n",
      "[Iter.  830]  loss:2.871401  pct:-0.239024849\n",
      "[Iter.  840]  loss:2.865004  pct:-0.222791841\n",
      "[Iter.  850]  loss:2.859056  pct:-0.207594483\n",
      "[Iter.  860]  loss:2.853530  pct:-0.193274527\n",
      "[Iter.  870]  loss:2.848396  pct:-0.179929539\n",
      "[Iter.  880]  loss:2.843624  pct:-0.167522681\n",
      "[Iter.  890]  loss:2.839189  pct:-0.155973514\n",
      "[Iter.  900]  loss:2.835064  pct:-0.145275335\n",
      "[Iter.  910]  loss:2.831233  pct:-0.135159659\n",
      "[Iter.  920]  loss:2.827666  pct:-0.125986838\n",
      "[Iter.  930]  loss:2.824350  pct:-0.117250385\n",
      "[Iter.  940]  loss:2.821265  pct:-0.109241939\n",
      "[Iter.  950]  loss:2.818393  pct:-0.101789518\n",
      "[Iter.  960]  loss:2.815719  pct:-0.094871949\n",
      "[Iter.  970]  loss:2.813230  pct:-0.088399796\n",
      "[Iter.  980]  loss:2.810914  pct:-0.082333704\n",
      "[Iter.  990]  loss:2.808756  pct:-0.076769574\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.776982  pct:100.000000000\n",
      "[Iter.    2]  loss:2.776981  pct:-0.000008586\n",
      "[Iter.    4]  loss:2.776979  pct:-0.000077270\n",
      "[Iter.    6]  loss:2.776979  pct:-0.000008586\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.776979\n",
      "Best loss: 2.776979 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:04<00:00, 80.64it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:267.637024  pct:100.000000000\n",
      "[Iter.   10]  loss:60.815868  pct:-77.276734250\n",
      "[Iter.   20]  loss:48.614662  pct:-20.062537184\n",
      "[Iter.   30]  loss:41.327072  pct:-14.990518707\n",
      "[Iter.   40]  loss:35.981075  pct:-12.935822887\n",
      "[Iter.   50]  loss:31.811619  pct:-11.587915171\n",
      "[Iter.   60]  loss:28.418209  pct:-10.667202288\n",
      "[Iter.   70]  loss:25.586111  pct:-9.965786372\n",
      "[Iter.   80]  loss:23.178312  pct:-9.410569510\n",
      "[Iter.   90]  loss:21.102222  pct:-8.957036354\n",
      "[Iter.  100]  loss:19.292719  pct:-8.574943043\n",
      "[Iter.  110]  loss:17.702265  pct:-8.243804882\n",
      "[Iter.  120]  loss:16.294779  pct:-7.950880743\n",
      "[Iter.  130]  loss:15.042315  pct:-7.686292081\n",
      "[Iter.  140]  loss:13.922645  pct:-7.443468304\n",
      "[Iter.  150]  loss:12.917834  pct:-7.217093886\n",
      "[Iter.  160]  loss:12.013138  pct:-7.003468575\n",
      "[Iter.  170]  loss:11.196357  pct:-6.799064961\n",
      "[Iter.  180]  loss:10.457230  pct:-6.601497023\n",
      "[Iter.  190]  loss:9.786997  pct:-6.409276621\n",
      "[Iter.  200]  loss:9.178198  pct:-6.220488170\n",
      "[Iter.  210]  loss:8.624422  pct:-6.033600449\n",
      "[Iter.  220]  loss:8.120048  pct:-5.848212202\n",
      "[Iter.  230]  loss:7.660182  pct:-5.663341784\n",
      "[Iter.  240]  loss:7.240507  pct:-5.478648192\n",
      "[Iter.  250]  loss:6.857202  pct:-5.293904736\n",
      "[Iter.  260]  loss:6.506900  pct:-5.108516380\n",
      "[Iter.  270]  loss:6.186555  pct:-4.923164433\n",
      "[Iter.  280]  loss:5.893446  pct:-4.737837851\n",
      "[Iter.  290]  loss:5.625148  pct:-4.552483395\n",
      "[Iter.  300]  loss:5.379474  pct:-4.367425376\n",
      "[Iter.  310]  loss:5.154469  pct:-4.182652172\n",
      "[Iter.  320]  loss:4.948317  pct:-3.999489348\n",
      "[Iter.  330]  loss:4.759408  pct:-3.817633212\n",
      "[Iter.  340]  loss:4.586276  pct:-3.637688126\n",
      "[Iter.  350]  loss:4.427572  pct:-3.460396666\n",
      "[Iter.  360]  loss:4.282088  pct:-3.285874047\n",
      "[Iter.  370]  loss:4.148680  pct:-3.115480086\n",
      "[Iter.  380]  loss:4.026343  pct:-2.948825533\n",
      "[Iter.  390]  loss:3.914171  pct:-2.785937889\n",
      "[Iter.  400]  loss:3.811307  pct:-2.627989809\n",
      "[Iter.  410]  loss:3.716970  pct:-2.475193269\n",
      "[Iter.  420]  loss:3.630468  pct:-2.327220032\n",
      "[Iter.  430]  loss:3.551137  pct:-2.185143026\n",
      "[Iter.  440]  loss:3.478384  pct:-2.048742798\n",
      "[Iter.  450]  loss:3.411650  pct:-1.918530155\n",
      "[Iter.  460]  loss:3.350443  pct:-1.794053403\n",
      "[Iter.  470]  loss:3.294318  pct:-1.675156567\n",
      "[Iter.  480]  loss:3.242844  pct:-1.562497060\n",
      "[Iter.  490]  loss:3.195634  pct:-1.455834860\n",
      "[Iter.  500]  loss:3.152343  pct:-1.354687622\n",
      "[Iter.  510]  loss:3.112636  pct:-1.259608691\n",
      "[Iter.  520]  loss:3.076224  pct:-1.169804846\n",
      "[Iter.  530]  loss:3.042816  pct:-1.086012157\n",
      "[Iter.  540]  loss:3.012130  pct:-1.008454931\n",
      "[Iter.  550]  loss:2.984007  pct:-0.933654905\n",
      "[Iter.  560]  loss:2.958233  pct:-0.863745876\n",
      "[Iter.  570]  loss:2.934596  pct:-0.799042274\n",
      "[Iter.  580]  loss:2.912911  pct:-0.738939469\n",
      "[Iter.  590]  loss:2.893018  pct:-0.682930974\n",
      "[Iter.  600]  loss:2.874766  pct:-0.630894757\n",
      "[Iter.  610]  loss:2.858018  pct:-0.582576642\n",
      "[Iter.  620]  loss:2.842661  pct:-0.537314359\n",
      "[Iter.  630]  loss:2.828572  pct:-0.495656038\n",
      "[Iter.  640]  loss:2.815640  pct:-0.457185667\n",
      "[Iter.  650]  loss:2.803773  pct:-0.421443516\n",
      "[Iter.  660]  loss:2.792884  pct:-0.388379879\n",
      "[Iter.  670]  loss:2.782891  pct:-0.357796368\n",
      "[Iter.  680]  loss:2.773724  pct:-0.329412595\n",
      "[Iter.  690]  loss:2.765309  pct:-0.303373554\n",
      "[Iter.  700]  loss:2.757575  pct:-0.279698804\n",
      "[Iter.  710]  loss:2.750472  pct:-0.257580187\n",
      "[Iter.  720]  loss:2.743945  pct:-0.237302813\n",
      "[Iter.  730]  loss:2.737949  pct:-0.218525791\n",
      "[Iter.  740]  loss:2.732443  pct:-0.201100874\n",
      "[Iter.  750]  loss:2.727382  pct:-0.185189467\n",
      "[Iter.  760]  loss:2.722729  pct:-0.170602294\n",
      "[Iter.  770]  loss:2.718453  pct:-0.157067169\n",
      "[Iter.  780]  loss:2.714520  pct:-0.144667374\n",
      "[Iter.  790]  loss:2.710905  pct:-0.133177896\n",
      "[Iter.  800]  loss:2.707579  pct:-0.122704998\n",
      "[Iter.  810]  loss:2.704519  pct:-0.113002280\n",
      "[Iter.  820]  loss:2.701702  pct:-0.104164692\n",
      "[Iter.  830]  loss:2.699108  pct:-0.096022162\n",
      "[Iter.  840]  loss:2.696721  pct:-0.088429537\n",
      "[Iter.  850]  loss:2.694522  pct:-0.081523370\n",
      "[Iter.  860]  loss:2.692496  pct:-0.075192587\n",
      "[Iter.  870]  loss:2.690630  pct:-0.069298658\n",
      "[Iter.  880]  loss:2.688909  pct:-0.063976907\n",
      "[Iter.  890]  loss:2.687325  pct:-0.058928355\n",
      "[Iter.  900]  loss:2.685863  pct:-0.054367422\n",
      "[Iter.  910]  loss:2.684515  pct:-0.050198272\n",
      "[Iter.  920]  loss:2.683273  pct:-0.046262445\n",
      "[Iter.  930]  loss:2.682127  pct:-0.042729712\n",
      "[Iter.  940]  loss:2.681071  pct:-0.039361207\n",
      "[Iter.  950]  loss:2.680101  pct:-0.036166455\n",
      "[Iter.  960]  loss:2.679201  pct:-0.033608631\n",
      "[Iter.  970]  loss:2.678360  pct:-0.031359617\n",
      "[Iter.  980]  loss:2.677583  pct:-0.029046121\n",
      "[Iter.  990]  loss:2.676862  pct:-0.026899732\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.666636  pct:100.000000000\n",
      "[Iter.    2]  loss:2.666637  pct:0.000026822\n",
      "[Iter.    4]  loss:2.666637  pct:0.000026822\n",
      "[Iter.    6]  loss:2.666637  pct:-0.000008941\n",
      "[Iter.    8]  loss:2.666637  pct:0.000008941\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.666637\n",
      "Best loss: 2.666637 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 29%|██▉       | 2938/10000 [00:44<01:45, 66.64it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:407.840179  pct:100.000000000\n",
      "[Iter.   10]  loss:67.156166  pct:-83.533707116\n",
      "[Iter.   20]  loss:57.755154  pct:-13.998733057\n",
      "[Iter.   30]  loss:51.055450  pct:-11.600182481\n",
      "[Iter.   40]  loss:45.644173  pct:-10.598824855\n",
      "[Iter.   50]  loss:41.146584  pct:-9.853588856\n",
      "[Iter.   60]  loss:37.336281  pct:-9.260313749\n",
      "[Iter.   70]  loss:34.061069  pct:-8.772194932\n",
      "[Iter.   80]  loss:31.210579  pct:-8.368764143\n",
      "[Iter.   90]  loss:28.704720  pct:-8.028878226\n",
      "[Iter.  100]  loss:26.483500  pct:-7.738170070\n",
      "[Iter.  110]  loss:24.501238  pct:-7.484893209\n",
      "[Iter.  120]  loss:22.722210  pct:-7.260971663\n",
      "[Iter.  130]  loss:21.118216  pct:-7.059147743\n",
      "[Iter.  140]  loss:19.666468  pct:-6.874387138\n",
      "[Iter.  150]  loss:18.348034  pct:-6.703968318\n",
      "[Iter.  160]  loss:17.147072  pct:-6.545453714\n",
      "[Iter.  170]  loss:16.050270  pct:-6.396437643\n",
      "[Iter.  180]  loss:15.046274  pct:-6.255320878\n",
      "[Iter.  190]  loss:14.125377  pct:-6.120435348\n",
      "[Iter.  200]  loss:13.279232  pct:-5.990245033\n",
      "[Iter.  210]  loss:12.500663  pct:-5.863059099\n",
      "[Iter.  220]  loss:11.783318  pct:-5.738457624\n",
      "[Iter.  230]  loss:11.121572  pct:-5.615956808\n",
      "[Iter.  240]  loss:10.510511  pct:-5.494368671\n",
      "[Iter.  250]  loss:9.945805  pct:-5.372781409\n",
      "[Iter.  260]  loss:9.423504  pct:-5.251467744\n",
      "[Iter.  270]  loss:8.940104  pct:-5.129720299\n",
      "[Iter.  280]  loss:8.492387  pct:-5.007969061\n",
      "[Iter.  290]  loss:8.077540  pct:-4.884921391\n",
      "[Iter.  300]  loss:7.692976  pct:-4.760915546\n",
      "[Iter.  310]  loss:7.336332  pct:-4.635965314\n",
      "[Iter.  320]  loss:7.005480  pct:-4.509774622\n",
      "[Iter.  330]  loss:6.698453  pct:-4.382667159\n",
      "[Iter.  340]  loss:6.413414  pct:-4.255288100\n",
      "[Iter.  350]  loss:6.148707  pct:-4.127404617\n",
      "[Iter.  360]  loss:5.902892  pct:-3.997836953\n",
      "[Iter.  370]  loss:5.674552  pct:-3.868259987\n",
      "[Iter.  380]  loss:5.462434  pct:-3.738059539\n",
      "[Iter.  390]  loss:5.265351  pct:-3.607970107\n",
      "[Iter.  400]  loss:5.082211  pct:-3.478206690\n",
      "[Iter.  410]  loss:4.912019  pct:-3.348783140\n",
      "[Iter.  420]  loss:4.753835  pct:-3.220346733\n",
      "[Iter.  430]  loss:4.606799  pct:-3.092999007\n",
      "[Iter.  440]  loss:4.470127  pct:-2.966745808\n",
      "[Iter.  450]  loss:4.343072  pct:-2.842316676\n",
      "[Iter.  460]  loss:4.224973  pct:-2.719255272\n",
      "[Iter.  470]  loss:4.115184  pct:-2.598559209\n",
      "[Iter.  480]  loss:4.013107  pct:-2.480496611\n",
      "[Iter.  490]  loss:3.918211  pct:-2.364671220\n",
      "[Iter.  500]  loss:3.829982  pct:-2.251766229\n",
      "[Iter.  510]  loss:3.747945  pct:-2.141949129\n",
      "[Iter.  520]  loss:3.671678  pct:-2.034920816\n",
      "[Iter.  530]  loss:3.600769  pct:-1.931230200\n",
      "[Iter.  540]  loss:3.534845  pct:-1.830829149\n",
      "[Iter.  550]  loss:3.473552  pct:-1.733962444\n",
      "[Iter.  560]  loss:3.416561  pct:-1.640708363\n",
      "[Iter.  570]  loss:3.363589  pct:-1.550450077\n",
      "[Iter.  580]  loss:3.314316  pct:-1.464893894\n",
      "[Iter.  590]  loss:3.268496  pct:-1.382487186\n",
      "[Iter.  600]  loss:3.225883  pct:-1.303772168\n",
      "[Iter.  610]  loss:3.186268  pct:-1.228018647\n",
      "[Iter.  620]  loss:3.149428  pct:-1.156202885\n",
      "[Iter.  630]  loss:3.115167  pct:-1.087847833\n",
      "[Iter.  640]  loss:3.083321  pct:-1.022313020\n",
      "[Iter.  650]  loss:3.053711  pct:-0.960302447\n",
      "[Iter.  660]  loss:3.026180  pct:-0.901571291\n",
      "[Iter.  670]  loss:3.000542  pct:-0.847194364\n",
      "[Iter.  680]  loss:2.976698  pct:-0.794656448\n",
      "[Iter.  690]  loss:2.954520  pct:-0.745083490\n",
      "[Iter.  700]  loss:2.933889  pct:-0.698256389\n",
      "[Iter.  710]  loss:2.914706  pct:-0.653871758\n",
      "[Iter.  720]  loss:2.896863  pct:-0.612139178\n",
      "[Iter.  730]  loss:2.880267  pct:-0.572898209\n",
      "[Iter.  740]  loss:2.864817  pct:-0.536416837\n",
      "[Iter.  750]  loss:2.850438  pct:-0.501926029\n",
      "[Iter.  760]  loss:2.837060  pct:-0.469328063\n",
      "[Iter.  770]  loss:2.824612  pct:-0.438749822\n",
      "[Iter.  780]  loss:2.813034  pct:-0.409916772\n",
      "[Iter.  790]  loss:2.802248  pct:-0.383439931\n",
      "[Iter.  800]  loss:2.792211  pct:-0.358166323\n",
      "[Iter.  810]  loss:2.782868  pct:-0.334614680\n",
      "[Iter.  820]  loss:2.774174  pct:-0.312409159\n",
      "[Iter.  830]  loss:2.766089  pct:-0.291438576\n",
      "[Iter.  840]  loss:2.758556  pct:-0.272328033\n",
      "[Iter.  850]  loss:2.751533  pct:-0.254584571\n",
      "[Iter.  860]  loss:2.744986  pct:-0.237930481\n",
      "[Iter.  870]  loss:2.738883  pct:-0.222334035\n",
      "[Iter.  880]  loss:2.733198  pct:-0.207578378\n",
      "[Iter.  890]  loss:2.727906  pct:-0.193608385\n",
      "[Iter.  900]  loss:2.722982  pct:-0.180498158\n",
      "[Iter.  910]  loss:2.718402  pct:-0.168224964\n",
      "[Iter.  920]  loss:2.714128  pct:-0.157203207\n",
      "[Iter.  930]  loss:2.710152  pct:-0.146505422\n",
      "[Iter.  940]  loss:2.706445  pct:-0.136770697\n",
      "[Iter.  950]  loss:2.702970  pct:-0.128395386\n",
      "[Iter.  960]  loss:2.699718  pct:-0.120339640\n",
      "[Iter.  970]  loss:2.696682  pct:-0.112439369\n",
      "[Iter.  980]  loss:2.693853  pct:-0.104918315\n",
      "[Iter.  990]  loss:2.691212  pct:-0.098010087\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.649719  pct:100.000000000\n",
      "[Iter.    2]  loss:2.649665  pct:-0.002024524\n",
      "[Iter.    4]  loss:2.649642  pct:-0.000872812\n",
      "[Iter.    6]  loss:2.649629  pct:-0.000467903\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.649629\n",
      "Best loss: 2.649629 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 28%|██▊       | 2782/10000 [00:29<01:15, 95.50it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:321.474457  pct:100.000000000\n",
      "[Iter.   10]  loss:72.419594  pct:-77.472677952\n",
      "[Iter.   20]  loss:59.915092  pct:-17.266737750\n",
      "[Iter.   30]  loss:51.420532  pct:-14.177663577\n",
      "[Iter.   40]  loss:45.049694  pct:-12.389677604\n",
      "[Iter.   50]  loss:40.028465  pct:-11.145977559\n",
      "[Iter.   60]  loss:35.925976  pct:-10.248930214\n",
      "[Iter.   70]  loss:32.489265  pct:-9.566087716\n",
      "[Iter.   80]  loss:29.556904  pct:-9.025632199\n",
      "[Iter.   90]  loss:27.019779  pct:-8.583864696\n",
      "[Iter.  100]  loss:24.800320  pct:-8.214203073\n",
      "[Iter.  110]  loss:22.841492  pct:-7.898398079\n",
      "[Iter.  120]  loss:21.100214  pct:-7.623309885\n",
      "[Iter.  130]  loss:19.543100  pct:-7.379610686\n",
      "[Iter.  140]  loss:18.143642  pct:-7.160879829\n",
      "[Iter.  150]  loss:16.880669  pct:-6.960971539\n",
      "[Iter.  160]  loss:15.736809  pct:-6.776152578\n",
      "[Iter.  170]  loss:14.697627  pct:-6.603509797\n",
      "[Iter.  180]  loss:13.751086  pct:-6.440092868\n",
      "[Iter.  190]  loss:12.886949  pct:-6.284141011\n",
      "[Iter.  200]  loss:12.096524  pct:-6.133526037\n",
      "[Iter.  210]  loss:11.372315  pct:-5.986916717\n",
      "[Iter.  220]  loss:10.707808  pct:-5.843198051\n",
      "[Iter.  230]  loss:10.097262  pct:-5.701877395\n",
      "[Iter.  240]  loss:9.535668  pct:-5.561844271\n",
      "[Iter.  250]  loss:9.018600  pct:-5.422461111\n",
      "[Iter.  260]  loss:8.542102  pct:-5.283509406\n",
      "[Iter.  270]  loss:8.102705  pct:-5.143896261\n",
      "[Iter.  280]  loss:7.697244  pct:-5.004024104\n",
      "[Iter.  290]  loss:7.322886  pct:-4.863523081\n",
      "[Iter.  300]  loss:6.977057  pct:-4.722584303\n",
      "[Iter.  310]  loss:6.657472  pct:-4.580517606\n",
      "[Iter.  320]  loss:6.362027  pct:-4.437788154\n",
      "[Iter.  330]  loss:6.088794  pct:-4.294753959\n",
      "[Iter.  340]  loss:5.836034  pct:-4.151231650\n",
      "[Iter.  350]  loss:5.602159  pct:-4.007435218\n",
      "[Iter.  360]  loss:5.385702  pct:-3.863803773\n",
      "[Iter.  370]  loss:5.185354  pct:-3.719995934\n",
      "[Iter.  380]  loss:4.999892  pct:-3.576659694\n",
      "[Iter.  390]  loss:4.828163  pct:-3.434646574\n",
      "[Iter.  400]  loss:4.669147  pct:-3.293511994\n",
      "[Iter.  410]  loss:4.521897  pct:-3.153684710\n",
      "[Iter.  420]  loss:4.385535  pct:-3.015594576\n",
      "[Iter.  430]  loss:4.259234  pct:-2.879930083\n",
      "[Iter.  440]  loss:4.142262  pct:-2.746314427\n",
      "[Iter.  450]  loss:4.033916  pct:-2.615634893\n",
      "[Iter.  460]  loss:3.933568  pct:-2.487601562\n",
      "[Iter.  470]  loss:3.840641  pct:-2.362421350\n",
      "[Iter.  480]  loss:3.754576  pct:-2.240903505\n",
      "[Iter.  490]  loss:3.674846  pct:-2.123530401\n",
      "[Iter.  500]  loss:3.601008  pct:-2.009281261\n",
      "[Iter.  510]  loss:3.532597  pct:-1.899789873\n",
      "[Iter.  520]  loss:3.469227  pct:-1.793844062\n",
      "[Iter.  530]  loss:3.410536  pct:-1.691760714\n",
      "[Iter.  540]  loss:3.356174  pct:-1.593951580\n",
      "[Iter.  550]  loss:3.305819  pct:-1.500367910\n",
      "[Iter.  560]  loss:3.259187  pct:-1.410612299\n",
      "[Iter.  570]  loss:3.215992  pct:-1.325323591\n",
      "[Iter.  580]  loss:3.175995  pct:-1.243694050\n",
      "[Iter.  590]  loss:3.138955  pct:-1.166241076\n",
      "[Iter.  600]  loss:3.104640  pct:-1.093194059\n",
      "[Iter.  610]  loss:3.072861  pct:-1.023598495\n",
      "[Iter.  620]  loss:3.043437  pct:-0.957542506\n",
      "[Iter.  630]  loss:3.016185  pct:-0.895433573\n",
      "[Iter.  640]  loss:2.990956  pct:-0.836461068\n",
      "[Iter.  650]  loss:2.967591  pct:-0.781181061\n",
      "[Iter.  660]  loss:2.945954  pct:-0.729132689\n",
      "[Iter.  670]  loss:2.925923  pct:-0.679948771\n",
      "[Iter.  680]  loss:2.907375  pct:-0.633895658\n",
      "[Iter.  690]  loss:2.890214  pct:-0.590278370\n",
      "[Iter.  700]  loss:2.874319  pct:-0.549947267\n",
      "[Iter.  710]  loss:2.859589  pct:-0.512468389\n",
      "[Iter.  720]  loss:2.845940  pct:-0.477314214\n",
      "[Iter.  730]  loss:2.833293  pct:-0.444392802\n",
      "[Iter.  740]  loss:2.821580  pct:-0.413390082\n",
      "[Iter.  750]  loss:2.810729  pct:-0.384593773\n",
      "[Iter.  760]  loss:2.800685  pct:-0.357323109\n",
      "[Iter.  770]  loss:2.791391  pct:-0.331848548\n",
      "[Iter.  780]  loss:2.782786  pct:-0.308269352\n",
      "[Iter.  790]  loss:2.774822  pct:-0.286184256\n",
      "[Iter.  800]  loss:2.767435  pct:-0.266212471\n",
      "[Iter.  810]  loss:2.760593  pct:-0.247254676\n",
      "[Iter.  820]  loss:2.754257  pct:-0.229506342\n",
      "[Iter.  830]  loss:2.748379  pct:-0.213396749\n",
      "[Iter.  840]  loss:2.742929  pct:-0.198333747\n",
      "[Iter.  850]  loss:2.737880  pct:-0.184055596\n",
      "[Iter.  860]  loss:2.733199  pct:-0.170975733\n",
      "[Iter.  870]  loss:2.728860  pct:-0.158750969\n",
      "[Iter.  880]  loss:2.724827  pct:-0.147785171\n",
      "[Iter.  890]  loss:2.721093  pct:-0.137031573\n",
      "[Iter.  900]  loss:2.717634  pct:-0.127108412\n",
      "[Iter.  910]  loss:2.714429  pct:-0.117935691\n",
      "[Iter.  920]  loss:2.711448  pct:-0.109836136\n",
      "[Iter.  930]  loss:2.708678  pct:-0.102139899\n",
      "[Iter.  940]  loss:2.706108  pct:-0.094912244\n",
      "[Iter.  950]  loss:2.703725  pct:-0.088059827\n",
      "[Iter.  960]  loss:2.701515  pct:-0.081726644\n",
      "[Iter.  970]  loss:2.699467  pct:-0.075801067\n",
      "[Iter.  980]  loss:2.697567  pct:-0.070391523\n",
      "[Iter.  990]  loss:2.695800  pct:-0.065509347\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.670127  pct:100.000000000\n",
      "[Iter.    2]  loss:2.670129  pct:0.000071433\n",
      "[Iter.    4]  loss:2.670131  pct:0.000071433\n",
      "[Iter.    6]  loss:2.670133  pct:0.000062504\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.670133\n",
      "Best loss: 2.670133 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 19%|█▉        | 1915/10000 [00:18<01:17, 104.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:450.144501  pct:100.000000000\n",
      "[Iter.   10]  loss:92.386818  pct:-79.476188168\n",
      "[Iter.   20]  loss:78.719299  pct:-14.793797342\n",
      "[Iter.   30]  loss:69.271698  pct:-12.001632891\n",
      "[Iter.   40]  loss:61.893372  pct:-10.651285632\n",
      "[Iter.   50]  loss:55.890423  pct:-9.698855641\n",
      "[Iter.   60]  loss:50.871975  pct:-8.979083755\n",
      "[Iter.   70]  loss:46.591057  pct:-8.415081439\n",
      "[Iter.   80]  loss:42.882137  pct:-7.960582519\n",
      "[Iter.   90]  loss:39.629147  pct:-7.585887569\n",
      "[Iter.  100]  loss:36.747646  pct:-7.271164012\n",
      "[Iter.  110]  loss:34.174477  pct:-7.002270799\n",
      "[Iter.  120]  loss:31.860994  pct:-6.769620235\n",
      "[Iter.  130]  loss:29.769196  pct:-6.565390772\n",
      "[Iter.  140]  loss:27.868647  pct:-6.384280460\n",
      "[Iter.  150]  loss:26.134741  pct:-6.221707913\n",
      "[Iter.  160]  loss:24.547117  pct:-6.074763115\n",
      "[Iter.  170]  loss:23.088923  pct:-5.940390958\n",
      "[Iter.  180]  loss:21.746008  pct:-5.816272203\n",
      "[Iter.  190]  loss:20.506260  pct:-5.701037200\n",
      "[Iter.  200]  loss:19.359352  pct:-5.592964348\n",
      "[Iter.  210]  loss:18.296360  pct:-5.490845405\n",
      "[Iter.  220]  loss:17.309538  pct:-5.393543456\n",
      "[Iter.  230]  loss:16.392046  pct:-5.300499175\n",
      "[Iter.  240]  loss:15.537920  pct:-5.210612378\n",
      "[Iter.  250]  loss:14.741869  pct:-5.123279213\n",
      "[Iter.  260]  loss:13.999146  pct:-5.038184186\n",
      "[Iter.  270]  loss:13.305565  pct:-4.954456209\n",
      "[Iter.  280]  loss:12.657319  pct:-4.871990158\n",
      "[Iter.  290]  loss:12.050959  pct:-4.790591374\n",
      "[Iter.  300]  loss:11.483429  pct:-4.709415206\n",
      "[Iter.  310]  loss:10.951920  pct:-4.628490336\n",
      "[Iter.  320]  loss:10.453849  pct:-4.547793785\n",
      "[Iter.  330]  loss:9.986910  pct:-4.466670407\n",
      "[Iter.  340]  loss:9.548960  pct:-4.385241683\n",
      "[Iter.  350]  loss:9.138069  pct:-4.302987873\n",
      "[Iter.  360]  loss:8.752392  pct:-4.220556128\n",
      "[Iter.  370]  loss:8.390266  pct:-4.137444991\n",
      "[Iter.  380]  loss:8.050182  pct:-4.053316772\n",
      "[Iter.  390]  loss:7.730698  pct:-3.968664275\n",
      "[Iter.  400]  loss:7.430504  pct:-3.883133503\n",
      "[Iter.  410]  loss:7.148387  pct:-3.796739464\n",
      "[Iter.  420]  loss:6.883204  pct:-3.709702757\n",
      "[Iter.  430]  loss:6.633894  pct:-3.621991747\n",
      "[Iter.  440]  loss:6.399513  pct:-3.533093234\n",
      "[Iter.  450]  loss:6.179104  pct:-3.444151523\n",
      "[Iter.  460]  loss:5.971808  pct:-3.354788973\n",
      "[Iter.  470]  loss:5.776844  pct:-3.264738810\n",
      "[Iter.  480]  loss:5.593467  pct:-3.174350305\n",
      "[Iter.  490]  loss:5.420982  pct:-3.083676105\n",
      "[Iter.  500]  loss:5.258718  pct:-2.993255824\n",
      "[Iter.  510]  loss:5.106061  pct:-2.902950328\n",
      "[Iter.  520]  loss:4.962435  pct:-2.812838964\n",
      "[Iter.  530]  loss:4.827318  pct:-2.722797322\n",
      "[Iter.  540]  loss:4.700191  pct:-2.633484860\n",
      "[Iter.  550]  loss:4.580584  pct:-2.544735648\n",
      "[Iter.  560]  loss:4.468041  pct:-2.456949333\n",
      "[Iter.  570]  loss:4.362154  pct:-2.369894992\n",
      "[Iter.  580]  loss:4.262526  pct:-2.283906197\n",
      "[Iter.  590]  loss:4.168778  pct:-2.199344122\n",
      "[Iter.  600]  loss:4.080555  pct:-2.116278958\n",
      "[Iter.  610]  loss:3.997553  pct:-2.034099733\n",
      "[Iter.  620]  loss:3.919461  pct:-1.953503535\n",
      "[Iter.  630]  loss:3.845973  pct:-1.874951831\n",
      "[Iter.  640]  loss:3.776848  pct:-1.797326891\n",
      "[Iter.  650]  loss:3.711795  pct:-1.722402174\n",
      "[Iter.  660]  loss:3.650604  pct:-1.648576816\n",
      "[Iter.  670]  loss:3.593023  pct:-1.577292876\n",
      "[Iter.  680]  loss:3.538858  pct:-1.507488605\n",
      "[Iter.  690]  loss:3.487897  pct:-1.440047893\n",
      "[Iter.  700]  loss:3.439962  pct:-1.374317201\n",
      "[Iter.  710]  loss:3.394872  pct:-1.310768986\n",
      "[Iter.  720]  loss:3.352451  pct:-1.249585034\n",
      "[Iter.  730]  loss:3.312530  pct:-1.190801893\n",
      "[Iter.  740]  loss:3.274984  pct:-1.133430010\n",
      "[Iter.  750]  loss:3.239653  pct:-1.078814643\n",
      "[Iter.  760]  loss:3.206416  pct:-1.025942496\n",
      "[Iter.  770]  loss:3.175150  pct:-0.975121359\n",
      "[Iter.  780]  loss:3.145733  pct:-0.926477134\n",
      "[Iter.  790]  loss:3.118066  pct:-0.879510329\n",
      "[Iter.  800]  loss:3.092040  pct:-0.834692092\n",
      "[Iter.  810]  loss:3.067558  pct:-0.791775497\n",
      "[Iter.  820]  loss:3.044531  pct:-0.750660506\n",
      "[Iter.  830]  loss:3.022872  pct:-0.711395667\n",
      "[Iter.  840]  loss:3.002494  pct:-0.674115085\n",
      "[Iter.  850]  loss:2.983322  pct:-0.638550082\n",
      "[Iter.  860]  loss:2.965297  pct:-0.604205608\n",
      "[Iter.  870]  loss:2.948344  pct:-0.571681037\n",
      "[Iter.  880]  loss:2.932392  pct:-0.541061225\n",
      "[Iter.  890]  loss:2.917393  pct:-0.511506955\n",
      "[Iter.  900]  loss:2.903281  pct:-0.483711265\n",
      "[Iter.  910]  loss:2.890008  pct:-0.457180661\n",
      "[Iter.  920]  loss:2.877525  pct:-0.431924212\n",
      "[Iter.  930]  loss:2.865784  pct:-0.408013280\n",
      "[Iter.  940]  loss:2.854737  pct:-0.385508418\n",
      "[Iter.  950]  loss:2.844343  pct:-0.364083315\n",
      "[Iter.  960]  loss:2.834565  pct:-0.343754115\n",
      "[Iter.  970]  loss:2.825364  pct:-0.324601782\n",
      "[Iter.  980]  loss:2.816707  pct:-0.306410641\n",
      "[Iter.  990]  loss:2.808563  pct:-0.289136920\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.674269  pct:100.000000000\n",
      "[Iter.    2]  loss:2.674267  pct:-0.000089153\n",
      "[Iter.    4]  loss:2.674265  pct:-0.000053492\n",
      "[Iter.    6]  loss:2.674265  pct:-0.000008915\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.674265\n",
      "Best loss: 2.674265 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 11%|█▏        | 1140/10000 [00:10<01:24, 104.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:807.735535  pct:100.000000000\n",
      "[Iter.   10]  loss:136.999756  pct:-83.039033201\n",
      "[Iter.   20]  loss:124.559486  pct:-9.080504846\n",
      "[Iter.   30]  loss:114.017296  pct:-8.463579015\n",
      "[Iter.   40]  loss:104.909470  pct:-7.988109318\n",
      "[Iter.   50]  loss:97.382408  pct:-7.174816049\n",
      "[Iter.   60]  loss:90.836105  pct:-6.722264237\n",
      "[Iter.   70]  loss:85.047966  pct:-6.372069037\n",
      "[Iter.   80]  loss:79.886635  pct:-6.068729705\n",
      "[Iter.   90]  loss:75.246071  pct:-5.808936595\n",
      "[Iter.  100]  loss:71.042778  pct:-5.586062898\n",
      "[Iter.  110]  loss:67.209412  pct:-5.395856555\n",
      "[Iter.  120]  loss:63.694378  pct:-5.229972465\n",
      "[Iter.  130]  loss:60.456596  pct:-5.083308184\n",
      "[Iter.  140]  loss:57.462406  pct:-4.952627828\n",
      "[Iter.  150]  loss:54.684078  pct:-4.835035857\n",
      "[Iter.  160]  loss:52.098454  pct:-4.728295290\n",
      "[Iter.  170]  loss:49.685493  pct:-4.631538730\n",
      "[Iter.  180]  loss:47.428143  pct:-4.543279666\n",
      "[Iter.  190]  loss:45.311569  pct:-4.462694974\n",
      "[Iter.  200]  loss:43.323559  pct:-4.387423435\n",
      "[Iter.  210]  loss:41.452732  pct:-4.318266488\n",
      "[Iter.  220]  loss:39.689209  pct:-4.254298844\n",
      "[Iter.  230]  loss:38.024460  pct:-4.194462898\n",
      "[Iter.  240]  loss:36.450809  pct:-4.138521275\n",
      "[Iter.  250]  loss:34.961449  pct:-4.085947145\n",
      "[Iter.  260]  loss:33.550232  pct:-4.036493880\n",
      "[Iter.  270]  loss:32.211689  pct:-3.989668211\n",
      "[Iter.  280]  loss:30.940897  pct:-3.945126900\n",
      "[Iter.  290]  loss:29.733305  pct:-3.902899166\n",
      "[Iter.  300]  loss:28.584780  pct:-3.862756727\n",
      "[Iter.  310]  loss:27.491621  pct:-3.824268481\n",
      "[Iter.  320]  loss:26.450308  pct:-3.787747440\n",
      "[Iter.  330]  loss:25.457981  pct:-3.751664223\n",
      "[Iter.  340]  loss:24.511776  pct:-3.716732820\n",
      "[Iter.  350]  loss:23.608919  pct:-3.683359492\n",
      "[Iter.  360]  loss:22.747105  pct:-3.650376765\n",
      "[Iter.  370]  loss:21.923937  pct:-3.618780560\n",
      "[Iter.  380]  loss:21.137413  pct:-3.587511789\n",
      "[Iter.  390]  loss:20.385538  pct:-3.557081100\n",
      "[Iter.  400]  loss:19.666529  pct:-3.527056268\n",
      "[Iter.  410]  loss:18.978687  pct:-3.497523258\n",
      "[Iter.  420]  loss:18.320471  pct:-3.468187586\n",
      "[Iter.  430]  loss:17.690355  pct:-3.439406747\n",
      "[Iter.  440]  loss:17.087015  pct:-3.410559814\n",
      "[Iter.  450]  loss:16.509161  pct:-3.381832060\n",
      "[Iter.  460]  loss:15.955521  pct:-3.353534233\n",
      "[Iter.  470]  loss:15.425018  pct:-3.324882538\n",
      "[Iter.  480]  loss:14.916567  pct:-3.296277849\n",
      "[Iter.  490]  loss:14.429083  pct:-3.268070885\n",
      "[Iter.  500]  loss:13.961649  pct:-3.239526265\n",
      "[Iter.  510]  loss:13.513378  pct:-3.210729618\n",
      "[Iter.  520]  loss:13.083432  pct:-3.181631870\n",
      "[Iter.  530]  loss:12.670994  pct:-3.152371537\n",
      "[Iter.  540]  loss:12.275255  pct:-3.123185188\n",
      "[Iter.  550]  loss:11.895514  pct:-3.093554166\n",
      "[Iter.  560]  loss:11.531069  pct:-3.063715842\n",
      "[Iter.  570]  loss:11.181252  pct:-3.033693424\n",
      "[Iter.  580]  loss:10.845279  pct:-3.004786943\n",
      "[Iter.  590]  loss:10.522714  pct:-2.974244245\n",
      "[Iter.  600]  loss:10.213214  pct:-2.941254039\n",
      "[Iter.  610]  loss:9.916050  pct:-2.909602850\n",
      "[Iter.  620]  loss:9.630672  pct:-2.877935304\n",
      "[Iter.  630]  loss:9.356677  pct:-2.845028743\n",
      "[Iter.  640]  loss:9.093494  pct:-2.812778923\n",
      "[Iter.  650]  loss:8.840755  pct:-2.779348562\n",
      "[Iter.  660]  loss:8.597983  pct:-2.746045583\n",
      "[Iter.  670]  loss:8.364771  pct:-2.712408960\n",
      "[Iter.  680]  loss:8.140777  pct:-2.677828933\n",
      "[Iter.  690]  loss:7.925577  pct:-2.643481783\n",
      "[Iter.  700]  loss:7.718848  pct:-2.608371183\n",
      "[Iter.  710]  loss:7.520236  pct:-2.573074770\n",
      "[Iter.  720]  loss:7.329228  pct:-2.539921333\n",
      "[Iter.  730]  loss:7.145683  pct:-2.504289709\n",
      "[Iter.  740]  loss:6.969547  pct:-2.464935634\n",
      "[Iter.  750]  loss:6.800333  pct:-2.427908925\n",
      "[Iter.  760]  loss:6.637742  pct:-2.390919894\n",
      "[Iter.  770]  loss:6.481516  pct:-2.353603939\n",
      "[Iter.  780]  loss:6.331428  pct:-2.315628567\n",
      "[Iter.  790]  loss:6.187208  pct:-2.277841179\n",
      "[Iter.  800]  loss:6.048626  pct:-2.239818440\n",
      "[Iter.  810]  loss:5.915507  pct:-2.200807764\n",
      "[Iter.  820]  loss:5.787550  pct:-2.163083185\n",
      "[Iter.  830]  loss:5.664586  pct:-2.124636206\n",
      "[Iter.  840]  loss:5.546453  pct:-2.085458669\n",
      "[Iter.  850]  loss:5.432949  pct:-2.046423777\n",
      "[Iter.  860]  loss:5.323901  pct:-2.007158329\n",
      "[Iter.  870]  loss:5.219120  pct:-1.968136225\n",
      "[Iter.  880]  loss:5.118411  pct:-1.929615916\n",
      "[Iter.  890]  loss:5.021668  pct:-1.890091237\n",
      "[Iter.  900]  loss:4.928688  pct:-1.851583689\n",
      "[Iter.  910]  loss:4.839353  pct:-1.812550774\n",
      "[Iter.  920]  loss:4.753517  pct:-1.773697100\n",
      "[Iter.  930]  loss:4.671029  pct:-1.735305825\n",
      "[Iter.  940]  loss:4.591782  pct:-1.696574205\n",
      "[Iter.  950]  loss:4.515625  pct:-1.658541773\n",
      "[Iter.  960]  loss:4.442446  pct:-1.620567876\n",
      "[Iter.  970]  loss:4.372164  pct:-1.582055893\n",
      "[Iter.  980]  loss:4.304615  pct:-1.544972878\n",
      "[Iter.  990]  loss:4.239711  pct:-1.507792968\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.651562  pct:100.000000000\n",
      "[Iter.    2]  loss:2.651557  pct:-0.000206807\n",
      "[Iter.    4]  loss:2.651551  pct:-0.000215799\n",
      "[Iter.    6]  loss:2.651551  pct:-0.000017983\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.651551\n",
      "Best loss: 2.651551 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 15%|█▌        | 1526/10000 [00:15<01:23, 101.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:181.115494  pct:100.000000000\n",
      "[Iter.   10]  loss:34.552444  pct:-80.922424836\n",
      "[Iter.   20]  loss:27.498989  pct:-20.413766561\n",
      "[Iter.   30]  loss:22.750187  pct:-17.269006387\n",
      "[Iter.   40]  loss:19.257761  pct:-15.351196589\n",
      "[Iter.   50]  loss:16.557405  pct:-14.022167632\n",
      "[Iter.   60]  loss:14.401687  pct:-13.019665473\n",
      "[Iter.   70]  loss:12.644619  pct:-12.200429858\n",
      "[Iter.   80]  loss:11.189056  pct:-11.511320293\n",
      "[Iter.   90]  loss:9.970169  pct:-10.893566767\n",
      "[Iter.  100]  loss:8.942110  pct:-10.311349775\n",
      "[Iter.  110]  loss:8.069200  pct:-9.761795522\n",
      "[Iter.  120]  loss:7.324536  pct:-9.228464767\n",
      "[Iter.  130]  loss:6.687241  pct:-8.700827165\n",
      "[Iter.  140]  loss:6.139282  pct:-8.194094463\n",
      "[Iter.  150]  loss:5.666881  pct:-7.694736966\n",
      "[Iter.  160]  loss:5.258481  pct:-7.206779359\n",
      "[Iter.  170]  loss:4.904789  pct:-6.726125910\n",
      "[Iter.  180]  loss:4.598320  pct:-6.248371587\n",
      "[Iter.  190]  loss:4.332350  pct:-5.784053819\n",
      "[Iter.  200]  loss:4.101339  pct:-5.332241789\n",
      "[Iter.  210]  loss:3.900539  pct:-4.895954962\n",
      "[Iter.  220]  loss:3.725938  pct:-4.476337996\n",
      "[Iter.  230]  loss:3.574027  pct:-4.077115300\n",
      "[Iter.  240]  loss:3.441823  pct:-3.699035579\n",
      "[Iter.  250]  loss:3.326730  pct:-3.343934109\n",
      "[Iter.  260]  loss:3.226517  pct:-3.012373114\n",
      "[Iter.  270]  loss:3.139231  pct:-2.705255423\n",
      "[Iter.  280]  loss:3.063210  pct:-2.421650063\n",
      "[Iter.  290]  loss:2.996970  pct:-2.162439794\n",
      "[Iter.  300]  loss:2.939205  pct:-1.927439040\n",
      "[Iter.  310]  loss:2.888819  pct:-1.714271345\n",
      "[Iter.  320]  loss:2.844864  pct:-1.521575467\n",
      "[Iter.  330]  loss:2.806523  pct:-1.347720370\n",
      "[Iter.  340]  loss:2.773034  pct:-1.193264018\n",
      "[Iter.  350]  loss:2.743785  pct:-1.054746536\n",
      "[Iter.  360]  loss:2.718226  pct:-0.931521500\n",
      "[Iter.  370]  loss:2.695894  pct:-0.821580998\n",
      "[Iter.  380]  loss:2.676368  pct:-0.724278035\n",
      "[Iter.  390]  loss:2.659281  pct:-0.638421394\n",
      "[Iter.  400]  loss:2.644322  pct:-0.562532976\n",
      "[Iter.  410]  loss:2.631194  pct:-0.496452513\n",
      "[Iter.  420]  loss:2.619671  pct:-0.437965504\n",
      "[Iter.  430]  loss:2.609546  pct:-0.386468682\n",
      "[Iter.  440]  loss:2.600654  pct:-0.340778555\n",
      "[Iter.  450]  loss:2.592832  pct:-0.300753617\n",
      "[Iter.  460]  loss:2.585940  pct:-0.265817610\n",
      "[Iter.  470]  loss:2.579868  pct:-0.234791519\n",
      "[Iter.  480]  loss:2.574518  pct:-0.207397785\n",
      "[Iter.  490]  loss:2.569801  pct:-0.183213855\n",
      "[Iter.  500]  loss:2.565641  pct:-0.161877422\n",
      "[Iter.  510]  loss:2.561958  pct:-0.143535806\n",
      "[Iter.  520]  loss:2.558695  pct:-0.127363379\n",
      "[Iter.  530]  loss:2.555813  pct:-0.112654313\n",
      "[Iter.  540]  loss:2.553264  pct:-0.099730817\n",
      "[Iter.  550]  loss:2.550999  pct:-0.088709064\n",
      "[Iter.  560]  loss:2.548958  pct:-0.080011851\n",
      "[Iter.  570]  loss:2.547130  pct:-0.071723182\n",
      "[Iter.  580]  loss:2.545487  pct:-0.064483000\n",
      "[Iter.  590]  loss:2.544018  pct:-0.057734022\n",
      "[Iter.  600]  loss:2.542694  pct:-0.052031872\n",
      "[Iter.  610]  loss:2.541506  pct:-0.046733042\n",
      "[Iter.  620]  loss:2.540441  pct:-0.041895536\n",
      "[Iter.  630]  loss:2.539486  pct:-0.037567873\n",
      "[Iter.  640]  loss:2.538631  pct:-0.033685782\n",
      "[Iter.  650]  loss:2.537863  pct:-0.030259800\n",
      "[Iter.  660]  loss:2.537168  pct:-0.027384860\n",
      "[Iter.  670]  loss:2.536529  pct:-0.025165263\n",
      "[Iter.  680]  loss:2.535951  pct:-0.022802949\n",
      "[Iter.  690]  loss:2.535428  pct:-0.020636392\n",
      "[Iter.  700]  loss:2.534954  pct:-0.018684727\n",
      "[Iter.  710]  loss:2.534525  pct:-0.016920033\n",
      "[Iter.  720]  loss:2.534132  pct:-0.015483651\n",
      "[Iter.  730]  loss:2.533775  pct:-0.014103029\n",
      "[Iter.  740]  loss:2.533451  pct:-0.012778262\n",
      "[Iter.  750]  loss:2.533159  pct:-0.011547078\n",
      "[Iter.  760]  loss:2.532895  pct:-0.010409570\n",
      "[Iter.  770]  loss:2.532657  pct:-0.009403475\n",
      "[Iter.  780]  loss:2.532446  pct:-0.008331189\n",
      "[Iter.  790]  loss:2.532263  pct:-0.007239795\n",
      "[Iter.  800]  loss:2.532097  pct:-0.006524761\n",
      "[Iter.  810]  loss:2.531934  pct:-0.006459276\n",
      "[Iter.  820]  loss:2.531780  pct:-0.006073618\n",
      "[Iter.  830]  loss:2.531639  pct:-0.005574884\n",
      "[Iter.  840]  loss:2.531512  pct:-0.004991306\n",
      "[Iter.  850]  loss:2.531402  pct:-0.004379383\n",
      "[Iter.  860]  loss:2.531307  pct:-0.003729703\n",
      "[Iter.  870]  loss:2.531217  pct:-0.003550885\n",
      "[Iter.  880]  loss:2.531129  pct:-0.003494496\n",
      "[Iter.  890]  loss:2.531052  pct:-0.003042484\n",
      "[Iter.  900]  loss:2.530986  pct:-0.002618689\n",
      "[Iter.  910]  loss:2.530937  pct:-0.001931098\n",
      "[Iter.  920]  loss:2.530891  pct:-0.001818093\n",
      "[Iter.  930]  loss:2.530828  pct:-0.002458709\n",
      "[Iter.  940]  loss:2.530767  pct:-0.002430508\n",
      "[Iter.  950]  loss:2.530710  pct:-0.002232730\n",
      "[Iter.  960]  loss:2.530658  pct:-0.002063202\n",
      "[Iter.  970]  loss:2.530610  pct:-0.001912505\n",
      "[Iter.  980]  loss:2.530566  pct:-0.001724114\n",
      "[Iter.  990]  loss:2.530528  pct:-0.001526291\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.529254  pct:100.000000000\n",
      "[Iter.    2]  loss:2.529255  pct:0.000028279\n",
      "[Iter.    4]  loss:2.529256  pct:0.000028279\n",
      "[Iter.    6]  loss:2.529256  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.529256\n",
      "Best loss: 2.529256 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 345/10000 [00:05<02:25, 66.57it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:2519.077637  pct:100.000000000\n",
      "[Iter.   10]  loss:317.264191  pct:-87.405542587\n",
      "[Iter.   20]  loss:305.847137  pct:-3.598594975\n",
      "[Iter.   30]  loss:295.712616  pct:-3.313590432\n",
      "[Iter.   40]  loss:286.203918  pct:-3.215519730\n",
      "[Iter.   50]  loss:277.257507  pct:-3.125887018\n",
      "[Iter.   60]  loss:268.812805  pct:-3.045797472\n",
      "[Iter.   70]  loss:260.823242  pct:-2.972166070\n",
      "[Iter.   80]  loss:253.250061  pct:-2.903568366\n",
      "[Iter.   90]  loss:246.059540  pct:-2.839296943\n",
      "[Iter.  100]  loss:239.220886  pct:-2.779267802\n",
      "[Iter.  110]  loss:232.706100  pct:-2.723334851\n",
      "[Iter.  120]  loss:226.489609  pct:-2.671391806\n",
      "[Iter.  130]  loss:220.547501  pct:-2.623567671\n",
      "[Iter.  140]  loss:214.856262  pct:-2.580504602\n",
      "[Iter.  150]  loss:209.391678  pct:-2.543367503\n",
      "[Iter.  160]  loss:204.129044  pct:-2.513296771\n",
      "[Iter.  170]  loss:199.048264  pct:-2.489003985\n",
      "[Iter.  180]  loss:194.150146  pct:-2.460768548\n",
      "[Iter.  190]  loss:189.450638  pct:-2.420553758\n",
      "[Iter.  200]  loss:184.948181  pct:-2.376585646\n",
      "[Iter.  210]  loss:180.626434  pct:-2.336733889\n",
      "[Iter.  220]  loss:176.469009  pct:-2.301670263\n",
      "[Iter.  230]  loss:172.464691  pct:-2.269133969\n",
      "[Iter.  240]  loss:168.601578  pct:-2.239944523\n",
      "[Iter.  250]  loss:164.871429  pct:-2.212404157\n",
      "[Iter.  260]  loss:161.266373  pct:-2.186586709\n",
      "[Iter.  270]  loss:157.778992  pct:-2.162497316\n",
      "[Iter.  280]  loss:154.402542  pct:-2.139986793\n",
      "[Iter.  290]  loss:151.131577  pct:-2.118466141\n",
      "[Iter.  300]  loss:147.960709  pct:-2.098084327\n",
      "[Iter.  310]  loss:144.884750  pct:-2.078902082\n",
      "[Iter.  320]  loss:141.898834  pct:-2.060890556\n",
      "[Iter.  330]  loss:139.000168  pct:-2.042769694\n",
      "[Iter.  340]  loss:136.184708  pct:-2.025508493\n",
      "[Iter.  350]  loss:133.448212  pct:-2.009400335\n",
      "[Iter.  360]  loss:130.787460  pct:-1.993845635\n",
      "[Iter.  370]  loss:128.199539  pct:-1.978722682\n",
      "[Iter.  380]  loss:125.681282  pct:-1.964326204\n",
      "[Iter.  390]  loss:123.229767  pct:-1.950580992\n",
      "[Iter.  400]  loss:120.842476  pct:-1.937268093\n",
      "[Iter.  410]  loss:118.517044  pct:-1.924349701\n",
      "[Iter.  420]  loss:116.250984  pct:-1.912011807\n",
      "[Iter.  430]  loss:114.042236  pct:-1.899982077\n",
      "[Iter.  440]  loss:111.888657  pct:-1.888405367\n",
      "[Iter.  450]  loss:109.788239  pct:-1.877239529\n",
      "[Iter.  460]  loss:107.738976  pct:-1.866559686\n",
      "[Iter.  470]  loss:105.739067  pct:-1.856253447\n",
      "[Iter.  480]  loss:103.786880  pct:-1.846230195\n",
      "[Iter.  490]  loss:101.880760  pct:-1.836571531\n",
      "[Iter.  500]  loss:100.019234  pct:-1.827161955\n",
      "[Iter.  510]  loss:98.200867  pct:-1.818017332\n",
      "[Iter.  520]  loss:96.424255  pct:-1.809160538\n",
      "[Iter.  530]  loss:94.687965  pct:-1.800677611\n",
      "[Iter.  540]  loss:92.990730  pct:-1.792450709\n",
      "[Iter.  550]  loss:91.331375  pct:-1.784430726\n",
      "[Iter.  560]  loss:89.708702  pct:-1.776687401\n",
      "[Iter.  570]  loss:88.121666  pct:-1.769099425\n",
      "[Iter.  580]  loss:86.569176  pct:-1.761757699\n",
      "[Iter.  590]  loss:85.050224  pct:-1.754610002\n",
      "[Iter.  600]  loss:83.563797  pct:-1.747705334\n",
      "[Iter.  610]  loss:82.108986  pct:-1.740958583\n",
      "[Iter.  620]  loss:80.684830  pct:-1.734470562\n",
      "[Iter.  630]  loss:79.290535  pct:-1.728075456\n",
      "[Iter.  640]  loss:77.925240  pct:-1.721889518\n",
      "[Iter.  650]  loss:76.588211  pct:-1.715783629\n",
      "[Iter.  660]  loss:75.278641  pct:-1.709884974\n",
      "[Iter.  670]  loss:73.995773  pct:-1.704158602\n",
      "[Iter.  680]  loss:72.738953  pct:-1.698503337\n",
      "[Iter.  690]  loss:71.507378  pct:-1.693143725\n",
      "[Iter.  700]  loss:70.300453  pct:-1.687832051\n",
      "[Iter.  710]  loss:69.117508  pct:-1.682699325\n",
      "[Iter.  720]  loss:67.958069  pct:-1.677489715\n",
      "[Iter.  730]  loss:66.821396  pct:-1.672609291\n",
      "[Iter.  740]  loss:65.707031  pct:-1.667676362\n",
      "[Iter.  750]  loss:64.614204  pct:-1.663180975\n",
      "[Iter.  760]  loss:63.542568  pct:-1.658514888\n",
      "[Iter.  770]  loss:62.491707  pct:-1.653791133\n",
      "[Iter.  780]  loss:61.461014  pct:-1.649327737\n",
      "[Iter.  790]  loss:60.449928  pct:-1.645084335\n",
      "[Iter.  800]  loss:59.458107  pct:-1.640731953\n",
      "[Iter.  810]  loss:58.485073  pct:-1.636503337\n",
      "[Iter.  820]  loss:57.530312  pct:-1.632487496\n",
      "[Iter.  830]  loss:56.593460  pct:-1.628448509\n",
      "[Iter.  840]  loss:55.674038  pct:-1.624608477\n",
      "[Iter.  850]  loss:54.771694  pct:-1.620762178\n",
      "[Iter.  860]  loss:53.886021  pct:-1.617027803\n",
      "[Iter.  870]  loss:53.016747  pct:-1.613171893\n",
      "[Iter.  880]  loss:52.163456  pct:-1.609473636\n",
      "[Iter.  890]  loss:51.325836  pct:-1.605759753\n",
      "[Iter.  900]  loss:50.503395  pct:-1.602392016\n",
      "[Iter.  910]  loss:49.695992  pct:-1.598711459\n",
      "[Iter.  920]  loss:48.903202  pct:-1.595278482\n",
      "[Iter.  930]  loss:48.124775  pct:-1.591771277\n",
      "[Iter.  940]  loss:47.360397  pct:-1.588324507\n",
      "[Iter.  950]  loss:46.609749  pct:-1.584970863\n",
      "[Iter.  960]  loss:45.872478  pct:-1.581794310\n",
      "[Iter.  970]  loss:45.148399  pct:-1.578460890\n",
      "[Iter.  980]  loss:44.437183  pct:-1.575285022\n",
      "[Iter.  990]  loss:43.738674  pct:-1.571902545\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.648964  pct:100.000000000\n",
      "[Iter.    2]  loss:2.648606  pct:-0.013536670\n",
      "[Iter.    4]  loss:2.648515  pct:-0.003429634\n",
      "[Iter.    6]  loss:2.648475  pct:-0.001494327\n",
      "[Iter.    8]  loss:2.648464  pct:-0.000423099\n",
      "[Iter.   10]  loss:2.648471  pct:0.000279067\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.648471\n",
      "Best loss: 2.648471 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 25%|██▍       | 2460/10000 [00:32<01:40, 74.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:439.875092  pct:100.000000000\n",
      "[Iter.   10]  loss:84.085144  pct:-80.884313375\n",
      "[Iter.   20]  loss:72.282257  pct:-14.036827905\n",
      "[Iter.   30]  loss:63.590420  pct:-12.024855977\n",
      "[Iter.   40]  loss:56.740345  pct:-10.772180453\n",
      "[Iter.   50]  loss:51.171661  pct:-9.814328101\n",
      "[Iter.   60]  loss:46.522160  pct:-9.086087251\n",
      "[Iter.   70]  loss:42.555225  pct:-8.526977768\n",
      "[Iter.   80]  loss:39.115826  pct:-8.082203041\n",
      "[Iter.   90]  loss:36.097191  pct:-7.717170086\n",
      "[Iter.  100]  loss:33.422703  pct:-7.409130750\n",
      "[Iter.  110]  loss:31.035418  pct:-7.142705506\n",
      "[Iter.  120]  loss:28.891249  pct:-6.908780428\n",
      "[Iter.  130]  loss:26.955011  pct:-6.701812563\n",
      "[Iter.  140]  loss:25.198158  pct:-6.517723475\n",
      "[Iter.  150]  loss:23.597567  pct:-6.352018440\n",
      "[Iter.  160]  loss:22.134226  pct:-6.201235847\n",
      "[Iter.  170]  loss:20.792303  pct:-6.062659563\n",
      "[Iter.  180]  loss:19.558172  pct:-5.935517842\n",
      "[Iter.  190]  loss:18.420769  pct:-5.815489684\n",
      "[Iter.  200]  loss:17.370657  pct:-5.700694610\n",
      "[Iter.  210]  loss:16.399191  pct:-5.592569506\n",
      "[Iter.  220]  loss:15.499148  pct:-5.488334999\n",
      "[Iter.  230]  loss:14.664093  pct:-5.387749903\n",
      "[Iter.  240]  loss:13.888470  pct:-5.289268969\n",
      "[Iter.  250]  loss:13.167250  pct:-5.192940851\n",
      "[Iter.  260]  loss:12.496060  pct:-5.097414604\n",
      "[Iter.  270]  loss:11.870864  pct:-5.003148499\n",
      "[Iter.  280]  loss:11.287998  pct:-4.910053044\n",
      "[Iter.  290]  loss:10.744275  pct:-4.816824886\n",
      "[Iter.  300]  loss:10.236787  pct:-4.723336347\n",
      "[Iter.  310]  loss:9.762825  pct:-4.629986317\n",
      "[Iter.  320]  loss:9.319957  pct:-4.536271337\n",
      "[Iter.  330]  loss:8.905928  pct:-4.442393148\n",
      "[Iter.  340]  loss:8.518747  pct:-4.347445244\n",
      "[Iter.  350]  loss:8.156519  pct:-4.252132145\n",
      "[Iter.  360]  loss:7.817524  pct:-4.156123250\n",
      "[Iter.  370]  loss:7.500183  pct:-4.059352457\n",
      "[Iter.  380]  loss:7.203074  pct:-3.961358314\n",
      "[Iter.  390]  loss:6.924819  pct:-3.863003304\n",
      "[Iter.  400]  loss:6.664173  pct:-3.763944495\n",
      "[Iter.  410]  loss:6.419992  pct:-3.664080932\n",
      "[Iter.  420]  loss:6.191224  pct:-3.563374374\n",
      "[Iter.  430]  loss:5.976832  pct:-3.462832575\n",
      "[Iter.  440]  loss:5.775905  pct:-3.361760605\n",
      "[Iter.  450]  loss:5.587583  pct:-3.260477154\n",
      "[Iter.  460]  loss:5.411049  pct:-3.159401369\n",
      "[Iter.  470]  loss:5.245551  pct:-3.058515701\n",
      "[Iter.  480]  loss:5.090405  pct:-2.957670581\n",
      "[Iter.  490]  loss:4.944952  pct:-2.857404341\n",
      "[Iter.  500]  loss:4.808573  pct:-2.757939368\n",
      "[Iter.  510]  loss:4.680691  pct:-2.659448720\n",
      "[Iter.  520]  loss:4.560789  pct:-2.561633052\n",
      "[Iter.  530]  loss:4.448361  pct:-2.465093404\n",
      "[Iter.  540]  loss:4.342935  pct:-2.370014010\n",
      "[Iter.  550]  loss:4.244086  pct:-2.276072559\n",
      "[Iter.  560]  loss:4.151388  pct:-2.184170901\n",
      "[Iter.  570]  loss:4.064469  pct:-2.093729310\n",
      "[Iter.  580]  loss:3.982969  pct:-2.005183128\n",
      "[Iter.  590]  loss:3.906542  pct:-1.918856367\n",
      "[Iter.  600]  loss:3.834871  pct:-1.834628565\n",
      "[Iter.  610]  loss:3.767665  pct:-1.752513085\n",
      "[Iter.  620]  loss:3.704641  pct:-1.672761499\n",
      "[Iter.  630]  loss:3.645559  pct:-1.594811405\n",
      "[Iter.  640]  loss:3.590150  pct:-1.519896464\n",
      "[Iter.  650]  loss:3.538182  pct:-1.447525531\n",
      "[Iter.  660]  loss:3.489460  pct:-1.377015647\n",
      "[Iter.  670]  loss:3.443781  pct:-1.309052145\n",
      "[Iter.  680]  loss:3.400943  pct:-1.243933060\n",
      "[Iter.  690]  loss:3.360786  pct:-1.180763077\n",
      "[Iter.  700]  loss:3.323124  pct:-1.120631593\n",
      "[Iter.  710]  loss:3.287808  pct:-1.062720330\n",
      "[Iter.  720]  loss:3.254692  pct:-1.007239413\n",
      "[Iter.  730]  loss:3.223636  pct:-0.954189345\n",
      "[Iter.  740]  loss:3.194513  pct:-0.903437646\n",
      "[Iter.  750]  loss:3.167202  pct:-0.854922523\n",
      "[Iter.  760]  loss:3.141594  pct:-0.808538920\n",
      "[Iter.  770]  loss:3.117583  pct:-0.764312977\n",
      "[Iter.  780]  loss:3.095069  pct:-0.722150178\n",
      "[Iter.  790]  loss:3.073954  pct:-0.682223999\n",
      "[Iter.  800]  loss:3.054150  pct:-0.644244020\n",
      "[Iter.  810]  loss:3.035579  pct:-0.608054640\n",
      "[Iter.  820]  loss:3.018164  pct:-0.573697699\n",
      "[Iter.  830]  loss:3.001832  pct:-0.541112846\n",
      "[Iter.  840]  loss:2.986517  pct:-0.510206148\n",
      "[Iter.  850]  loss:2.972150  pct:-0.481049601\n",
      "[Iter.  860]  loss:2.958680  pct:-0.453213078\n",
      "[Iter.  870]  loss:2.946045  pct:-0.427040253\n",
      "[Iter.  880]  loss:2.934193  pct:-0.402311022\n",
      "[Iter.  890]  loss:2.923075  pct:-0.378901359\n",
      "[Iter.  900]  loss:2.912642  pct:-0.356917215\n",
      "[Iter.  910]  loss:2.902856  pct:-0.335996206\n",
      "[Iter.  920]  loss:2.893676  pct:-0.316226245\n",
      "[Iter.  930]  loss:2.885060  pct:-0.297776428\n",
      "[Iter.  940]  loss:2.876972  pct:-0.280311652\n",
      "[Iter.  950]  loss:2.869382  pct:-0.263845792\n",
      "[Iter.  960]  loss:2.862261  pct:-0.248166627\n",
      "[Iter.  970]  loss:2.855577  pct:-0.233532292\n",
      "[Iter.  980]  loss:2.849306  pct:-0.219601381\n",
      "[Iter.  990]  loss:2.843416  pct:-0.206688158\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.748676  pct:100.000000000\n",
      "[Iter.    2]  loss:2.748687  pct:0.000390327\n",
      "[Iter.    4]  loss:2.748690  pct:0.000121435\n",
      "[Iter.    6]  loss:2.748692  pct:0.000069391\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.748692\n",
      "Best loss: 2.748692 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 256/10000 [00:02<01:49, 89.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:4636.393066  pct:100.000000000\n",
      "[Iter.   10]  loss:730.634583  pct:-84.241316234\n",
      "[Iter.   20]  loss:709.199158  pct:-2.933809228\n",
      "[Iter.   30]  loss:689.605591  pct:-2.762773571\n",
      "[Iter.   40]  loss:671.259338  pct:-2.660397869\n",
      "[Iter.   50]  loss:653.957764  pct:-2.577479927\n",
      "[Iter.   60]  loss:637.599731  pct:-2.501389713\n",
      "[Iter.   70]  loss:622.108459  pct:-2.429623353\n",
      "[Iter.   80]  loss:607.414001  pct:-2.362041182\n",
      "[Iter.   90]  loss:593.451172  pct:-2.298733575\n",
      "[Iter.  100]  loss:580.159119  pct:-2.239788858\n",
      "[Iter.  110]  loss:567.484558  pct:-2.184669712\n",
      "[Iter.  120]  loss:555.380066  pct:-2.133008205\n",
      "[Iter.  130]  loss:543.803345  pct:-2.084468259\n",
      "[Iter.  140]  loss:532.716553  pct:-2.038750239\n",
      "[Iter.  150]  loss:522.086426  pct:-1.995456477\n",
      "[Iter.  160]  loss:511.882263  pct:-1.954496822\n",
      "[Iter.  170]  loss:502.075897  pct:-1.915746388\n",
      "[Iter.  180]  loss:492.642242  pct:-1.878930026\n",
      "[Iter.  190]  loss:483.558563  pct:-1.843869327\n",
      "[Iter.  200]  loss:474.802429  pct:-1.810770132\n",
      "[Iter.  210]  loss:466.354614  pct:-1.779227405\n",
      "[Iter.  220]  loss:458.197083  pct:-1.749212185\n",
      "[Iter.  230]  loss:450.312775  pct:-1.720724152\n",
      "[Iter.  240]  loss:442.686432  pct:-1.693565718\n",
      "[Iter.  250]  loss:435.303162  pct:-1.667832970\n",
      "[Iter.  260]  loss:428.150269  pct:-1.643198051\n",
      "[Iter.  270]  loss:421.214966  pct:-1.619829122\n",
      "[Iter.  280]  loss:414.485931  pct:-1.597529758\n",
      "[Iter.  290]  loss:407.952942  pct:-1.576166766\n",
      "[Iter.  300]  loss:401.605682  pct:-1.555880316\n",
      "[Iter.  310]  loss:395.435425  pct:-1.536396978\n",
      "[Iter.  320]  loss:389.433624  pct:-1.517770073\n",
      "[Iter.  330]  loss:383.592987  pct:-1.499777329\n",
      "[Iter.  340]  loss:377.905365  pct:-1.482723163\n",
      "[Iter.  350]  loss:372.364532  pct:-1.466195781\n",
      "[Iter.  360]  loss:366.963654  pct:-1.450427856\n",
      "[Iter.  370]  loss:361.696869  pct:-1.435233331\n",
      "[Iter.  380]  loss:356.558990  pct:-1.420492921\n",
      "[Iter.  390]  loss:351.543976  pct:-1.406503491\n",
      "[Iter.  400]  loss:346.647217  pct:-1.392929298\n",
      "[Iter.  410]  loss:341.864105  pct:-1.379821138\n",
      "[Iter.  420]  loss:337.189972  pct:-1.367248924\n",
      "[Iter.  430]  loss:332.620605  pct:-1.355131183\n",
      "[Iter.  440]  loss:328.152344  pct:-1.343350846\n",
      "[Iter.  450]  loss:323.780975  pct:-1.332115553\n",
      "[Iter.  460]  loss:319.503174  pct:-1.321202245\n",
      "[Iter.  470]  loss:315.315704  pct:-1.310619057\n",
      "[Iter.  480]  loss:311.215332  pct:-1.300402187\n",
      "[Iter.  490]  loss:307.199371  pct:-1.290412226\n",
      "[Iter.  500]  loss:303.264465  pct:-1.280896503\n",
      "[Iter.  510]  loss:299.408264  pct:-1.271563804\n",
      "[Iter.  520]  loss:295.628052  pct:-1.262561143\n",
      "[Iter.  530]  loss:291.921539  pct:-1.253775624\n",
      "[Iter.  540]  loss:288.286346  pct:-1.245263669\n",
      "[Iter.  550]  loss:284.720215  pct:-1.237010228\n",
      "[Iter.  560]  loss:281.221069  pct:-1.228976843\n",
      "[Iter.  570]  loss:277.786621  pct:-1.221262777\n",
      "[Iter.  580]  loss:274.415161  pct:-1.213686947\n",
      "[Iter.  590]  loss:271.104645  pct:-1.206389743\n",
      "[Iter.  600]  loss:267.853607  pct:-1.199181814\n",
      "[Iter.  610]  loss:264.660217  pct:-1.192214630\n",
      "[Iter.  620]  loss:261.522858  pct:-1.185429246\n",
      "[Iter.  630]  loss:258.439789  pct:-1.178890777\n",
      "[Iter.  640]  loss:255.409500  pct:-1.172531796\n",
      "[Iter.  650]  loss:252.430695  pct:-1.166286117\n",
      "[Iter.  660]  loss:249.502075  pct:-1.160167701\n",
      "[Iter.  670]  loss:246.622116  pct:-1.154282626\n",
      "[Iter.  680]  loss:243.789780  pct:-1.148451919\n",
      "[Iter.  690]  loss:241.003754  pct:-1.142798523\n",
      "[Iter.  700]  loss:238.262924  pct:-1.137255925\n",
      "[Iter.  710]  loss:235.565948  pct:-1.131932598\n",
      "[Iter.  720]  loss:232.911758  pct:-1.126729088\n",
      "[Iter.  730]  loss:230.299377  pct:-1.121618333\n",
      "[Iter.  740]  loss:227.727859  pct:-1.116597871\n",
      "[Iter.  750]  loss:225.195923  pct:-1.111825602\n",
      "[Iter.  760]  loss:222.702759  pct:-1.107108881\n",
      "[Iter.  770]  loss:220.247543  pct:-1.102462972\n",
      "[Iter.  780]  loss:217.829376  pct:-1.097931481\n",
      "[Iter.  790]  loss:215.447311  pct:-1.093546179\n",
      "[Iter.  800]  loss:213.100494  pct:-1.089276539\n",
      "[Iter.  810]  loss:210.788300  pct:-1.085025556\n",
      "[Iter.  820]  loss:208.509705  pct:-1.080987406\n",
      "[Iter.  830]  loss:206.264099  pct:-1.076978874\n",
      "[Iter.  840]  loss:204.050797  pct:-1.073043066\n",
      "[Iter.  850]  loss:201.869049  pct:-1.069217800\n",
      "[Iter.  860]  loss:199.718201  pct:-1.065467142\n",
      "[Iter.  870]  loss:197.597672  pct:-1.061760604\n",
      "[Iter.  880]  loss:195.506866  pct:-1.058112192\n",
      "[Iter.  890]  loss:193.445084  pct:-1.054583337\n",
      "[Iter.  900]  loss:191.411804  pct:-1.051088702\n",
      "[Iter.  910]  loss:189.406586  pct:-1.047593963\n",
      "[Iter.  920]  loss:187.428238  pct:-1.044497883\n",
      "[Iter.  930]  loss:185.476868  pct:-1.041129267\n",
      "[Iter.  940]  loss:183.551910  pct:-1.037842239\n",
      "[Iter.  950]  loss:181.652557  pct:-1.034777041\n",
      "[Iter.  960]  loss:179.778473  pct:-1.031686259\n",
      "[Iter.  970]  loss:177.929153  pct:-1.028665684\n",
      "[Iter.  980]  loss:176.104340  pct:-1.025584514\n",
      "[Iter.  990]  loss:174.303284  pct:-1.022720912\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.783266  pct:100.000000000\n",
      "[Iter.    2]  loss:2.781810  pct:-0.052304880\n",
      "[Iter.    4]  loss:2.781225  pct:-0.021032320\n",
      "[Iter.    6]  loss:2.780987  pct:-0.008555286\n",
      "[Iter.    8]  loss:2.780879  pct:-0.003883643\n",
      "[Iter.   10]  loss:2.780752  pct:-0.004578247\n",
      "[Iter.   12]  loss:2.780698  pct:-0.001929125\n",
      "[Iter.   14]  loss:2.780657  pct:-0.001474737\n",
      "[Iter.   16]  loss:2.780583  pct:-0.002666570\n",
      "[Iter.   18]  loss:2.780516  pct:-0.002392260\n",
      "[Iter.   20]  loss:2.780494  pct:-0.000788865\n",
      "[Iter.   22]  loss:2.780420  pct:-0.002692450\n",
      "[Iter.   24]  loss:2.780396  pct:-0.000840342\n",
      "[Iter.   26]  loss:2.780329  pct:-0.002400996\n",
      "[Iter.   28]  loss:2.780317  pct:-0.000463060\n",
      "[Iter.   30]  loss:2.780291  pct:-0.000917550\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.780291\n",
      "Best loss: 2.780291 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [01:51<00:00, 90.01it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:333.977081  pct:100.000000000\n",
      "[Iter.   10]  loss:71.334221  pct:-78.640980921\n",
      "[Iter.   20]  loss:59.971203  pct:-15.929266339\n",
      "[Iter.   30]  loss:51.921555  pct:-13.422522648\n",
      "[Iter.   40]  loss:45.587505  pct:-12.199267294\n",
      "[Iter.   50]  loss:40.630180  pct:-10.874306336\n",
      "[Iter.   60]  loss:36.588165  pct:-9.948306997\n",
      "[Iter.   70]  loss:33.185104  pct:-9.300988139\n",
      "[Iter.   80]  loss:30.267622  pct:-8.791541963\n",
      "[Iter.   90]  loss:27.732977  pct:-8.374113702\n",
      "[Iter.  100]  loss:25.507303  pct:-8.025368796\n",
      "[Iter.  110]  loss:23.536398  pct:-7.726827433\n",
      "[Iter.  120]  loss:21.779793  pct:-7.463355919\n",
      "[Iter.  130]  loss:20.204884  pct:-7.231056905\n",
      "[Iter.  140]  loss:18.786127  pct:-7.021849345\n",
      "[Iter.  150]  loss:17.502823  pct:-6.831127077\n",
      "[Iter.  160]  loss:16.338114  pct:-6.654407117\n",
      "[Iter.  170]  loss:15.277930  pct:-6.489020330\n",
      "[Iter.  180]  loss:14.310414  pct:-6.332768438\n",
      "[Iter.  190]  loss:13.425565  pct:-6.183255976\n",
      "[Iter.  200]  loss:12.614732  pct:-6.039470156\n",
      "[Iter.  210]  loss:11.870514  pct:-5.899593310\n",
      "[Iter.  220]  loss:11.186477  pct:-5.762490263\n",
      "[Iter.  230]  loss:10.556952  pct:-5.627546970\n",
      "[Iter.  240]  loss:9.976968  pct:-5.493864505\n",
      "[Iter.  250]  loss:9.442067  pct:-5.361355027\n",
      "[Iter.  260]  loss:8.948364  pct:-5.228758500\n",
      "[Iter.  270]  loss:8.492344  pct:-5.096130891\n",
      "[Iter.  280]  loss:8.070857  pct:-4.963139263\n",
      "[Iter.  290]  loss:7.681065  pct:-4.829628873\n",
      "[Iter.  300]  loss:7.320402  pct:-4.695481104\n",
      "[Iter.  310]  loss:6.986542  pct:-4.560671105\n",
      "[Iter.  320]  loss:6.677378  pct:-4.425136746\n",
      "[Iter.  330]  loss:6.390997  pct:-4.288820555\n",
      "[Iter.  340]  loss:6.125638  pct:-4.152081196\n",
      "[Iter.  350]  loss:5.879703  pct:-4.014854285\n",
      "[Iter.  360]  loss:5.651711  pct:-3.877595873\n",
      "[Iter.  370]  loss:5.440343  pct:-3.739903605\n",
      "[Iter.  380]  loss:5.244320  pct:-3.603127855\n",
      "[Iter.  390]  loss:5.062523  pct:-3.466550744\n",
      "[Iter.  400]  loss:4.893890  pct:-3.331015955\n",
      "[Iter.  410]  loss:4.737466  pct:-3.196303386\n",
      "[Iter.  420]  loss:4.592344  pct:-3.063284063\n",
      "[Iter.  430]  loss:4.457701  pct:-2.931902957\n",
      "[Iter.  440]  loss:4.332779  pct:-2.802392304\n",
      "[Iter.  450]  loss:4.216877  pct:-2.674991548\n",
      "[Iter.  460]  loss:4.109319  pct:-2.550661039\n",
      "[Iter.  470]  loss:4.009528  pct:-2.428408329\n",
      "[Iter.  480]  loss:3.916926  pct:-2.309560818\n",
      "[Iter.  490]  loss:3.830999  pct:-2.193736028\n",
      "[Iter.  500]  loss:3.751270  pct:-2.081137897\n",
      "[Iter.  510]  loss:3.677278  pct:-1.972471280\n",
      "[Iter.  520]  loss:3.608636  pct:-1.866643499\n",
      "[Iter.  530]  loss:3.544930  pct:-1.765387247\n",
      "[Iter.  540]  loss:3.485831  pct:-1.667140599\n",
      "[Iter.  550]  loss:3.430979  pct:-1.573557041\n",
      "[Iter.  560]  loss:3.380078  pct:-1.483582755\n",
      "[Iter.  570]  loss:3.332844  pct:-1.397404105\n",
      "[Iter.  580]  loss:3.289020  pct:-1.314933112\n",
      "[Iter.  590]  loss:3.248358  pct:-1.236267998\n",
      "[Iter.  600]  loss:3.210620  pct:-1.161758622\n",
      "[Iter.  610]  loss:3.175600  pct:-1.090773291\n",
      "[Iter.  620]  loss:3.143103  pct:-1.023316987\n",
      "[Iter.  630]  loss:3.112956  pct:-0.959172749\n",
      "[Iter.  640]  loss:3.084979  pct:-0.898704587\n",
      "[Iter.  650]  loss:3.059015  pct:-0.841634825\n",
      "[Iter.  660]  loss:3.034924  pct:-0.787549046\n",
      "[Iter.  670]  loss:3.012569  pct:-0.736578720\n",
      "[Iter.  680]  loss:2.991824  pct:-0.688616184\n",
      "[Iter.  690]  loss:2.972573  pct:-0.643449240\n",
      "[Iter.  700]  loss:2.954710  pct:-0.600952400\n",
      "[Iter.  710]  loss:2.938130  pct:-0.561125479\n",
      "[Iter.  720]  loss:2.922742  pct:-0.523734908\n",
      "[Iter.  730]  loss:2.908456  pct:-0.488780856\n",
      "[Iter.  740]  loss:2.895205  pct:-0.455621218\n",
      "[Iter.  750]  loss:2.882903  pct:-0.424882125\n",
      "[Iter.  760]  loss:2.871490  pct:-0.395913817\n",
      "[Iter.  770]  loss:2.860892  pct:-0.369066498\n",
      "[Iter.  780]  loss:2.851056  pct:-0.343790757\n",
      "[Iter.  790]  loss:2.841926  pct:-0.320240585\n",
      "[Iter.  800]  loss:2.833441  pct:-0.298576280\n",
      "[Iter.  810]  loss:2.825567  pct:-0.277887353\n",
      "[Iter.  820]  loss:2.818256  pct:-0.258739831\n",
      "[Iter.  830]  loss:2.811465  pct:-0.240960227\n",
      "[Iter.  840]  loss:2.805154  pct:-0.224488515\n",
      "[Iter.  850]  loss:2.799293  pct:-0.208946912\n",
      "[Iter.  860]  loss:2.793852  pct:-0.194368765\n",
      "[Iter.  870]  loss:2.788796  pct:-0.180965387\n",
      "[Iter.  880]  loss:2.784094  pct:-0.168580866\n",
      "[Iter.  890]  loss:2.779729  pct:-0.156782308\n",
      "[Iter.  900]  loss:2.775683  pct:-0.145552417\n",
      "[Iter.  910]  loss:2.771912  pct:-0.135878015\n",
      "[Iter.  920]  loss:2.768400  pct:-0.126687551\n",
      "[Iter.  930]  loss:2.765137  pct:-0.117874399\n",
      "[Iter.  940]  loss:2.762101  pct:-0.109787827\n",
      "[Iter.  950]  loss:2.759273  pct:-0.102407473\n",
      "[Iter.  960]  loss:2.756636  pct:-0.095548105\n",
      "[Iter.  970]  loss:2.754184  pct:-0.088953890\n",
      "[Iter.  980]  loss:2.751903  pct:-0.082808996\n",
      "[Iter.  990]  loss:2.749779  pct:-0.077202857\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.718548  pct:100.000000000\n",
      "[Iter.    2]  loss:2.718552  pct:0.000140321\n",
      "[Iter.    4]  loss:2.718552  pct:0.000017540\n",
      "[Iter.    6]  loss:2.718553  pct:0.000026310\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.718553\n",
      "Best loss: 2.718553 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [01:44<00:00, 95.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:265.438110  pct:100.000000000\n",
      "[Iter.   10]  loss:59.089436  pct:-77.738904303\n",
      "[Iter.   20]  loss:48.254913  pct:-18.335802570\n",
      "[Iter.   30]  loss:41.258125  pct:-14.499638570\n",
      "[Iter.   40]  loss:36.007736  pct:-12.725709325\n",
      "[Iter.   50]  loss:31.864201  pct:-11.507348283\n",
      "[Iter.   60]  loss:28.482227  pct:-10.613708186\n",
      "[Iter.   70]  loss:25.654573  pct:-9.927783570\n",
      "[Iter.   80]  loss:23.248245  pct:-9.379724075\n",
      "[Iter.   90]  loss:21.172413  pct:-8.928985158\n",
      "[Iter.  100]  loss:19.362200  pct:-8.549866753\n",
      "[Iter.  110]  loss:17.769152  pct:-8.227619349\n",
      "[Iter.  120]  loss:16.356266  pct:-7.951340001\n",
      "[Iter.  130]  loss:15.095456  pct:-7.708421327\n",
      "[Iter.  140]  loss:13.965864  pct:-7.482993111\n",
      "[Iter.  150]  loss:12.952107  pct:-7.258818637\n",
      "[Iter.  160]  loss:12.041777  pct:-7.028437475\n",
      "[Iter.  170]  loss:11.221339  pct:-6.813258996\n",
      "[Iter.  180]  loss:10.481155  pct:-6.596216507\n",
      "[Iter.  190]  loss:9.809897  pct:-6.404427254\n",
      "[Iter.  200]  loss:9.200247  pct:-6.214648182\n",
      "[Iter.  210]  loss:8.643749  pct:-6.048724402\n",
      "[Iter.  220]  loss:8.134012  pct:-5.897174950\n",
      "[Iter.  230]  loss:7.666465  pct:-5.748048160\n",
      "[Iter.  240]  loss:7.240706  pct:-5.553528253\n",
      "[Iter.  250]  loss:6.854703  pct:-5.331013567\n",
      "[Iter.  260]  loss:6.503831  pct:-5.118698294\n",
      "[Iter.  270]  loss:6.183172  pct:-4.930319044\n",
      "[Iter.  280]  loss:5.890401  pct:-4.734954771\n",
      "[Iter.  290]  loss:5.623689  pct:-4.527920070\n",
      "[Iter.  300]  loss:5.380269  pct:-4.328477929\n",
      "[Iter.  310]  loss:5.157666  pct:-4.137383931\n",
      "[Iter.  320]  loss:4.953924  pct:-3.950275554\n",
      "[Iter.  330]  loss:4.767362  pct:-3.765945028\n",
      "[Iter.  340]  loss:4.596490  pct:-3.584198775\n",
      "[Iter.  350]  loss:4.439930  pct:-3.406086121\n",
      "[Iter.  360]  loss:4.296474  pct:-3.231041558\n",
      "[Iter.  370]  loss:4.164998  pct:-3.060088949\n",
      "[Iter.  380]  loss:4.044472  pct:-2.893778949\n",
      "[Iter.  390]  loss:3.933970  pct:-2.732185450\n",
      "[Iter.  400]  loss:3.832736  pct:-2.573310186\n",
      "[Iter.  410]  loss:3.739958  pct:-2.420690358\n",
      "[Iter.  420]  loss:3.654912  pct:-2.273990662\n",
      "[Iter.  430]  loss:3.576962  pct:-2.132740067\n",
      "[Iter.  440]  loss:3.505517  pct:-1.997365170\n",
      "[Iter.  450]  loss:3.440024  pct:-1.868286794\n",
      "[Iter.  460]  loss:3.379993  pct:-1.745052589\n",
      "[Iter.  470]  loss:3.324961  pct:-1.628169280\n",
      "[Iter.  480]  loss:3.274517  pct:-1.517148518\n",
      "[Iter.  490]  loss:3.228275  pct:-1.412177772\n",
      "[Iter.  500]  loss:3.185880  pct:-1.313228904\n",
      "[Iter.  510]  loss:3.147015  pct:-1.219924341\n",
      "[Iter.  520]  loss:3.111387  pct:-1.132123043\n",
      "[Iter.  530]  loss:3.078727  pct:-1.049677619\n",
      "[Iter.  540]  loss:3.048792  pct:-0.972321325\n",
      "[Iter.  550]  loss:3.021345  pct:-0.900265500\n",
      "[Iter.  560]  loss:2.996180  pct:-0.832909908\n",
      "[Iter.  570]  loss:2.973106  pct:-0.770103086\n",
      "[Iter.  580]  loss:2.951918  pct:-0.712648027\n",
      "[Iter.  590]  loss:2.932434  pct:-0.660046814\n",
      "[Iter.  600]  loss:2.914635  pct:-0.606966727\n",
      "[Iter.  610]  loss:2.898330  pct:-0.559441669\n",
      "[Iter.  620]  loss:2.883381  pct:-0.515782699\n",
      "[Iter.  630]  loss:2.869669  pct:-0.475542169\n",
      "[Iter.  640]  loss:2.857093  pct:-0.438242298\n",
      "[Iter.  650]  loss:2.845552  pct:-0.403921520\n",
      "[Iter.  660]  loss:2.834963  pct:-0.372128892\n",
      "[Iter.  670]  loss:2.825249  pct:-0.342671221\n",
      "[Iter.  680]  loss:2.816334  pct:-0.315537143\n",
      "[Iter.  690]  loss:2.808152  pct:-0.290512786\n",
      "[Iter.  700]  loss:2.800642  pct:-0.267425263\n",
      "[Iter.  710]  loss:2.793747  pct:-0.246221379\n",
      "[Iter.  720]  loss:2.787416  pct:-0.226594991\n",
      "[Iter.  730]  loss:2.781605  pct:-0.208480327\n",
      "[Iter.  740]  loss:2.776265  pct:-0.191979067\n",
      "[Iter.  750]  loss:2.771361  pct:-0.176632760\n",
      "[Iter.  760]  loss:2.766856  pct:-0.162561185\n",
      "[Iter.  770]  loss:2.762716  pct:-0.149641944\n",
      "[Iter.  780]  loss:2.758911  pct:-0.137723989\n",
      "[Iter.  790]  loss:2.755412  pct:-0.126800620\n",
      "[Iter.  800]  loss:2.752195  pct:-0.116777337\n",
      "[Iter.  810]  loss:2.749237  pct:-0.107462693\n",
      "[Iter.  820]  loss:2.746518  pct:-0.098914799\n",
      "[Iter.  830]  loss:2.744011  pct:-0.091278181\n",
      "[Iter.  840]  loss:2.741701  pct:-0.084167339\n",
      "[Iter.  850]  loss:2.739575  pct:-0.077551009\n",
      "[Iter.  860]  loss:2.737615  pct:-0.071554078\n",
      "[Iter.  870]  loss:2.735809  pct:-0.065961889\n",
      "[Iter.  880]  loss:2.734144  pct:-0.060837587\n",
      "[Iter.  890]  loss:2.732609  pct:-0.056165799\n",
      "[Iter.  900]  loss:2.731194  pct:-0.051791266\n",
      "[Iter.  910]  loss:2.729886  pct:-0.047889844\n",
      "[Iter.  920]  loss:2.728679  pct:-0.044200989\n",
      "[Iter.  930]  loss:2.727565  pct:-0.040839119\n",
      "[Iter.  940]  loss:2.726536  pct:-0.037709016\n",
      "[Iter.  950]  loss:2.725585  pct:-0.034898807\n",
      "[Iter.  960]  loss:2.724705  pct:-0.032251772\n",
      "[Iter.  970]  loss:2.723893  pct:-0.029829607\n",
      "[Iter.  980]  loss:2.723140  pct:-0.027650292\n",
      "[Iter.  990]  loss:2.722443  pct:-0.025565427\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.712479  pct:100.000000000\n",
      "[Iter.    2]  loss:2.712473  pct:-0.000202163\n",
      "[Iter.    4]  loss:2.712470  pct:-0.000114266\n",
      "[Iter.    6]  loss:2.712469  pct:-0.000043949\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.712469\n",
      "Best loss: 2.712469 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 10%|▉         | 975/10000 [00:09<01:24, 106.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:409.364105  pct:100.000000000\n",
      "[Iter.   10]  loss:65.445129  pct:-84.012978041\n",
      "[Iter.   20]  loss:56.858391  pct:-13.120515867\n",
      "[Iter.   30]  loss:50.332127  pct:-11.478102173\n",
      "[Iter.   40]  loss:45.047703  pct:-10.499106998\n",
      "[Iter.   50]  loss:40.651058  pct:-9.759975138\n",
      "[Iter.   60]  loss:36.919773  pct:-9.178814183\n",
      "[Iter.   70]  loss:33.704552  pct:-8.708670544\n",
      "[Iter.   80]  loss:30.900507  pct:-8.319483816\n",
      "[Iter.   90]  loss:28.431347  pct:-7.990678218\n",
      "[Iter.  100]  loss:26.239838  pct:-7.708073962\n",
      "[Iter.  110]  loss:24.281998  pct:-7.461326523\n",
      "[Iter.  120]  loss:22.523298  pct:-7.242811898\n",
      "[Iter.  130]  loss:20.936371  pct:-7.045715043\n",
      "[Iter.  140]  loss:19.499012  pct:-6.865367768\n",
      "[Iter.  150]  loss:18.192238  pct:-6.701745400\n",
      "[Iter.  160]  loss:17.000734  pct:-6.549515977\n",
      "[Iter.  170]  loss:15.913142  pct:-6.397324397\n",
      "[Iter.  180]  loss:14.917082  pct:-6.259356943\n",
      "[Iter.  190]  loss:14.003284  pct:-6.125851842\n",
      "[Iter.  200]  loss:13.164050  pct:-5.993118674\n",
      "[Iter.  210]  loss:12.391823  pct:-5.866183137\n",
      "[Iter.  220]  loss:11.680714  pct:-5.738535581\n",
      "[Iter.  230]  loss:11.024775  pct:-5.615573856\n",
      "[Iter.  240]  loss:10.419064  pct:-5.494089520\n",
      "[Iter.  250]  loss:9.859137  pct:-5.374062487\n",
      "[Iter.  260]  loss:9.340735  pct:-5.258078551\n",
      "[Iter.  270]  loss:8.860597  pct:-5.140267402\n",
      "[Iter.  280]  loss:8.415776  pct:-5.020208246\n",
      "[Iter.  290]  loss:8.004015  pct:-4.892730884\n",
      "[Iter.  300]  loss:7.622478  pct:-4.766825636\n",
      "[Iter.  310]  loss:7.268786  pct:-4.640113093\n",
      "[Iter.  320]  loss:6.940340  pct:-4.518586602\n",
      "[Iter.  330]  loss:6.635636  pct:-4.390328594\n",
      "[Iter.  340]  loss:6.352919  pct:-4.260582669\n",
      "[Iter.  350]  loss:6.090551  pct:-4.129884200\n",
      "[Iter.  360]  loss:5.846855  pct:-4.001210070\n",
      "[Iter.  370]  loss:5.620116  pct:-3.877972008\n",
      "[Iter.  380]  loss:5.409673  pct:-3.744452110\n",
      "[Iter.  390]  loss:5.214359  pct:-3.610465912\n",
      "[Iter.  400]  loss:5.032893  pct:-3.480113903\n",
      "[Iter.  410]  loss:4.864118  pct:-3.353450042\n",
      "[Iter.  420]  loss:4.707382  pct:-3.222288383\n",
      "[Iter.  430]  loss:4.561779  pct:-3.093061807\n",
      "[Iter.  440]  loss:4.426488  pct:-2.965763172\n",
      "[Iter.  450]  loss:4.300723  pct:-2.841198843\n",
      "[Iter.  460]  loss:4.183838  pct:-2.717792318\n",
      "[Iter.  470]  loss:4.075202  pct:-2.596572384\n",
      "[Iter.  480]  loss:3.974235  pct:-2.477593565\n",
      "[Iter.  490]  loss:3.880401  pct:-2.361050487\n",
      "[Iter.  500]  loss:3.793184  pct:-2.247637120\n",
      "[Iter.  510]  loss:3.712126  pct:-2.136939169\n",
      "[Iter.  520]  loss:3.636791  pct:-2.029418136\n",
      "[Iter.  530]  loss:3.566767  pct:-1.925427534\n",
      "[Iter.  540]  loss:3.501669  pct:-1.825140811\n",
      "[Iter.  550]  loss:3.441142  pct:-1.728521216\n",
      "[Iter.  560]  loss:3.384870  pct:-1.635251352\n",
      "[Iter.  570]  loss:3.332585  pct:-1.544687587\n",
      "[Iter.  580]  loss:3.283970  pct:-1.458755563\n",
      "[Iter.  590]  loss:3.238784  pct:-1.375964954\n",
      "[Iter.  600]  loss:3.196785  pct:-1.296747861\n",
      "[Iter.  610]  loss:3.157746  pct:-1.221199812\n",
      "[Iter.  620]  loss:3.121463  pct:-1.149031371\n",
      "[Iter.  630]  loss:3.087745  pct:-1.080171700\n",
      "[Iter.  640]  loss:3.056393  pct:-1.015370078\n",
      "[Iter.  650]  loss:3.027232  pct:-0.954105416\n",
      "[Iter.  660]  loss:3.000134  pct:-0.895139154\n",
      "[Iter.  670]  loss:2.974953  pct:-0.839330948\n",
      "[Iter.  680]  loss:2.951541  pct:-0.786970007\n",
      "[Iter.  690]  loss:2.929780  pct:-0.737281901\n",
      "[Iter.  700]  loss:2.909561  pct:-0.690123066\n",
      "[Iter.  710]  loss:2.890747  pct:-0.646637959\n",
      "[Iter.  720]  loss:2.873241  pct:-0.605575245\n",
      "[Iter.  730]  loss:2.856960  pct:-0.566630206\n",
      "[Iter.  740]  loss:2.841826  pct:-0.529735558\n",
      "[Iter.  750]  loss:2.827756  pct:-0.495113798\n",
      "[Iter.  760]  loss:2.814666  pct:-0.462916005\n",
      "[Iter.  770]  loss:2.802483  pct:-0.432829845\n",
      "[Iter.  780]  loss:2.791158  pct:-0.404110268\n",
      "[Iter.  790]  loss:2.780625  pct:-0.377348047\n",
      "[Iter.  800]  loss:2.770824  pct:-0.352497190\n",
      "[Iter.  810]  loss:2.761705  pct:-0.329091920\n",
      "[Iter.  820]  loss:2.753225  pct:-0.307076548\n",
      "[Iter.  830]  loss:2.745326  pct:-0.286892958\n",
      "[Iter.  840]  loss:2.737973  pct:-0.267839518\n",
      "[Iter.  850]  loss:2.731130  pct:-0.249924024\n",
      "[Iter.  860]  loss:2.724761  pct:-0.233178169\n",
      "[Iter.  870]  loss:2.718828  pct:-0.217745530\n",
      "[Iter.  880]  loss:2.713307  pct:-0.203076346\n",
      "[Iter.  890]  loss:2.708168  pct:-0.189404008\n",
      "[Iter.  900]  loss:2.703379  pct:-0.176848422\n",
      "[Iter.  910]  loss:2.698916  pct:-0.165088133\n",
      "[Iter.  920]  loss:2.694761  pct:-0.153938937\n",
      "[Iter.  930]  loss:2.690887  pct:-0.143771632\n",
      "[Iter.  940]  loss:2.687278  pct:-0.134099483\n",
      "[Iter.  950]  loss:2.683914  pct:-0.125176755\n",
      "[Iter.  960]  loss:2.680776  pct:-0.116921214\n",
      "[Iter.  970]  loss:2.677847  pct:-0.109285039\n",
      "[Iter.  980]  loss:2.675120  pct:-0.101818932\n",
      "[Iter.  990]  loss:2.672574  pct:-0.095167076\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.633903  pct:100.000000000\n",
      "[Iter.    2]  loss:2.633884  pct:-0.000706049\n",
      "[Iter.    4]  loss:2.633878  pct:-0.000226299\n",
      "[Iter.    6]  loss:2.633878  pct:-0.000018104\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.633878\n",
      "Best loss: 2.633878 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 26%|██▌       | 2599/10000 [00:34<01:38, 74.78it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:324.996277  pct:100.000000000\n",
      "[Iter.   10]  loss:71.924706  pct:-77.869065507\n",
      "[Iter.   20]  loss:59.881123  pct:-16.744709390\n",
      "[Iter.   30]  loss:51.528732  pct:-13.948286084\n",
      "[Iter.   40]  loss:45.217308  pct:-12.248359262\n",
      "[Iter.   50]  loss:40.213146  pct:-11.066916743\n",
      "[Iter.   60]  loss:36.115654  pct:-10.189434561\n",
      "[Iter.   70]  loss:32.678909  pct:-9.515942009\n",
      "[Iter.   80]  loss:29.743559  pct:-8.982400211\n",
      "[Iter.   90]  loss:27.201500  pct:-8.546586354\n",
      "[Iter.  100]  loss:24.975891  pct:-8.181934197\n",
      "[Iter.  110]  loss:23.010262  pct:-7.870107892\n",
      "[Iter.  120]  loss:21.261858  pct:-7.598364523\n",
      "[Iter.  130]  loss:19.697535  pct:-7.357416395\n",
      "[Iter.  140]  loss:18.291019  pct:-7.140564303\n",
      "[Iter.  150]  loss:17.021088  pct:-6.942925174\n",
      "[Iter.  160]  loss:15.870443  pct:-6.760110319\n",
      "[Iter.  170]  loss:14.824711  pct:-6.589182643\n",
      "[Iter.  180]  loss:13.871805  pct:-6.427819502\n",
      "[Iter.  190]  loss:13.001555  pct:-6.273514775\n",
      "[Iter.  200]  loss:12.205254  pct:-6.124665970\n",
      "[Iter.  210]  loss:11.475394  pct:-5.979878632\n",
      "[Iter.  220]  loss:10.805478  pct:-5.837848691\n",
      "[Iter.  230]  loss:10.189739  pct:-5.698395418\n",
      "[Iter.  240]  loss:9.623210  pct:-5.559801496\n",
      "[Iter.  250]  loss:9.101439  pct:-5.422010260\n",
      "[Iter.  260]  loss:8.620437  pct:-5.284899225\n",
      "[Iter.  270]  loss:8.176717  pct:-5.147301476\n",
      "[Iter.  280]  loss:7.767108  pct:-5.009447849\n",
      "[Iter.  290]  loss:7.388793  pct:-4.870743450\n",
      "[Iter.  300]  loss:7.039185  pct:-4.731597806\n",
      "[Iter.  310]  loss:6.715948  pct:-4.591965718\n",
      "[Iter.  320]  loss:6.416983  pct:-4.451568026\n",
      "[Iter.  330]  loss:6.140401  pct:-4.310152906\n",
      "[Iter.  340]  loss:5.884449  pct:-4.168325263\n",
      "[Iter.  350]  loss:5.647531  pct:-4.026178988\n",
      "[Iter.  360]  loss:5.428187  pct:-3.883877799\n",
      "[Iter.  370]  loss:5.225071  pct:-3.741882936\n",
      "[Iter.  380]  loss:5.036961  pct:-3.600149227\n",
      "[Iter.  390]  loss:4.862721  pct:-3.459212259\n",
      "[Iter.  400]  loss:4.701313  pct:-3.319292487\n",
      "[Iter.  410]  loss:4.551763  pct:-3.181035216\n",
      "[Iter.  420]  loss:4.413209  pct:-3.043965480\n",
      "[Iter.  430]  loss:4.284825  pct:-2.909087569\n",
      "[Iter.  440]  loss:4.165866  pct:-2.776273883\n",
      "[Iter.  450]  loss:4.055634  pct:-2.646096157\n",
      "[Iter.  460]  loss:3.953490  pct:-2.518547302\n",
      "[Iter.  470]  loss:3.858850  pct:-2.393834427\n",
      "[Iter.  480]  loss:3.771135  pct:-2.273096297\n",
      "[Iter.  490]  loss:3.689852  pct:-2.155413884\n",
      "[Iter.  500]  loss:3.614534  pct:-2.041197158\n",
      "[Iter.  510]  loss:3.544743  pct:-1.930845616\n",
      "[Iter.  520]  loss:3.480068  pct:-1.824542023\n",
      "[Iter.  530]  loss:3.420130  pct:-1.722328211\n",
      "[Iter.  540]  loss:3.364597  pct:-1.623701365\n",
      "[Iter.  550]  loss:3.313134  pct:-1.529555079\n",
      "[Iter.  560]  loss:3.265453  pct:-1.439153904\n",
      "[Iter.  570]  loss:3.221276  pct:-1.352847021\n",
      "[Iter.  580]  loss:3.180344  pct:-1.270674711\n",
      "[Iter.  590]  loss:3.142413  pct:-1.192668597\n",
      "[Iter.  600]  loss:3.107267  pct:-1.118454935\n",
      "[Iter.  610]  loss:3.074710  pct:-1.047747069\n",
      "[Iter.  620]  loss:3.044534  pct:-0.981423311\n",
      "[Iter.  630]  loss:3.016569  pct:-0.918533501\n",
      "[Iter.  640]  loss:2.990659  pct:-0.858919436\n",
      "[Iter.  650]  loss:2.966648  pct:-0.802878887\n",
      "[Iter.  660]  loss:2.944394  pct:-0.750155268\n",
      "[Iter.  670]  loss:2.923774  pct:-0.700285238\n",
      "[Iter.  680]  loss:2.904670  pct:-0.653426565\n",
      "[Iter.  690]  loss:2.886968  pct:-0.609435981\n",
      "[Iter.  700]  loss:2.870555  pct:-0.568494701\n",
      "[Iter.  710]  loss:2.855348  pct:-0.529776540\n",
      "[Iter.  720]  loss:2.841256  pct:-0.493528956\n",
      "[Iter.  730]  loss:2.828197  pct:-0.459608809\n",
      "[Iter.  740]  loss:2.816095  pct:-0.427909585\n",
      "[Iter.  750]  loss:2.804875  pct:-0.398423273\n",
      "[Iter.  760]  loss:2.794480  pct:-0.370623517\n",
      "[Iter.  770]  loss:2.784846  pct:-0.344726176\n",
      "[Iter.  780]  loss:2.775919  pct:-0.320568688\n",
      "[Iter.  790]  loss:2.767643  pct:-0.298143579\n",
      "[Iter.  800]  loss:2.759973  pct:-0.277119845\n",
      "[Iter.  810]  loss:2.752861  pct:-0.257693264\n",
      "[Iter.  820]  loss:2.746264  pct:-0.239634481\n",
      "[Iter.  830]  loss:2.740148  pct:-0.222708081\n",
      "[Iter.  840]  loss:2.734477  pct:-0.206951824\n",
      "[Iter.  850]  loss:2.729222  pct:-0.192175097\n",
      "[Iter.  860]  loss:2.724342  pct:-0.178803753\n",
      "[Iter.  870]  loss:2.719814  pct:-0.166224443\n",
      "[Iter.  880]  loss:2.715614  pct:-0.154404143\n",
      "[Iter.  890]  loss:2.711720  pct:-0.143387540\n",
      "[Iter.  900]  loss:2.708112  pct:-0.133078021\n",
      "[Iter.  910]  loss:2.704767  pct:-0.123518276\n",
      "[Iter.  920]  loss:2.701659  pct:-0.114873905\n",
      "[Iter.  930]  loss:2.698780  pct:-0.106587068\n",
      "[Iter.  940]  loss:2.696113  pct:-0.098820593\n",
      "[Iter.  950]  loss:2.693639  pct:-0.091764318\n",
      "[Iter.  960]  loss:2.691343  pct:-0.085245629\n",
      "[Iter.  970]  loss:2.689214  pct:-0.079090677\n",
      "[Iter.  980]  loss:2.687243  pct:-0.073301895\n",
      "[Iter.  990]  loss:2.685415  pct:-0.068023451\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.658314  pct:100.000000000\n",
      "[Iter.    2]  loss:2.658319  pct:0.000197313\n",
      "[Iter.    4]  loss:2.658322  pct:0.000107625\n",
      "[Iter.    6]  loss:2.658324  pct:0.000089688\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.658324\n",
      "Best loss: 2.658324 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  9%|▉         | 897/10000 [00:12<02:03, 73.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:454.978973  pct:100.000000000\n",
      "[Iter.   10]  loss:93.342041  pct:-79.484317633\n",
      "[Iter.   20]  loss:79.776031  pct:-14.533654261\n",
      "[Iter.   30]  loss:70.207664  pct:-11.994037338\n",
      "[Iter.   40]  loss:62.645332  pct:-10.771376898\n",
      "[Iter.   50]  loss:56.551567  pct:-9.727405110\n",
      "[Iter.   60]  loss:51.485229  pct:-8.958792563\n",
      "[Iter.   70]  loss:47.166679  pct:-8.387939905\n",
      "[Iter.   80]  loss:43.424873  pct:-7.933155523\n",
      "[Iter.   90]  loss:40.142521  pct:-7.558692045\n",
      "[Iter.  100]  loss:37.234268  pct:-7.244818339\n",
      "[Iter.  110]  loss:34.636738  pct:-6.976182134\n",
      "[Iter.  120]  loss:32.300964  pct:-6.743630073\n",
      "[Iter.  130]  loss:30.188538  pct:-6.539825668\n",
      "[Iter.  140]  loss:28.268850  pct:-6.358993922\n",
      "[Iter.  150]  loss:26.517004  pct:-6.197090767\n",
      "[Iter.  160]  loss:24.912601  pct:-6.050466868\n",
      "[Iter.  170]  loss:23.438658  pct:-5.916458432\n",
      "[Iter.  180]  loss:22.080866  pct:-5.792959283\n",
      "[Iter.  190]  loss:20.827082  pct:-5.678147712\n",
      "[Iter.  200]  loss:19.666908  pct:-5.570503991\n",
      "[Iter.  210]  loss:18.591331  pct:-5.468967302\n",
      "[Iter.  220]  loss:17.592560  pct:-5.372243879\n",
      "[Iter.  230]  loss:16.663748  pct:-5.279572938\n",
      "[Iter.  240]  loss:15.798832  pct:-5.190404096\n",
      "[Iter.  250]  loss:14.992509  pct:-5.103687757\n",
      "[Iter.  260]  loss:14.240031  pct:-5.019024177\n",
      "[Iter.  270]  loss:13.537090  pct:-4.936372181\n",
      "[Iter.  280]  loss:12.879919  pct:-4.854597515\n",
      "[Iter.  290]  loss:12.265030  pct:-4.774014048\n",
      "[Iter.  300]  loss:11.689368  pct:-4.693520225\n",
      "[Iter.  310]  loss:11.150049  pct:-4.613756937\n",
      "[Iter.  320]  loss:10.644529  pct:-4.533790456\n",
      "[Iter.  330]  loss:10.170443  pct:-4.453806704\n",
      "[Iter.  340]  loss:9.725661  pct:-4.373273826\n",
      "[Iter.  350]  loss:9.308195  pct:-4.292419319\n",
      "[Iter.  360]  loss:8.916225  pct:-4.211017023\n",
      "[Iter.  370]  loss:8.548074  pct:-4.129008037\n",
      "[Iter.  380]  loss:8.202247  pct:-4.045672885\n",
      "[Iter.  390]  loss:7.877265  pct:-3.962099969\n",
      "[Iter.  400]  loss:7.571803  pct:-3.877777625\n",
      "[Iter.  410]  loss:7.284629  pct:-3.792667159\n",
      "[Iter.  420]  loss:7.014611  pct:-3.706682769\n",
      "[Iter.  430]  loss:6.760638  pct:-3.620628405\n",
      "[Iter.  440]  loss:6.521743  pct:-3.533622239\n",
      "[Iter.  450]  loss:6.297039  pct:-3.445456145\n",
      "[Iter.  460]  loss:6.085631  pct:-3.357255059\n",
      "[Iter.  470]  loss:5.886740  pct:-3.268217012\n",
      "[Iter.  480]  loss:5.699578  pct:-3.179373544\n",
      "[Iter.  490]  loss:5.523468  pct:-3.089890790\n",
      "[Iter.  500]  loss:5.357734  pct:-3.000539299\n",
      "[Iter.  510]  loss:5.201761  pct:-2.911165219\n",
      "[Iter.  520]  loss:5.054989  pct:-2.821581016\n",
      "[Iter.  530]  loss:4.916821  pct:-2.733296727\n",
      "[Iter.  540]  loss:4.786774  pct:-2.644946982\n",
      "[Iter.  550]  loss:4.664372  pct:-2.557081456\n",
      "[Iter.  560]  loss:4.549179  pct:-2.469653640\n",
      "[Iter.  570]  loss:4.440743  pct:-2.383630988\n",
      "[Iter.  580]  loss:4.338678  pct:-2.298378606\n",
      "[Iter.  590]  loss:4.242615  pct:-2.214110834\n",
      "[Iter.  600]  loss:4.152181  pct:-2.131564608\n",
      "[Iter.  610]  loss:4.067070  pct:-2.049793781\n",
      "[Iter.  620]  loss:3.986951  pct:-1.969935738\n",
      "[Iter.  630]  loss:3.911533  pct:-1.891614906\n",
      "[Iter.  640]  loss:3.840565  pct:-1.814324730\n",
      "[Iter.  650]  loss:3.773770  pct:-1.739212433\n",
      "[Iter.  660]  loss:3.710884  pct:-1.666397424\n",
      "[Iter.  670]  loss:3.651705  pct:-1.594736868\n",
      "[Iter.  680]  loss:3.596016  pct:-1.524996801\n",
      "[Iter.  690]  loss:3.543609  pct:-1.457383271\n",
      "[Iter.  700]  loss:3.494285  pct:-1.391908698\n",
      "[Iter.  710]  loss:3.447856  pct:-1.328710182\n",
      "[Iter.  720]  loss:3.404169  pct:-1.267073434\n",
      "[Iter.  730]  loss:3.363051  pct:-1.207876107\n",
      "[Iter.  740]  loss:3.324354  pct:-1.150644656\n",
      "[Iter.  750]  loss:3.287935  pct:-1.095511323\n",
      "[Iter.  760]  loss:3.253661  pct:-1.042427375\n",
      "[Iter.  770]  loss:3.221418  pct:-0.990969047\n",
      "[Iter.  780]  loss:3.191106  pct:-0.940961283\n",
      "[Iter.  790]  loss:3.162579  pct:-0.893961501\n",
      "[Iter.  800]  loss:3.135730  pct:-0.848960072\n",
      "[Iter.  810]  loss:3.110457  pct:-0.805971379\n",
      "[Iter.  820]  loss:3.086677  pct:-0.764513746\n",
      "[Iter.  830]  loss:3.064290  pct:-0.725263839\n",
      "[Iter.  840]  loss:3.043223  pct:-0.687512570\n",
      "[Iter.  850]  loss:3.023391  pct:-0.651666277\n",
      "[Iter.  860]  loss:3.004725  pct:-0.617410805\n",
      "[Iter.  870]  loss:2.987157  pct:-0.584659131\n",
      "[Iter.  880]  loss:2.970629  pct:-0.553306337\n",
      "[Iter.  890]  loss:2.955082  pct:-0.523358428\n",
      "[Iter.  900]  loss:2.940454  pct:-0.495025540\n",
      "[Iter.  910]  loss:2.926698  pct:-0.467812132\n",
      "[Iter.  920]  loss:2.913749  pct:-0.442443705\n",
      "[Iter.  930]  loss:2.901559  pct:-0.418348587\n",
      "[Iter.  940]  loss:2.890089  pct:-0.395323886\n",
      "[Iter.  950]  loss:2.879300  pct:-0.373291008\n",
      "[Iter.  960]  loss:2.869146  pct:-0.352663733\n",
      "[Iter.  970]  loss:2.859593  pct:-0.332962996\n",
      "[Iter.  980]  loss:2.850592  pct:-0.314757352\n",
      "[Iter.  990]  loss:2.842133  pct:-0.296740223\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.701507  pct:100.000000000\n",
      "[Iter.    2]  loss:2.701513  pct:0.000211809\n",
      "[Iter.    4]  loss:2.701522  pct:0.000326539\n",
      "[Iter.    6]  loss:2.701528  pct:0.000220634\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.701528\n",
      "Best loss: 2.701528 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 15%|█▍        | 1486/10000 [00:18<01:43, 82.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:799.570984  pct:100.000000000\n",
      "[Iter.   10]  loss:143.421860  pct:-82.062646358\n",
      "[Iter.   20]  loss:129.073578  pct:-10.004250319\n",
      "[Iter.   30]  loss:117.856750  pct:-8.690258360\n",
      "[Iter.   40]  loss:108.493546  pct:-7.944563987\n",
      "[Iter.   50]  loss:100.494057  pct:-7.373239386\n",
      "[Iter.   60]  loss:93.547165  pct:-6.912738935\n",
      "[Iter.   70]  loss:87.435150  pct:-6.533618390\n",
      "[Iter.   80]  loss:82.000160  pct:-6.216024013\n",
      "[Iter.   90]  loss:77.124313  pct:-5.946143093\n",
      "[Iter.  100]  loss:72.717255  pct:-5.714227491\n",
      "[Iter.  110]  loss:68.708679  pct:-5.512550576\n",
      "[Iter.  120]  loss:65.042503  pct:-5.335826398\n",
      "[Iter.  130]  loss:61.673538  pct:-5.179636353\n",
      "[Iter.  140]  loss:58.564903  pct:-5.040467985\n",
      "[Iter.  150]  loss:55.685905  pct:-4.915909773\n",
      "[Iter.  160]  loss:53.011002  pct:-4.803556390\n",
      "[Iter.  170]  loss:50.518562  pct:-4.701739630\n",
      "[Iter.  180]  loss:48.190094  pct:-4.609134180\n",
      "[Iter.  190]  loss:46.009903  pct:-4.524147723\n",
      "[Iter.  200]  loss:43.964214  pct:-4.446192010\n",
      "[Iter.  210]  loss:42.041176  pct:-4.374099508\n",
      "[Iter.  220]  loss:40.230289  pct:-4.307411357\n",
      "[Iter.  230]  loss:38.522385  pct:-4.245320724\n",
      "[Iter.  240]  loss:36.909363  pct:-4.187232606\n",
      "[Iter.  250]  loss:35.383957  pct:-4.132842640\n",
      "[Iter.  260]  loss:33.939739  pct:-4.081560707\n",
      "[Iter.  270]  loss:32.570812  pct:-4.033404596\n",
      "[Iter.  280]  loss:31.271980  pct:-3.987717379\n",
      "[Iter.  290]  loss:30.038568  pct:-3.944143536\n",
      "[Iter.  300]  loss:28.866285  pct:-3.902593337\n",
      "[Iter.  310]  loss:27.751146  pct:-3.863119189\n",
      "[Iter.  320]  loss:26.689693  pct:-3.824897370\n",
      "[Iter.  330]  loss:25.678551  pct:-3.788513842\n",
      "[Iter.  340]  loss:24.714771  pct:-3.753247058\n",
      "[Iter.  350]  loss:23.795654  pct:-3.718897350\n",
      "[Iter.  360]  loss:22.918692  pct:-3.685389991\n",
      "[Iter.  370]  loss:22.081417  pct:-3.653238870\n",
      "[Iter.  380]  loss:21.281754  pct:-3.621432178\n",
      "[Iter.  390]  loss:20.517708  pct:-3.590144552\n",
      "[Iter.  400]  loss:19.787270  pct:-3.560038181\n",
      "[Iter.  410]  loss:19.088869  pct:-3.529544560\n",
      "[Iter.  420]  loss:18.420763  pct:-3.499977268\n",
      "[Iter.  430]  loss:17.781464  pct:-3.470536981\n",
      "[Iter.  440]  loss:17.169533  pct:-3.441397515\n",
      "[Iter.  450]  loss:16.583658  pct:-3.412291791\n",
      "[Iter.  460]  loss:16.022558  pct:-3.383451339\n",
      "[Iter.  470]  loss:15.485052  pct:-3.354683418\n",
      "[Iter.  480]  loss:14.970053  pct:-3.325784027\n",
      "[Iter.  490]  loss:14.476500  pct:-3.296936697\n",
      "[Iter.  500]  loss:14.003418  pct:-3.267928043\n",
      "[Iter.  510]  loss:13.549901  pct:-3.238616180\n",
      "[Iter.  520]  loss:13.115051  pct:-3.209246612\n",
      "[Iter.  530]  loss:12.698055  pct:-3.179522471\n",
      "[Iter.  540]  loss:12.298071  pct:-3.149965497\n",
      "[Iter.  550]  loss:11.914393  pct:-3.119818429\n",
      "[Iter.  560]  loss:11.546252  pct:-3.089885999\n",
      "[Iter.  570]  loss:11.193019  pct:-3.059289973\n",
      "[Iter.  580]  loss:10.854069  pct:-3.028228218\n",
      "[Iter.  590]  loss:10.528742  pct:-2.997280807\n",
      "[Iter.  600]  loss:10.216533  pct:-2.965303302\n",
      "[Iter.  610]  loss:9.916837  pct:-2.933441092\n",
      "[Iter.  620]  loss:9.629162  pct:-2.900873650\n",
      "[Iter.  630]  loss:9.352970  pct:-2.868284033\n",
      "[Iter.  640]  loss:9.087791  pct:-2.835234978\n",
      "[Iter.  650]  loss:8.833200  pct:-2.801472101\n",
      "[Iter.  660]  loss:8.588739  pct:-2.767514827\n",
      "[Iter.  670]  loss:8.353992  pct:-2.733205373\n",
      "[Iter.  680]  loss:8.128513  pct:-2.699047181\n",
      "[Iter.  690]  loss:7.912043  pct:-2.663097575\n",
      "[Iter.  700]  loss:7.704106  pct:-2.628104540\n",
      "[Iter.  710]  loss:7.504394  pct:-2.592289679\n",
      "[Iter.  720]  loss:7.312585  pct:-2.555945684\n",
      "[Iter.  730]  loss:7.128395  pct:-2.518818464\n",
      "[Iter.  740]  loss:6.951452  pct:-2.482218765\n",
      "[Iter.  750]  loss:6.781502  pct:-2.444813000\n",
      "[Iter.  760]  loss:6.618272  pct:-2.406994987\n",
      "[Iter.  770]  loss:6.461498  pct:-2.368799156\n",
      "[Iter.  780]  loss:6.310883  pct:-2.330964263\n",
      "[Iter.  790]  loss:6.166227  pct:-2.292162657\n",
      "[Iter.  800]  loss:6.027277  pct:-2.253409422\n",
      "[Iter.  810]  loss:5.893788  pct:-2.214742333\n",
      "[Iter.  820]  loss:5.765561  pct:-2.175633507\n",
      "[Iter.  830]  loss:5.642392  pct:-2.136287225\n",
      "[Iter.  840]  loss:5.524094  pct:-2.096602421\n",
      "[Iter.  850]  loss:5.410423  pct:-2.057719452\n",
      "[Iter.  860]  loss:5.301250  pct:-2.017832843\n",
      "[Iter.  870]  loss:5.196373  pct:-1.978344644\n",
      "[Iter.  880]  loss:5.095636  pct:-1.938604006\n",
      "[Iter.  890]  loss:4.998856  pct:-1.899278171\n",
      "[Iter.  900]  loss:4.905889  pct:-1.859766334\n",
      "[Iter.  910]  loss:4.816592  pct:-1.820196622\n",
      "[Iter.  920]  loss:4.730808  pct:-1.780999641\n",
      "[Iter.  930]  loss:4.648393  pct:-1.742103595\n",
      "[Iter.  940]  loss:4.569248  pct:-1.702620311\n",
      "[Iter.  950]  loss:4.493232  pct:-1.663642375\n",
      "[Iter.  960]  loss:4.420208  pct:-1.625205840\n",
      "[Iter.  970]  loss:4.350062  pct:-1.586940800\n",
      "[Iter.  980]  loss:4.282689  pct:-1.548788900\n",
      "[Iter.  990]  loss:4.217969  pct:-1.511181548\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.646825  pct:100.000000000\n",
      "[Iter.    2]  loss:2.646830  pct:0.000180154\n",
      "[Iter.    4]  loss:2.646824  pct:-0.000216185\n",
      "[Iter.    6]  loss:2.646826  pct:0.000045039\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.646826\n",
      "Best loss: 2.646826 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [01:56<00:00, 85.94it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:181.370865  pct:100.000000000\n",
      "[Iter.   10]  loss:35.471119  pct:-80.442771148\n",
      "[Iter.   20]  loss:27.534376  pct:-22.375225318\n",
      "[Iter.   30]  loss:22.744364  pct:-17.396480438\n",
      "[Iter.   40]  loss:19.228607  pct:-15.457704776\n",
      "[Iter.   50]  loss:16.517963  pct:-14.096932468\n",
      "[Iter.   60]  loss:14.360797  pct:-13.059518462\n",
      "[Iter.   70]  loss:12.606259  pct:-12.217550260\n",
      "[Iter.   80]  loss:11.156548  pct:-11.499936340\n",
      "[Iter.   90]  loss:9.944598  pct:-10.863121798\n",
      "[Iter.  100]  loss:8.922407  pct:-10.278857198\n",
      "[Iter.  110]  loss:8.054385  pct:-9.728562600\n",
      "[Iter.  120]  loss:7.313325  pct:-9.200699533\n",
      "[Iter.  130]  loss:6.678096  pct:-8.685919911\n",
      "[Iter.  140]  loss:6.131901  pct:-8.178903764\n",
      "[Iter.  150]  loss:5.661072  pct:-7.678353358\n",
      "[Iter.  160]  loss:5.254321  pct:-7.185047196\n",
      "[Iter.  170]  loss:4.902426  pct:-6.697246874\n",
      "[Iter.  180]  loss:4.597642  pct:-6.217009351\n",
      "[Iter.  190]  loss:4.333495  pct:-5.745277358\n",
      "[Iter.  200]  loss:4.104449  pct:-5.285466093\n",
      "[Iter.  210]  loss:3.905709  pct:-4.842062657\n",
      "[Iter.  220]  loss:3.733197  pct:-4.416926122\n",
      "[Iter.  230]  loss:3.583394  pct:-4.012724864\n",
      "[Iter.  240]  loss:3.453238  pct:-3.632200041\n",
      "[Iter.  250]  loss:3.340124  pct:-3.275602685\n",
      "[Iter.  260]  loss:3.241782  pct:-2.944253376\n",
      "[Iter.  270]  loss:3.156250  pct:-2.638438617\n",
      "[Iter.  280]  loss:3.081863  pct:-2.356788311\n",
      "[Iter.  290]  loss:3.017157  pct:-2.099599947\n",
      "[Iter.  300]  loss:2.960850  pct:-1.866198252\n",
      "[Iter.  310]  loss:2.911849  pct:-1.654987109\n",
      "[Iter.  320]  loss:2.869190  pct:-1.465023971\n",
      "[Iter.  330]  loss:2.832026  pct:-1.295252809\n",
      "[Iter.  340]  loss:2.799667  pct:-1.142630657\n",
      "[Iter.  350]  loss:2.771424  pct:-1.008800992\n",
      "[Iter.  360]  loss:2.746778  pct:-0.889283239\n",
      "[Iter.  370]  loss:2.725270  pct:-0.783025793\n",
      "[Iter.  380]  loss:2.706480  pct:-0.689464517\n",
      "[Iter.  390]  loss:2.690068  pct:-0.606379605\n",
      "[Iter.  400]  loss:2.675729  pct:-0.533051374\n",
      "[Iter.  410]  loss:2.663178  pct:-0.469079922\n",
      "[Iter.  420]  loss:2.652158  pct:-0.413798277\n",
      "[Iter.  430]  loss:2.642502  pct:-0.364079143\n",
      "[Iter.  440]  loss:2.634025  pct:-0.320775426\n",
      "[Iter.  450]  loss:2.626566  pct:-0.283166885\n",
      "[Iter.  460]  loss:2.620009  pct:-0.249668274\n",
      "[Iter.  470]  loss:2.614233  pct:-0.220463653\n",
      "[Iter.  480]  loss:2.609130  pct:-0.195195828\n",
      "[Iter.  490]  loss:2.604635  pct:-0.172257772\n",
      "[Iter.  500]  loss:2.600671  pct:-0.152215654\n",
      "[Iter.  510]  loss:2.597166  pct:-0.134745104\n",
      "[Iter.  520]  loss:2.594068  pct:-0.119302636\n",
      "[Iter.  530]  loss:2.591324  pct:-0.105759864\n",
      "[Iter.  540]  loss:2.588882  pct:-0.094251418\n",
      "[Iter.  550]  loss:2.586709  pct:-0.083952215\n",
      "[Iter.  560]  loss:2.584773  pct:-0.074833342\n",
      "[Iter.  570]  loss:2.583054  pct:-0.066514022\n",
      "[Iter.  580]  loss:2.581527  pct:-0.059091137\n",
      "[Iter.  590]  loss:2.580167  pct:-0.052688888\n",
      "[Iter.  600]  loss:2.578946  pct:-0.047338733\n",
      "[Iter.  610]  loss:2.577852  pct:-0.042405935\n",
      "[Iter.  620]  loss:2.576875  pct:-0.037882799\n",
      "[Iter.  630]  loss:2.576004  pct:-0.033826172\n",
      "[Iter.  640]  loss:2.575229  pct:-0.030089195\n",
      "[Iter.  650]  loss:2.574542  pct:-0.026681993\n",
      "[Iter.  660]  loss:2.573921  pct:-0.024105401\n",
      "[Iter.  670]  loss:2.573364  pct:-0.021628768\n",
      "[Iter.  680]  loss:2.572874  pct:-0.019048551\n",
      "[Iter.  690]  loss:2.572439  pct:-0.016911590\n",
      "[Iter.  700]  loss:2.572028  pct:-0.015959826\n",
      "[Iter.  710]  loss:2.571654  pct:-0.014553384\n",
      "[Iter.  720]  loss:2.571317  pct:-0.013090681\n",
      "[Iter.  730]  loss:2.571016  pct:-0.011738649\n",
      "[Iter.  740]  loss:2.570746  pct:-0.010478855\n",
      "[Iter.  750]  loss:2.570507  pct:-0.009311392\n",
      "[Iter.  760]  loss:2.570295  pct:-0.008227066\n",
      "[Iter.  770]  loss:2.570111  pct:-0.007170287\n",
      "[Iter.  780]  loss:2.569944  pct:-0.006484334\n",
      "[Iter.  790]  loss:2.569779  pct:-0.006447646\n",
      "[Iter.  800]  loss:2.569629  pct:-0.005817172\n",
      "[Iter.  810]  loss:2.569497  pct:-0.005149471\n",
      "[Iter.  820]  loss:2.569383  pct:-0.004416711\n",
      "[Iter.  830]  loss:2.569286  pct:-0.003795198\n",
      "[Iter.  840]  loss:2.569193  pct:-0.003600472\n",
      "[Iter.  850]  loss:2.569111  pct:-0.003192286\n",
      "[Iter.  860]  loss:2.569042  pct:-0.002681977\n",
      "[Iter.  870]  loss:2.568985  pct:-0.002227307\n",
      "[Iter.  880]  loss:2.568932  pct:-0.002060305\n",
      "[Iter.  890]  loss:2.568881  pct:-0.002013943\n",
      "[Iter.  900]  loss:2.568835  pct:-0.001781958\n",
      "[Iter.  910]  loss:2.568797  pct:-0.001466429\n",
      "[Iter.  920]  loss:2.568766  pct:-0.001225136\n",
      "[Iter.  930]  loss:2.568741  pct:-0.000955989\n",
      "[Iter.  940]  loss:2.568723  pct:-0.000696115\n",
      "converged\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.567175  pct:100.000000000\n",
      "[Iter.    2]  loss:2.567181  pct:0.000232180\n",
      "[Iter.    4]  loss:2.567186  pct:0.000213605\n",
      "[Iter.    6]  loss:2.567191  pct:0.000176456\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.567191\n",
      "Best loss: 2.567191 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:49<00:00, 58.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:2386.363525  pct:100.000000000\n",
      "[Iter.   10]  loss:386.495789  pct:-83.803982692\n",
      "[Iter.   20]  loss:366.178406  pct:-5.256818680\n",
      "[Iter.   30]  loss:351.117279  pct:-4.113057043\n",
      "[Iter.   40]  loss:337.260742  pct:-3.946412692\n",
      "[Iter.   50]  loss:324.446228  pct:-3.799586657\n",
      "[Iter.   60]  loss:312.551788  pct:-3.666074274\n",
      "[Iter.   70]  loss:301.473297  pct:-3.544529779\n",
      "[Iter.   80]  loss:291.122131  pct:-3.433526574\n",
      "[Iter.   90]  loss:281.423370  pct:-3.331509336\n",
      "[Iter.  100]  loss:272.314636  pct:-3.236665853\n",
      "[Iter.  110]  loss:263.742249  pct:-3.147971704\n",
      "[Iter.  120]  loss:255.658707  pct:-3.064940075\n",
      "[Iter.  130]  loss:248.019562  pct:-2.988024541\n",
      "[Iter.  140]  loss:240.785797  pct:-2.916610527\n",
      "[Iter.  150]  loss:233.922134  pct:-2.850526402\n",
      "[Iter.  160]  loss:227.397842  pct:-2.789087065\n",
      "[Iter.  170]  loss:221.184601  pct:-2.732322133\n",
      "[Iter.  180]  loss:215.258224  pct:-2.679380174\n",
      "[Iter.  190]  loss:209.597153  pct:-2.629898017\n",
      "[Iter.  200]  loss:204.181961  pct:-2.583618900\n",
      "[Iter.  210]  loss:198.995071  pct:-2.540327080\n",
      "[Iter.  220]  loss:194.020844  pct:-2.499673922\n",
      "[Iter.  230]  loss:189.245132  pct:-2.461442273\n",
      "[Iter.  240]  loss:184.655411  pct:-2.425278590\n",
      "[Iter.  250]  loss:180.239914  pct:-2.391209014\n",
      "[Iter.  260]  loss:175.987991  pct:-2.359034974\n",
      "[Iter.  270]  loss:171.890182  pct:-2.328459349\n",
      "[Iter.  280]  loss:167.937683  pct:-2.299432889\n",
      "[Iter.  290]  loss:164.122314  pct:-2.271895492\n",
      "[Iter.  300]  loss:160.436310  pct:-2.245888776\n",
      "[Iter.  310]  loss:156.873047  pct:-2.220982858\n",
      "[Iter.  320]  loss:153.426071  pct:-2.197302709\n",
      "[Iter.  330]  loss:150.089676  pct:-2.174594734\n",
      "[Iter.  340]  loss:146.858154  pct:-2.153060553\n",
      "[Iter.  350]  loss:143.726517  pct:-2.132423350\n",
      "[Iter.  360]  loss:140.690002  pct:-2.112702897\n",
      "[Iter.  370]  loss:137.744202  pct:-2.093823818\n",
      "[Iter.  380]  loss:134.885040  pct:-2.075703618\n",
      "[Iter.  390]  loss:132.108490  pct:-2.058456807\n",
      "[Iter.  400]  loss:129.410950  pct:-2.041912888\n",
      "[Iter.  410]  loss:126.789238  pct:-2.025880914\n",
      "[Iter.  420]  loss:124.240135  pct:-2.010504065\n",
      "[Iter.  430]  loss:121.760468  pct:-1.995866843\n",
      "[Iter.  440]  loss:119.347488  pct:-1.981742658\n",
      "[Iter.  450]  loss:116.998611  pct:-1.968099190\n",
      "[Iter.  460]  loss:114.711281  pct:-1.955006644\n",
      "[Iter.  470]  loss:112.483101  pct:-1.942424420\n",
      "[Iter.  480]  loss:110.311935  pct:-1.930214805\n",
      "[Iter.  490]  loss:108.195549  pct:-1.918547078\n",
      "[Iter.  500]  loss:106.132004  pct:-1.907236708\n",
      "[Iter.  510]  loss:104.119453  pct:-1.896270948\n",
      "[Iter.  520]  loss:102.156151  pct:-1.885625162\n",
      "[Iter.  530]  loss:100.240257  pct:-1.875455897\n",
      "[Iter.  540]  loss:98.370209  pct:-1.865566364\n",
      "[Iter.  550]  loss:96.544395  pct:-1.856063250\n",
      "[Iter.  560]  loss:94.761436  pct:-1.846776269\n",
      "[Iter.  570]  loss:93.019882  pct:-1.837830161\n",
      "[Iter.  580]  loss:91.318283  pct:-1.829285397\n",
      "[Iter.  590]  loss:89.655563  pct:-1.820796089\n",
      "[Iter.  600]  loss:88.030312  pct:-1.812772916\n",
      "[Iter.  610]  loss:86.441559  pct:-1.804779193\n",
      "[Iter.  620]  loss:84.888130  pct:-1.797085419\n",
      "[Iter.  630]  loss:83.369064  pct:-1.789491480\n",
      "[Iter.  640]  loss:81.883125  pct:-1.782362604\n",
      "[Iter.  650]  loss:80.429466  pct:-1.775285264\n",
      "[Iter.  660]  loss:79.007118  pct:-1.768441454\n",
      "[Iter.  670]  loss:77.615166  pct:-1.761806462\n",
      "[Iter.  680]  loss:76.252708  pct:-1.755400846\n",
      "[Iter.  690]  loss:74.919044  pct:-1.749005337\n",
      "[Iter.  700]  loss:73.613289  pct:-1.742888773\n",
      "[Iter.  710]  loss:72.334732  pct:-1.736855999\n",
      "[Iter.  720]  loss:71.082535  pct:-1.731114819\n",
      "[Iter.  730]  loss:69.856049  pct:-1.725439603\n",
      "[Iter.  740]  loss:68.654579  pct:-1.719921819\n",
      "[Iter.  750]  loss:67.477585  pct:-1.714371187\n",
      "[Iter.  760]  loss:66.324318  pct:-1.709111121\n",
      "[Iter.  770]  loss:65.194153  pct:-1.703998074\n",
      "[Iter.  780]  loss:64.086563  pct:-1.698909601\n",
      "[Iter.  790]  loss:63.000961  pct:-1.693961657\n",
      "[Iter.  800]  loss:61.936787  pct:-1.689140340\n",
      "[Iter.  810]  loss:60.893551  pct:-1.684355672\n",
      "[Iter.  820]  loss:59.870632  pct:-1.679847351\n",
      "[Iter.  830]  loss:58.867641  pct:-1.675263291\n",
      "[Iter.  840]  loss:57.884159  pct:-1.670667172\n",
      "[Iter.  850]  loss:56.919544  pct:-1.666457427\n",
      "[Iter.  860]  loss:55.973495  pct:-1.662080661\n",
      "[Iter.  870]  loss:55.045547  pct:-1.657834641\n",
      "[Iter.  880]  loss:54.135311  pct:-1.653605787\n",
      "[Iter.  890]  loss:53.242271  pct:-1.649643615\n",
      "[Iter.  900]  loss:52.366135  pct:-1.645566119\n",
      "[Iter.  910]  loss:51.506374  pct:-1.641824989\n",
      "[Iter.  920]  loss:50.662766  pct:-1.637872723\n",
      "[Iter.  930]  loss:49.834881  pct:-1.634108730\n",
      "[Iter.  940]  loss:49.022419  pct:-1.630307607\n",
      "[Iter.  950]  loss:48.224968  pct:-1.626706792\n",
      "[Iter.  960]  loss:47.442326  pct:-1.622898672\n",
      "[Iter.  970]  loss:46.674076  pct:-1.619333585\n",
      "[Iter.  980]  loss:45.919914  pct:-1.615804528\n",
      "[Iter.  990]  loss:45.179485  pct:-1.612435338\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.662982  pct:100.000000000\n",
      "[Iter.    2]  loss:2.662575  pct:-0.015291838\n",
      "[Iter.    4]  loss:2.662444  pct:-0.004915985\n",
      "[Iter.    6]  loss:2.662405  pct:-0.001450690\n",
      "[Iter.    8]  loss:2.662385  pct:-0.000761176\n",
      "[Iter.   10]  loss:2.662347  pct:-0.001423857\n",
      "[Iter.   12]  loss:2.662333  pct:-0.000555223\n",
      "[Iter.   14]  loss:2.662307  pct:-0.000958212\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.662307\n",
      "Best loss: 2.662307 (trial 0)\n",
      "iteration 0\n",
      "Switched off factor 1\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 27%|██▋       | 2706/10000 [00:50<02:15, 53.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:442.168396  pct:100.000000000\n",
      "[Iter.   10]  loss:82.078392  pct:-81.437298459\n",
      "[Iter.   20]  loss:70.806458  pct:-13.733132717\n",
      "[Iter.   30]  loss:62.472488  pct:-11.770069296\n",
      "[Iter.   40]  loss:55.897194  pct:-10.525104190\n",
      "[Iter.   50]  loss:50.510483  pct:-9.636818495\n",
      "[Iter.   60]  loss:45.978523  pct:-8.972314822\n",
      "[Iter.   70]  loss:42.092480  pct:-8.451866814\n",
      "[Iter.   80]  loss:38.716183  pct:-8.021140642\n",
      "[Iter.   90]  loss:35.751335  pct:-7.657902606\n",
      "[Iter.  100]  loss:33.123528  pct:-7.350236310\n",
      "[Iter.  110]  loss:30.776854  pct:-7.084613689\n",
      "[Iter.  120]  loss:28.667755  pct:-6.852872176\n",
      "[Iter.  130]  loss:26.761515  pct:-6.649423559\n",
      "[Iter.  140]  loss:25.030531  pct:-6.468182971\n",
      "[Iter.  150]  loss:23.452326  pct:-6.305120387\n",
      "[Iter.  160]  loss:22.008421  pct:-6.156766232\n",
      "[Iter.  170]  loss:20.683474  pct:-6.020183640\n",
      "[Iter.  180]  loss:19.464546  pct:-5.893243117\n",
      "[Iter.  190]  loss:18.340378  pct:-5.775466760\n",
      "[Iter.  200]  loss:17.301584  pct:-5.663970365\n",
      "[Iter.  210]  loss:16.340586  pct:-5.554396185\n",
      "[Iter.  220]  loss:15.449978  pct:-5.450280974\n",
      "[Iter.  230]  loss:14.623384  pct:-5.350132922\n",
      "[Iter.  240]  loss:13.855320  pct:-5.252297077\n",
      "[Iter.  250]  loss:13.140905  pct:-5.156247548\n",
      "[Iter.  260]  loss:12.475759  pct:-5.061651450\n",
      "[Iter.  270]  loss:11.855897  pct:-4.968528368\n",
      "[Iter.  280]  loss:11.277798  pct:-4.876048208\n",
      "[Iter.  290]  loss:10.738292  pct:-4.783788227\n",
      "[Iter.  300]  loss:10.234490  pct:-4.691633995\n",
      "[Iter.  310]  loss:9.763783  pct:-4.599222057\n",
      "[Iter.  320]  loss:9.323315  pct:-4.511251096\n",
      "[Iter.  330]  loss:8.911628  pct:-4.415670950\n",
      "[Iter.  340]  loss:8.526689  pct:-4.319516071\n",
      "[Iter.  350]  loss:8.166496  pct:-4.224292886\n",
      "[Iter.  360]  loss:7.829290  pct:-4.129143665\n",
      "[Iter.  370]  loss:7.513521  pct:-4.033171874\n",
      "[Iter.  380]  loss:7.217415  pct:-3.940979613\n",
      "[Iter.  390]  loss:6.940289  pct:-3.839682780\n",
      "[Iter.  400]  loss:6.680684  pct:-3.740549278\n",
      "[Iter.  410]  loss:6.437396  pct:-3.641663592\n",
      "[Iter.  420]  loss:6.209374  pct:-3.542148048\n",
      "[Iter.  430]  loss:5.995629  pct:-3.442297386\n",
      "[Iter.  440]  loss:5.795244  pct:-3.342178484\n",
      "[Iter.  450]  loss:5.607368  pct:-3.241895262\n",
      "[Iter.  460]  loss:5.431186  pct:-3.141977757\n",
      "[Iter.  470]  loss:5.265996  pct:-3.041521514\n",
      "[Iter.  480]  loss:5.111086  pct:-2.941696602\n",
      "[Iter.  490]  loss:4.965791  pct:-2.842735758\n",
      "[Iter.  500]  loss:4.829513  pct:-2.744339145\n",
      "[Iter.  510]  loss:4.701711  pct:-2.646278622\n",
      "[Iter.  520]  loss:4.581828  pct:-2.549765208\n",
      "[Iter.  530]  loss:4.469398  pct:-2.453815725\n",
      "[Iter.  540]  loss:4.363955  pct:-2.359231956\n",
      "[Iter.  550]  loss:4.265052  pct:-2.266354760\n",
      "[Iter.  560]  loss:4.172272  pct:-2.175368158\n",
      "[Iter.  570]  loss:4.085259  pct:-2.085489551\n",
      "[Iter.  580]  loss:4.003634  pct:-1.998036746\n",
      "[Iter.  590]  loss:3.927062  pct:-1.912572667\n",
      "[Iter.  600]  loss:3.855232  pct:-1.829109702\n",
      "[Iter.  610]  loss:3.787828  pct:-1.748372017\n",
      "[Iter.  620]  loss:3.724627  pct:-1.668514893\n",
      "[Iter.  630]  loss:3.665316  pct:-1.592424124\n",
      "[Iter.  640]  loss:3.609683  pct:-1.517805244\n",
      "[Iter.  650]  loss:3.557515  pct:-1.445220771\n",
      "[Iter.  660]  loss:3.508570  pct:-1.375831772\n",
      "[Iter.  670]  loss:3.462680  pct:-1.307949730\n",
      "[Iter.  680]  loss:3.419654  pct:-1.242562845\n",
      "[Iter.  690]  loss:3.379299  pct:-1.180089307\n",
      "[Iter.  700]  loss:3.341436  pct:-1.120440057\n",
      "[Iter.  710]  loss:3.305910  pct:-1.063175397\n",
      "[Iter.  720]  loss:3.272589  pct:-1.007919159\n",
      "[Iter.  730]  loss:3.241333  pct:-0.955098029\n",
      "[Iter.  740]  loss:3.212020  pct:-0.904345495\n",
      "[Iter.  750]  loss:3.184534  pct:-0.855718399\n",
      "[Iter.  760]  loss:3.158753  pct:-0.809581262\n",
      "[Iter.  770]  loss:3.134574  pct:-0.765452189\n",
      "[Iter.  780]  loss:3.111883  pct:-0.723886938\n",
      "[Iter.  790]  loss:3.090602  pct:-0.683870172\n",
      "[Iter.  800]  loss:3.070646  pct:-0.645710748\n",
      "[Iter.  810]  loss:3.051948  pct:-0.608911004\n",
      "[Iter.  820]  loss:3.034398  pct:-0.575057854\n",
      "[Iter.  830]  loss:3.017934  pct:-0.542562940\n",
      "[Iter.  840]  loss:3.002479  pct:-0.512121297\n",
      "[Iter.  850]  loss:2.987979  pct:-0.482923114\n",
      "[Iter.  860]  loss:2.974381  pct:-0.455097003\n",
      "[Iter.  870]  loss:2.961625  pct:-0.428858003\n",
      "[Iter.  880]  loss:2.949651  pct:-0.404292206\n",
      "[Iter.  890]  loss:2.938425  pct:-0.380617593\n",
      "[Iter.  900]  loss:2.927893  pct:-0.358395689\n",
      "[Iter.  910]  loss:2.918014  pct:-0.337429945\n",
      "[Iter.  920]  loss:2.908754  pct:-0.317320717\n",
      "[Iter.  930]  loss:2.900055  pct:-0.299060671\n",
      "[Iter.  940]  loss:2.891885  pct:-0.281739607\n",
      "[Iter.  950]  loss:2.884225  pct:-0.264859626\n",
      "[Iter.  960]  loss:2.877058  pct:-0.248493096\n",
      "[Iter.  970]  loss:2.870328  pct:-0.233938831\n",
      "[Iter.  980]  loss:2.864014  pct:-0.219951330\n",
      "[Iter.  990]  loss:2.858120  pct:-0.205793159\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.760040  pct:100.000000000\n",
      "[Iter.    2]  loss:2.760053  pct:0.000466464\n",
      "[Iter.    4]  loss:2.760057  pct:0.000164126\n",
      "[Iter.    6]  loss:2.760061  pct:0.000138211\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.760061\n",
      "Best loss: 2.760061 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 271/10000 [00:04<02:35, 62.55it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:4775.207031  pct:100.000000000\n",
      "[Iter.   10]  loss:707.814575  pct:-85.177303340\n",
      "[Iter.   20]  loss:687.946655  pct:-2.806938514\n",
      "[Iter.   30]  loss:669.416382  pct:-2.693562545\n",
      "[Iter.   40]  loss:652.562683  pct:-2.517670494\n",
      "[Iter.   50]  loss:636.876648  pct:-2.403759143\n",
      "[Iter.   60]  loss:622.063232  pct:-2.325947352\n",
      "[Iter.   70]  loss:607.986816  pct:-2.262859350\n",
      "[Iter.   80]  loss:594.571045  pct:-2.206589209\n",
      "[Iter.   90]  loss:581.761169  pct:-2.154473481\n",
      "[Iter.  100]  loss:569.511292  pct:-2.105654102\n",
      "[Iter.  110]  loss:557.781982  pct:-2.059539338\n",
      "[Iter.  120]  loss:546.537964  pct:-2.015844704\n",
      "[Iter.  130]  loss:535.747803  pct:-1.974274771\n",
      "[Iter.  140]  loss:525.382874  pct:-1.934665741\n",
      "[Iter.  150]  loss:515.415283  pct:-1.897205035\n",
      "[Iter.  160]  loss:505.819427  pct:-1.861771668\n",
      "[Iter.  170]  loss:496.571838  pct:-1.828239211\n",
      "[Iter.  180]  loss:487.650421  pct:-1.796601528\n",
      "[Iter.  190]  loss:479.035797  pct:-1.766557282\n",
      "[Iter.  200]  loss:470.709473  pct:-1.738142434\n",
      "[Iter.  210]  loss:462.655457  pct:-1.711037610\n",
      "[Iter.  220]  loss:454.859161  pct:-1.685119035\n",
      "[Iter.  230]  loss:447.307587  pct:-1.660200640\n",
      "[Iter.  240]  loss:439.988159  pct:-1.636329834\n",
      "[Iter.  250]  loss:432.889404  pct:-1.613396801\n",
      "[Iter.  260]  loss:426.000488  pct:-1.591380142\n",
      "[Iter.  270]  loss:419.310944  pct:-1.570313852\n",
      "[Iter.  280]  loss:412.811066  pct:-1.550133148\n",
      "[Iter.  290]  loss:406.492249  pct:-1.530680174\n",
      "[Iter.  300]  loss:400.345703  pct:-1.512094125\n",
      "[Iter.  310]  loss:394.364197  pct:-1.494085312\n",
      "[Iter.  320]  loss:388.540405  pct:-1.476754622\n",
      "[Iter.  330]  loss:382.867279  pct:-1.460112293\n",
      "[Iter.  340]  loss:377.338074  pct:-1.444157186\n",
      "[Iter.  350]  loss:371.946747  pct:-1.428778933\n",
      "[Iter.  360]  loss:366.687469  pct:-1.413986650\n",
      "[Iter.  370]  loss:361.554779  pct:-1.399745248\n",
      "[Iter.  380]  loss:356.543488  pct:-1.386039348\n",
      "[Iter.  390]  loss:351.649200  pct:-1.372704111\n",
      "[Iter.  400]  loss:346.866943  pct:-1.359951075\n",
      "[Iter.  410]  loss:342.192780  pct:-1.347537985\n",
      "[Iter.  420]  loss:337.622650  pct:-1.335542322\n",
      "[Iter.  430]  loss:333.152313  pct:-1.324063096\n",
      "[Iter.  440]  loss:328.778503  pct:-1.312855904\n",
      "[Iter.  450]  loss:324.497620  pct:-1.302057082\n",
      "[Iter.  460]  loss:320.306641  pct:-1.291528427\n",
      "[Iter.  470]  loss:316.202118  pct:-1.281435407\n",
      "[Iter.  480]  loss:312.181183  pct:-1.271634449\n",
      "[Iter.  490]  loss:308.240997  pct:-1.262147036\n",
      "[Iter.  500]  loss:304.378784  pct:-1.252984894\n",
      "[Iter.  510]  loss:300.592102  pct:-1.244069011\n",
      "[Iter.  520]  loss:296.878510  pct:-1.235425849\n",
      "[Iter.  530]  loss:293.235809  pct:-1.227000298\n",
      "[Iter.  540]  loss:289.661743  pct:-1.218836871\n",
      "[Iter.  550]  loss:286.154419  pct:-1.210834465\n",
      "[Iter.  560]  loss:282.711517  pct:-1.203162133\n",
      "[Iter.  570]  loss:279.331238  pct:-1.195663895\n",
      "[Iter.  580]  loss:276.011627  pct:-1.188413663\n",
      "[Iter.  590]  loss:272.751068  pct:-1.181312220\n",
      "[Iter.  600]  loss:269.547913  pct:-1.174387891\n",
      "[Iter.  610]  loss:266.400513  pct:-1.167658793\n",
      "[Iter.  620]  loss:263.307251  pct:-1.161132044\n",
      "[Iter.  630]  loss:260.266602  pct:-1.154791371\n",
      "[Iter.  640]  loss:257.277130  pct:-1.148618923\n",
      "[Iter.  650]  loss:254.337616  pct:-1.142547788\n",
      "[Iter.  660]  loss:251.446320  pct:-1.136794640\n",
      "[Iter.  670]  loss:248.602402  pct:-1.131023851\n",
      "[Iter.  680]  loss:245.804581  pct:-1.125419958\n",
      "[Iter.  690]  loss:243.051514  pct:-1.120022666\n",
      "[Iter.  700]  loss:240.342041  pct:-1.114773002\n",
      "[Iter.  710]  loss:237.675613  pct:-1.109430377\n",
      "[Iter.  720]  loss:235.050674  pct:-1.104420823\n",
      "[Iter.  730]  loss:232.466553  pct:-1.099389189\n",
      "[Iter.  740]  loss:229.921982  pct:-1.094596574\n",
      "[Iter.  750]  loss:227.416061  pct:-1.089900318\n",
      "[Iter.  760]  loss:224.947952  pct:-1.085283562\n",
      "[Iter.  770]  loss:222.516754  pct:-1.080782508\n",
      "[Iter.  780]  loss:220.121674  pct:-1.076359655\n",
      "[Iter.  790]  loss:217.761871  pct:-1.072044478\n",
      "[Iter.  800]  loss:215.436386  pct:-1.067902850\n",
      "[Iter.  810]  loss:213.144714  pct:-1.063734773\n",
      "[Iter.  820]  loss:210.885788  pct:-1.059808778\n",
      "[Iter.  830]  loss:208.658936  pct:-1.055951868\n",
      "[Iter.  840]  loss:206.463593  pct:-1.052120299\n",
      "[Iter.  850]  loss:204.299194  pct:-1.048319545\n",
      "[Iter.  860]  loss:202.165039  pct:-1.044622462\n",
      "[Iter.  870]  loss:200.060349  pct:-1.041075431\n",
      "[Iter.  880]  loss:197.984680  pct:-1.037521103\n",
      "[Iter.  890]  loss:195.937393  pct:-1.034063335\n",
      "[Iter.  900]  loss:193.917847  pct:-1.030710104\n",
      "[Iter.  910]  loss:191.925720  pct:-1.027304345\n",
      "[Iter.  920]  loss:189.960327  pct:-1.024038396\n",
      "[Iter.  930]  loss:188.021301  pct:-1.020753074\n",
      "[Iter.  940]  loss:186.107834  pct:-1.017686504\n",
      "[Iter.  950]  loss:184.219559  pct:-1.014613468\n",
      "[Iter.  960]  loss:182.355988  pct:-1.011603317\n",
      "[Iter.  970]  loss:180.516602  pct:-1.008678690\n",
      "[Iter.  980]  loss:178.701004  pct:-1.005778703\n",
      "[Iter.  990]  loss:176.908585  pct:-1.003027064\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.802433  pct:100.000000000\n",
      "[Iter.    2]  loss:2.801023  pct:-0.050313695\n",
      "[Iter.    4]  loss:2.800502  pct:-0.018581347\n",
      "[Iter.    6]  loss:2.800352  pct:-0.005380483\n",
      "[Iter.    8]  loss:2.800233  pct:-0.004248426\n",
      "[Iter.   10]  loss:2.800177  pct:-0.001992333\n",
      "[Iter.   12]  loss:2.800137  pct:-0.001430421\n",
      "[Iter.   14]  loss:2.800112  pct:-0.000876997\n",
      "[Iter.   16]  loss:2.800100  pct:-0.000434245\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.800100\n",
      "Best loss: 2.800100 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▋        | 1635/10000 [00:35<03:02, 45.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:334.565399  pct:100.000000000\n",
      "[Iter.   10]  loss:71.367416  pct:-78.668624640\n",
      "[Iter.   20]  loss:58.629631  pct:-17.848180564\n",
      "[Iter.   30]  loss:50.856689  pct:-13.257701696\n",
      "[Iter.   40]  loss:44.882645  pct:-11.746822029\n",
      "[Iter.   50]  loss:40.085300  pct:-10.688639773\n",
      "[Iter.   60]  loss:36.121494  pct:-9.888428198\n",
      "[Iter.   70]  loss:32.776108  pct:-9.261484251\n",
      "[Iter.   80]  loss:29.905550  pct:-8.758080134\n",
      "[Iter.   90]  loss:27.409952  pct:-8.344932092\n",
      "[Iter.  100]  loss:25.218075  pct:-7.996647904\n",
      "[Iter.  110]  loss:23.277100  pct:-7.696761964\n",
      "[Iter.  120]  loss:21.546446  pct:-7.435006044\n",
      "[Iter.  130]  loss:19.994530  pct:-7.202654830\n",
      "[Iter.  140]  loss:18.596306  pct:-6.993032075\n",
      "[Iter.  150]  loss:17.331425  pct:-6.801787110\n",
      "[Iter.  160]  loss:16.183201  pct:-6.625098028\n",
      "[Iter.  170]  loss:15.137839  pct:-6.459547338\n",
      "[Iter.  180]  loss:14.183605  pct:-6.303634906\n",
      "[Iter.  190]  loss:13.310579  pct:-6.155176221\n",
      "[Iter.  200]  loss:12.509979  pct:-6.014764901\n",
      "[Iter.  210]  loss:11.775010  pct:-5.875062816\n",
      "[Iter.  220]  loss:11.099620  pct:-5.735793322\n",
      "[Iter.  230]  loss:10.478350  pct:-5.597220331\n",
      "[Iter.  240]  loss:9.906083  pct:-5.461418981\n",
      "[Iter.  250]  loss:9.378479  pct:-5.326061748\n",
      "[Iter.  260]  loss:8.891718  pct:-5.190192279\n",
      "[Iter.  270]  loss:8.441824  pct:-5.059696629\n",
      "[Iter.  280]  loss:8.025411  pct:-4.932740948\n",
      "[Iter.  290]  loss:7.640032  pct:-4.801982781\n",
      "[Iter.  300]  loss:7.282587  pct:-4.678576894\n",
      "[Iter.  310]  loss:6.950846  pct:-4.555267663\n",
      "[Iter.  320]  loss:6.641892  pct:-4.444830136\n",
      "[Iter.  330]  loss:6.355662  pct:-4.309473045\n",
      "[Iter.  340]  loss:6.089751  pct:-4.183845959\n",
      "[Iter.  350]  loss:5.844083  pct:-4.034113496\n",
      "[Iter.  360]  loss:5.616595  pct:-3.892629617\n",
      "[Iter.  370]  loss:5.406088  pct:-3.747945577\n",
      "[Iter.  380]  loss:5.212342  pct:-3.583850178\n",
      "[Iter.  390]  loss:5.032998  pct:-3.440760116\n",
      "[Iter.  400]  loss:4.867359  pct:-3.291049583\n",
      "[Iter.  410]  loss:4.714236  pct:-3.145913355\n",
      "[Iter.  420]  loss:4.571989  pct:-3.017396502\n",
      "[Iter.  430]  loss:4.436653  pct:-2.960110370\n",
      "[Iter.  440]  loss:4.312943  pct:-2.788366628\n",
      "[Iter.  450]  loss:4.199284  pct:-2.635298113\n",
      "[Iter.  460]  loss:4.093848  pct:-2.510816681\n",
      "[Iter.  470]  loss:3.996401  pct:-2.380320080\n",
      "[Iter.  480]  loss:3.905975  pct:-2.262696946\n",
      "[Iter.  490]  loss:3.822202  pct:-2.144719064\n",
      "[Iter.  500]  loss:3.743947  pct:-2.047396875\n",
      "[Iter.  510]  loss:3.671627  pct:-1.931644600\n",
      "[Iter.  520]  loss:3.604856  pct:-1.818580876\n",
      "[Iter.  530]  loss:3.542944  pct:-1.717443151\n",
      "[Iter.  540]  loss:3.485516  pct:-1.620921947\n",
      "[Iter.  550]  loss:3.432211  pct:-1.529312629\n",
      "[Iter.  560]  loss:3.382568  pct:-1.446407263\n",
      "[Iter.  570]  loss:3.336632  pct:-1.357996611\n",
      "[Iter.  580]  loss:3.294103  pct:-1.274633078\n",
      "[Iter.  590]  loss:3.254653  pct:-1.197585374\n",
      "[Iter.  600]  loss:3.217973  pct:-1.127001355\n",
      "[Iter.  610]  loss:3.183900  pct:-1.058845207\n",
      "[Iter.  620]  loss:3.152415  pct:-0.988876719\n",
      "[Iter.  630]  loss:3.123237  pct:-0.925557998\n",
      "[Iter.  640]  loss:3.096171  pct:-0.866608187\n",
      "[Iter.  650]  loss:3.071068  pct:-0.810793974\n",
      "[Iter.  660]  loss:3.047783  pct:-0.758179243\n",
      "[Iter.  670]  loss:3.026181  pct:-0.708798099\n",
      "[Iter.  680]  loss:3.006148  pct:-0.661969907\n",
      "[Iter.  690]  loss:2.987566  pct:-0.618128752\n",
      "[Iter.  700]  loss:2.970330  pct:-0.576932193\n",
      "[Iter.  710]  loss:2.954339  pct:-0.538372776\n",
      "[Iter.  720]  loss:2.939505  pct:-0.502098432\n",
      "[Iter.  730]  loss:2.925740  pct:-0.468271283\n",
      "[Iter.  740]  loss:2.912970  pct:-0.436484899\n",
      "[Iter.  750]  loss:2.901127  pct:-0.406551676\n",
      "[Iter.  760]  loss:2.890135  pct:-0.378880738\n",
      "[Iter.  770]  loss:2.879938  pct:-0.352842980\n",
      "[Iter.  780]  loss:2.870470  pct:-0.328743290\n",
      "[Iter.  790]  loss:2.861676  pct:-0.306346776\n",
      "[Iter.  800]  loss:2.853519  pct:-0.285068219\n",
      "[Iter.  810]  loss:2.845945  pct:-0.265421178\n",
      "[Iter.  820]  loss:2.838919  pct:-0.246867701\n",
      "[Iter.  830]  loss:2.832394  pct:-0.229859187\n",
      "[Iter.  840]  loss:2.826340  pct:-0.213721978\n",
      "[Iter.  850]  loss:2.820715  pct:-0.199020965\n",
      "[Iter.  860]  loss:2.815487  pct:-0.185336123\n",
      "[Iter.  870]  loss:2.810630  pct:-0.172512344\n",
      "[Iter.  880]  loss:2.806121  pct:-0.160451106\n",
      "[Iter.  890]  loss:2.801930  pct:-0.149340813\n",
      "[Iter.  900]  loss:2.798036  pct:-0.138970363\n",
      "[Iter.  910]  loss:2.794416  pct:-0.129390258\n",
      "[Iter.  920]  loss:2.791046  pct:-0.120590798\n",
      "[Iter.  930]  loss:2.787912  pct:-0.112279551\n",
      "[Iter.  940]  loss:2.784997  pct:-0.104546592\n",
      "[Iter.  950]  loss:2.782286  pct:-0.097345068\n",
      "[Iter.  960]  loss:2.779764  pct:-0.090644577\n",
      "[Iter.  970]  loss:2.777417  pct:-0.084439922\n",
      "[Iter.  980]  loss:2.775234  pct:-0.078605366\n",
      "[Iter.  990]  loss:2.773201  pct:-0.073246321\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.743420  pct:100.000000000\n",
      "[Iter.    2]  loss:2.743425  pct:0.000156430\n",
      "[Iter.    4]  loss:2.743424  pct:-0.000008691\n",
      "[Iter.    6]  loss:2.743424  pct:-0.000017381\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.743424\n",
      "Best loss: 2.743424 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 14%|█▍        | 1389/10000 [00:22<02:19, 61.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:265.131348  pct:100.000000000\n",
      "[Iter.   10]  loss:56.414074  pct:-78.722215137\n",
      "[Iter.   20]  loss:46.633278  pct:-17.337510602\n",
      "[Iter.   30]  loss:39.865490  pct:-14.512786231\n",
      "[Iter.   40]  loss:34.781956  pct:-12.751716449\n",
      "[Iter.   50]  loss:30.771225  pct:-11.531067361\n",
      "[Iter.   60]  loss:27.493441  pct:-10.652108748\n",
      "[Iter.   70]  loss:24.750586  pct:-9.976398040\n",
      "[Iter.   80]  loss:22.415449  pct:-9.434671387\n",
      "[Iter.   90]  loss:20.401159  pct:-8.986167724\n",
      "[Iter.  100]  loss:18.645702  pct:-8.604692017\n",
      "[Iter.  110]  loss:17.103235  pct:-8.272507452\n",
      "[Iter.  120]  loss:15.738830  pct:-7.977471002\n",
      "[Iter.  130]  loss:14.525360  pct:-7.710036484\n",
      "[Iter.  140]  loss:13.441223  pct:-7.463752739\n",
      "[Iter.  150]  loss:12.468990  pct:-7.233216859\n",
      "[Iter.  160]  loss:11.594326  pct:-7.014716379\n",
      "[Iter.  170]  loss:10.805337  pct:-6.804958441\n",
      "[Iter.  180]  loss:10.091985  pct:-6.601850609\n",
      "[Iter.  190]  loss:9.445772  pct:-6.403225866\n",
      "[Iter.  200]  loss:8.859391  pct:-6.207866842\n",
      "[Iter.  210]  loss:8.326564  pct:-6.014266269\n",
      "[Iter.  220]  loss:7.841801  pct:-5.821881369\n",
      "[Iter.  230]  loss:7.400302  pct:-5.630074314\n",
      "[Iter.  240]  loss:6.997857  pct:-5.438221888\n",
      "[Iter.  250]  loss:6.630739  pct:-5.246147168\n",
      "[Iter.  260]  loss:6.295628  pct:-5.053903200\n",
      "[Iter.  270]  loss:5.989555  pct:-4.861670806\n",
      "[Iter.  280]  loss:5.709862  pct:-4.669688929\n",
      "[Iter.  290]  loss:5.454176  pct:-4.477960070\n",
      "[Iter.  300]  loss:5.220358  pct:-4.286963105\n",
      "[Iter.  310]  loss:5.006484  pct:-4.096928685\n",
      "[Iter.  320]  loss:4.810801  pct:-3.908582211\n",
      "[Iter.  330]  loss:4.631732  pct:-3.722229232\n",
      "[Iter.  340]  loss:4.467835  pct:-3.538558818\n",
      "[Iter.  350]  loss:4.317816  pct:-3.357759509\n",
      "[Iter.  360]  loss:4.180469  pct:-3.180952825\n",
      "[Iter.  370]  loss:4.054720  pct:-3.007991944\n",
      "[Iter.  380]  loss:3.939593  pct:-2.839346541\n",
      "[Iter.  390]  loss:3.834185  pct:-2.675593158\n",
      "[Iter.  400]  loss:3.737654  pct:-2.517638173\n",
      "[Iter.  410]  loss:3.649264  pct:-2.364855103\n",
      "[Iter.  420]  loss:3.568317  pct:-2.218177509\n",
      "[Iter.  430]  loss:3.494187  pct:-2.077444808\n",
      "[Iter.  440]  loss:3.426302  pct:-1.942816180\n",
      "[Iter.  450]  loss:3.364134  pct:-1.814417153\n",
      "[Iter.  460]  loss:3.307207  pct:-1.692179885\n",
      "[Iter.  470]  loss:3.255071  pct:-1.576441423\n",
      "[Iter.  480]  loss:3.207329  pct:-1.466707420\n",
      "[Iter.  490]  loss:3.163605  pct:-1.363239919\n",
      "[Iter.  500]  loss:3.123570  pct:-1.265486990\n",
      "[Iter.  510]  loss:3.086904  pct:-1.173846491\n",
      "[Iter.  520]  loss:3.053335  pct:-1.087475847\n",
      "[Iter.  530]  loss:3.022592  pct:-1.006862548\n",
      "[Iter.  540]  loss:2.994417  pct:-0.932127192\n",
      "[Iter.  550]  loss:2.968610  pct:-0.861857764\n",
      "[Iter.  560]  loss:2.944976  pct:-0.796136806\n",
      "[Iter.  570]  loss:2.923332  pct:-0.734934391\n",
      "[Iter.  580]  loss:2.903501  pct:-0.678375933\n",
      "[Iter.  590]  loss:2.885335  pct:-0.625660763\n",
      "[Iter.  600]  loss:2.868697  pct:-0.576625080\n",
      "[Iter.  610]  loss:2.853451  pct:-0.531457786\n",
      "[Iter.  620]  loss:2.839479  pct:-0.489662559\n",
      "[Iter.  630]  loss:2.826676  pct:-0.450895317\n",
      "[Iter.  640]  loss:2.814943  pct:-0.415091577\n",
      "[Iter.  650]  loss:2.804188  pct:-0.382053448\n",
      "[Iter.  660]  loss:2.794332  pct:-0.351465279\n",
      "[Iter.  670]  loss:2.785300  pct:-0.323251691\n",
      "[Iter.  680]  loss:2.777018  pct:-0.297344902\n",
      "[Iter.  690]  loss:2.769428  pct:-0.273282453\n",
      "[Iter.  700]  loss:2.762473  pct:-0.251157365\n",
      "[Iter.  710]  loss:2.756102  pct:-0.230627469\n",
      "[Iter.  720]  loss:2.750252  pct:-0.212241788\n",
      "[Iter.  730]  loss:2.744886  pct:-0.195103869\n",
      "[Iter.  740]  loss:2.739966  pct:-0.179251288\n",
      "[Iter.  750]  loss:2.735447  pct:-0.164937226\n",
      "[Iter.  760]  loss:2.731306  pct:-0.151368883\n",
      "[Iter.  770]  loss:2.727493  pct:-0.139630754\n",
      "[Iter.  780]  loss:2.723984  pct:-0.128619634\n",
      "[Iter.  790]  loss:2.720762  pct:-0.118282197\n",
      "[Iter.  800]  loss:2.717803  pct:-0.108765518\n",
      "[Iter.  810]  loss:2.715081  pct:-0.100146562\n",
      "[Iter.  820]  loss:2.712575  pct:-0.092299908\n",
      "[Iter.  830]  loss:2.710272  pct:-0.084914206\n",
      "[Iter.  840]  loss:2.708154  pct:-0.078133625\n",
      "[Iter.  850]  loss:2.706203  pct:-0.072040915\n",
      "[Iter.  860]  loss:2.704406  pct:-0.066427972\n",
      "[Iter.  870]  loss:2.702752  pct:-0.061156121\n",
      "[Iter.  880]  loss:2.701230  pct:-0.056306530\n",
      "[Iter.  890]  loss:2.699831  pct:-0.051801535\n",
      "[Iter.  900]  loss:2.698545  pct:-0.047616058\n",
      "[Iter.  910]  loss:2.697362  pct:-0.043857328\n",
      "[Iter.  920]  loss:2.696265  pct:-0.040668031\n",
      "[Iter.  930]  loss:2.695253  pct:-0.037536628\n",
      "[Iter.  940]  loss:2.694318  pct:-0.034675815\n",
      "[Iter.  950]  loss:2.693458  pct:-0.031918125\n",
      "[Iter.  960]  loss:2.692669  pct:-0.029299342\n",
      "[Iter.  970]  loss:2.691943  pct:-0.026952670\n",
      "[Iter.  980]  loss:2.691270  pct:-0.024993738\n",
      "[Iter.  990]  loss:2.690648  pct:-0.023121887\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.681528  pct:100.000000000\n",
      "[Iter.    2]  loss:2.681527  pct:-0.000017782\n",
      "[Iter.    4]  loss:2.681528  pct:0.000017782\n",
      "[Iter.    6]  loss:2.681528  pct:0.000008891\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.681528\n",
      "Best loss: 2.681528 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  4%|▎         | 368/10000 [00:08<03:33, 45.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:408.340393  pct:100.000000000\n",
      "[Iter.   10]  loss:65.297821  pct:-84.008973358\n",
      "[Iter.   20]  loss:56.809769  pct:-12.998982558\n",
      "[Iter.   30]  loss:50.245148  pct:-11.555443940\n",
      "[Iter.   40]  loss:44.936428  pct:-10.565636440\n",
      "[Iter.   50]  loss:40.525513  pct:-9.815901184\n",
      "[Iter.   60]  loss:36.786667  pct:-9.225906291\n",
      "[Iter.   70]  loss:33.568539  pct:-8.748083146\n",
      "[Iter.   80]  loss:30.764793  pct:-8.352300640\n",
      "[Iter.   90]  loss:28.297947  pct:-8.018407386\n",
      "[Iter.  100]  loss:26.110033  pct:-7.731705413\n",
      "[Iter.  110]  loss:24.156578  pct:-7.481625813\n",
      "[Iter.  120]  loss:22.402767  pct:-7.260179310\n",
      "[Iter.  130]  loss:20.820898  pct:-7.061043453\n",
      "[Iter.  140]  loss:19.388515  pct:-6.879542754\n",
      "[Iter.  150]  loss:18.087072  pct:-6.712443260\n",
      "[Iter.  160]  loss:16.901077  pct:-6.557142458\n",
      "[Iter.  170]  loss:15.817780  pct:-6.409631519\n",
      "[Iter.  180]  loss:14.825973  pct:-6.270209262\n",
      "[Iter.  190]  loss:13.916252  pct:-6.135991533\n",
      "[Iter.  200]  loss:13.080437  pct:-6.006038275\n",
      "[Iter.  210]  loss:12.311406  pct:-5.879242324\n",
      "[Iter.  220]  loss:11.602882  pct:-5.755018903\n",
      "[Iter.  230]  loss:10.949412  pct:-5.631962970\n",
      "[Iter.  240]  loss:10.346058  pct:-5.510382064\n",
      "[Iter.  250]  loss:9.788574  pct:-5.388368004\n",
      "[Iter.  260]  loss:9.273061  pct:-5.266481191\n",
      "[Iter.  270]  loss:8.796076  pct:-5.143770628\n",
      "[Iter.  280]  loss:8.354485  pct:-5.020321241\n",
      "[Iter.  290]  loss:7.945406  pct:-4.896515101\n",
      "[Iter.  300]  loss:7.566297  pct:-4.771428729\n",
      "[Iter.  310]  loss:7.214886  pct:-4.644417323\n",
      "[Iter.  320]  loss:6.889039  pct:-4.516323851\n",
      "[Iter.  330]  loss:6.586804  pct:-4.387174931\n",
      "[Iter.  340]  loss:6.306417  pct:-4.256797502\n",
      "[Iter.  350]  loss:6.046200  pct:-4.126228421\n",
      "[Iter.  360]  loss:5.804696  pct:-3.994313475\n",
      "[Iter.  370]  loss:5.580505  pct:-3.862229973\n",
      "[Iter.  380]  loss:5.372236  pct:-3.732083484\n",
      "[Iter.  390]  loss:5.178672  pct:-3.603042180\n",
      "[Iter.  400]  loss:4.999017  pct:-3.469143074\n",
      "[Iter.  410]  loss:4.832174  pct:-3.337515064\n",
      "[Iter.  420]  loss:4.677209  pct:-3.206930310\n",
      "[Iter.  430]  loss:4.533235  pct:-3.078219712\n",
      "[Iter.  440]  loss:4.399479  pct:-2.950557291\n",
      "[Iter.  450]  loss:4.275204  pct:-2.824760231\n",
      "[Iter.  460]  loss:4.159727  pct:-2.701089356\n",
      "[Iter.  470]  loss:4.052425  pct:-2.579548763\n",
      "[Iter.  480]  loss:3.952734  pct:-2.460031128\n",
      "[Iter.  490]  loss:3.860111  pct:-2.343276132\n",
      "[Iter.  500]  loss:3.774045  pct:-2.229606947\n",
      "[Iter.  510]  loss:3.694054  pct:-2.119505659\n",
      "[Iter.  520]  loss:3.619689  pct:-2.013110119\n",
      "[Iter.  530]  loss:3.550549  pct:-1.910114395\n",
      "[Iter.  540]  loss:3.486276  pct:-1.810217243\n",
      "[Iter.  550]  loss:3.426549  pct:-1.713188462\n",
      "[Iter.  560]  loss:3.371048  pct:-1.619740899\n",
      "[Iter.  570]  loss:3.319463  pct:-1.530227910\n",
      "[Iter.  580]  loss:3.271536  pct:-1.443843458\n",
      "[Iter.  590]  loss:3.227021  pct:-1.360680056\n",
      "[Iter.  600]  loss:3.185659  pct:-1.281718908\n",
      "[Iter.  610]  loss:3.147188  pct:-1.207622757\n",
      "[Iter.  620]  loss:3.111425  pct:-1.136363550\n",
      "[Iter.  630]  loss:3.078181  pct:-1.068461308\n",
      "[Iter.  640]  loss:3.047297  pct:-1.003297620\n",
      "[Iter.  650]  loss:3.018593  pct:-0.941970572\n",
      "[Iter.  660]  loss:2.991923  pct:-0.883499942\n",
      "[Iter.  670]  loss:2.967120  pct:-0.828995948\n",
      "[Iter.  680]  loss:2.944059  pct:-0.777227490\n",
      "[Iter.  690]  loss:2.922631  pct:-0.727850415\n",
      "[Iter.  700]  loss:2.902736  pct:-0.680724943\n",
      "[Iter.  710]  loss:2.884236  pct:-0.637324685\n",
      "[Iter.  720]  loss:2.867038  pct:-0.596270493\n",
      "[Iter.  730]  loss:2.851059  pct:-0.557319948\n",
      "[Iter.  740]  loss:2.836207  pct:-0.520947533\n",
      "[Iter.  750]  loss:2.822373  pct:-0.487755712\n",
      "[Iter.  760]  loss:2.809497  pct:-0.456204572\n",
      "[Iter.  770]  loss:2.797533  pct:-0.425852728\n",
      "[Iter.  780]  loss:2.786418  pct:-0.397299949\n",
      "[Iter.  790]  loss:2.776111  pct:-0.369929681\n",
      "[Iter.  800]  loss:2.766529  pct:-0.345134926\n",
      "[Iter.  810]  loss:2.757636  pct:-0.321450163\n",
      "[Iter.  820]  loss:2.749355  pct:-0.300310426\n",
      "[Iter.  830]  loss:2.741595  pct:-0.282258449\n",
      "[Iter.  840]  loss:2.734364  pct:-0.263742816\n",
      "[Iter.  850]  loss:2.727643  pct:-0.245798301\n",
      "[Iter.  860]  loss:2.721385  pct:-0.229420528\n",
      "[Iter.  870]  loss:2.715561  pct:-0.214020709\n",
      "[Iter.  880]  loss:2.710138  pct:-0.199694694\n",
      "[Iter.  890]  loss:2.705106  pct:-0.185649128\n",
      "[Iter.  900]  loss:2.700441  pct:-0.172483102\n",
      "[Iter.  910]  loss:2.696129  pct:-0.159652580\n",
      "[Iter.  920]  loss:2.692137  pct:-0.148058264\n",
      "[Iter.  930]  loss:2.688424  pct:-0.137933868\n",
      "[Iter.  940]  loss:2.684972  pct:-0.128404689\n",
      "[Iter.  950]  loss:2.681739  pct:-0.120400416\n",
      "[Iter.  960]  loss:2.678733  pct:-0.112090739\n",
      "[Iter.  970]  loss:2.675948  pct:-0.103992534\n",
      "[Iter.  980]  loss:2.673290  pct:-0.099298469\n",
      "[Iter.  990]  loss:2.670801  pct:-0.093136351\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.630497  pct:100.000000000\n",
      "[Iter.    2]  loss:2.630467  pct:-0.001114827\n",
      "[Iter.    4]  loss:2.630462  pct:-0.000208466\n",
      "[Iter.    6]  loss:2.630462  pct:0.000018128\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.630462\n",
      "Best loss: 2.630462 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1807/10000 [00:26<01:58, 69.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:327.295135  pct:100.000000000\n",
      "[Iter.   10]  loss:71.105797  pct:-78.274713941\n",
      "[Iter.   20]  loss:59.230728  pct:-16.700563381\n",
      "[Iter.   30]  loss:51.107666  pct:-13.714270257\n",
      "[Iter.   40]  loss:44.941006  pct:-12.066018250\n",
      "[Iter.   50]  loss:40.033958  pct:-10.918863952\n",
      "[Iter.   60]  loss:36.002045  pct:-10.071234309\n",
      "[Iter.   70]  loss:32.611858  pct:-9.416649360\n",
      "[Iter.   80]  loss:29.711678  pct:-8.893025304\n",
      "[Iter.   90]  loss:27.197243  pct:-8.462783059\n",
      "[Iter.  100]  loss:24.993977  pct:-8.101064380\n",
      "[Iter.  110]  loss:23.046926  pct:-7.790077291\n",
      "[Iter.  120]  loss:21.314386  pct:-7.517445464\n",
      "[Iter.  130]  loss:19.763945  pct:-7.274156127\n",
      "[Iter.  140]  loss:18.369892  pct:-7.053513516\n",
      "[Iter.  150]  loss:17.111763  pct:-6.848865043\n",
      "[Iter.  160]  loss:15.972737  pct:-6.656390041\n",
      "[Iter.  170]  loss:14.939134  pct:-6.471049063\n",
      "[Iter.  180]  loss:13.999982  pct:-6.286520934\n",
      "[Iter.  190]  loss:13.146229  pct:-6.098244249\n",
      "[Iter.  200]  loss:12.370335  pct:-5.902028463\n",
      "[Iter.  210]  loss:11.666249  pct:-5.691724366\n",
      "[Iter.  220]  loss:11.028258  pct:-5.468689520\n",
      "[Iter.  230]  loss:10.450196  pct:-5.241644152\n",
      "[Iter.  240]  loss:9.926167  pct:-5.014535275\n",
      "[Iter.  250]  loss:9.451131  pct:-4.785700238\n",
      "[Iter.  260]  loss:9.022631  pct:-4.533850832\n",
      "[Iter.  270]  loss:8.637366  pct:-4.269978567\n",
      "[Iter.  280]  loss:8.293874  pct:-3.976819973\n",
      "[Iter.  290]  loss:7.989625  pct:-3.668350168\n",
      "[Iter.  300]  loss:7.707824  pct:-3.527095259\n",
      "[Iter.  310]  loss:7.440796  pct:-3.464367980\n",
      "[Iter.  320]  loss:7.187785  pct:-3.400324561\n",
      "[Iter.  330]  loss:6.948067  pct:-3.335074093\n",
      "[Iter.  340]  loss:6.720938  pct:-3.268958881\n",
      "[Iter.  350]  loss:6.505797  pct:-3.201053606\n",
      "[Iter.  360]  loss:6.302016  pct:-3.132293460\n",
      "[Iter.  370]  loss:6.109005  pct:-3.062690986\n",
      "[Iter.  380]  loss:5.926191  pct:-2.992527346\n",
      "[Iter.  390]  loss:5.753086  pct:-2.921028146\n",
      "[Iter.  400]  loss:5.589172  pct:-2.849136289\n",
      "[Iter.  410]  loss:5.433978  pct:-2.776695232\n",
      "[Iter.  420]  loss:5.287045  pct:-2.703977701\n",
      "[Iter.  430]  loss:5.147952  pct:-2.630825626\n",
      "[Iter.  440]  loss:5.016275  pct:-2.557836732\n",
      "[Iter.  450]  loss:4.891665  pct:-2.484112955\n",
      "[Iter.  460]  loss:4.773721  pct:-2.411136215\n",
      "[Iter.  470]  loss:4.662105  pct:-2.338137078\n",
      "[Iter.  480]  loss:4.556501  pct:-2.265140467\n",
      "[Iter.  490]  loss:4.456572  pct:-2.193125845\n",
      "[Iter.  500]  loss:4.362035  pct:-2.121289418\n",
      "[Iter.  510]  loss:4.272604  pct:-2.050197587\n",
      "[Iter.  520]  loss:4.188009  pct:-1.979955807\n",
      "[Iter.  530]  loss:4.107992  pct:-1.910623734\n",
      "[Iter.  540]  loss:4.032313  pct:-1.842222530\n",
      "[Iter.  550]  loss:3.960739  pct:-1.775010188\n",
      "[Iter.  560]  loss:3.893053  pct:-1.708925393\n",
      "[Iter.  570]  loss:3.829040  pct:-1.644294026\n",
      "[Iter.  580]  loss:3.768522  pct:-1.580501263\n",
      "[Iter.  590]  loss:3.711292  pct:-1.518645034\n",
      "[Iter.  600]  loss:3.657196  pct:-1.457605331\n",
      "[Iter.  610]  loss:3.606051  pct:-1.398471517\n",
      "[Iter.  620]  loss:3.557688  pct:-1.341148451\n",
      "[Iter.  630]  loss:3.511981  pct:-1.284731711\n",
      "[Iter.  640]  loss:3.468773  pct:-1.230319671\n",
      "[Iter.  650]  loss:3.427937  pct:-1.177242175\n",
      "[Iter.  660]  loss:3.389343  pct:-1.125880784\n",
      "[Iter.  670]  loss:3.352865  pct:-1.076250148\n",
      "[Iter.  680]  loss:3.318382  pct:-1.028462482\n",
      "[Iter.  690]  loss:3.285793  pct:-0.982073819\n",
      "[Iter.  700]  loss:3.255008  pct:-0.936914936\n",
      "[Iter.  710]  loss:3.225906  pct:-0.894063882\n",
      "[Iter.  720]  loss:3.198411  pct:-0.852315854\n",
      "[Iter.  730]  loss:3.172433  pct:-0.812211128\n",
      "[Iter.  740]  loss:3.147878  pct:-0.774025603\n",
      "[Iter.  750]  loss:3.124681  pct:-0.736907146\n",
      "[Iter.  760]  loss:3.102767  pct:-0.701304554\n",
      "[Iter.  770]  loss:3.082058  pct:-0.667453054\n",
      "[Iter.  780]  loss:3.062492  pct:-0.634814324\n",
      "[Iter.  790]  loss:3.044012  pct:-0.603439900\n",
      "[Iter.  800]  loss:3.026549  pct:-0.573698306\n",
      "[Iter.  810]  loss:3.010048  pct:-0.545183199\n",
      "[Iter.  820]  loss:2.994465  pct:-0.517708513\n",
      "[Iter.  830]  loss:2.979743  pct:-0.491644026\n",
      "[Iter.  840]  loss:2.965826  pct:-0.467044670\n",
      "[Iter.  850]  loss:2.952684  pct:-0.443109922\n",
      "[Iter.  860]  loss:2.940260  pct:-0.420777465\n",
      "[Iter.  870]  loss:2.928527  pct:-0.399064412\n",
      "[Iter.  880]  loss:2.917440  pct:-0.378559831\n",
      "[Iter.  890]  loss:2.906971  pct:-0.358848742\n",
      "[Iter.  900]  loss:2.897072  pct:-0.340522841\n",
      "[Iter.  910]  loss:2.887720  pct:-0.322832260\n",
      "[Iter.  920]  loss:2.878881  pct:-0.306093788\n",
      "[Iter.  930]  loss:2.870533  pct:-0.289973416\n",
      "[Iter.  940]  loss:2.862642  pct:-0.274886324\n",
      "[Iter.  950]  loss:2.855189  pct:-0.260361012\n",
      "[Iter.  960]  loss:2.848145  pct:-0.246703089\n",
      "[Iter.  970]  loss:2.841487  pct:-0.233760548\n",
      "[Iter.  980]  loss:2.835195  pct:-0.221428655\n",
      "[Iter.  990]  loss:2.829251  pct:-0.209667776\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.723150  pct:100.000000000\n",
      "[Iter.    2]  loss:2.723152  pct:0.000070042\n",
      "[Iter.    4]  loss:2.723152  pct:-0.000008755\n",
      "[Iter.    6]  loss:2.723152  pct:-0.000008755\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.723152\n",
      "Best loss: 2.723152 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1776/10000 [00:29<02:18, 59.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:457.609741  pct:100.000000000\n",
      "[Iter.   10]  loss:92.208168  pct:-79.850044242\n",
      "[Iter.   20]  loss:79.347801  pct:-13.947101538\n",
      "[Iter.   30]  loss:69.942001  pct:-11.853888479\n",
      "[Iter.   40]  loss:62.569225  pct:-10.541271182\n",
      "[Iter.   50]  loss:56.559792  pct:-9.604456051\n",
      "[Iter.   60]  loss:51.527256  pct:-8.897726483\n",
      "[Iter.   70]  loss:47.228161  pct:-8.343341925\n",
      "[Iter.   80]  loss:43.498886  pct:-7.896294672\n",
      "[Iter.   90]  loss:40.224602  pct:-7.527283238\n",
      "[Iter.  100]  loss:37.321617  pct:-7.216938125\n",
      "[Iter.  110]  loss:34.727066  pct:-6.951872095\n",
      "[Iter.  120]  loss:32.392708  pct:-6.722013926\n",
      "[Iter.  130]  loss:30.280584  pct:-6.520367179\n",
      "[Iter.  140]  loss:28.360348  pct:-6.341477979\n",
      "[Iter.  150]  loss:26.607405  pct:-6.180964544\n",
      "[Iter.  160]  loss:25.001484  pct:-6.035616060\n",
      "[Iter.  170]  loss:23.525698  pct:-5.902794466\n",
      "[Iter.  180]  loss:22.165850  pct:-5.780266496\n",
      "[Iter.  190]  loss:20.909849  pct:-5.666376596\n",
      "[Iter.  200]  loss:19.747332  pct:-5.559664914\n",
      "[Iter.  210]  loss:18.669334  pct:-5.458951257\n",
      "[Iter.  220]  loss:17.668110  pct:-5.362936330\n",
      "[Iter.  230]  loss:16.736801  pct:-5.271128332\n",
      "[Iter.  240]  loss:15.869372  pct:-5.182763253\n",
      "[Iter.  250]  loss:15.060505  pct:-5.097034941\n",
      "[Iter.  260]  loss:14.305511  pct:-5.013068573\n",
      "[Iter.  270]  loss:13.600117  pct:-4.930929916\n",
      "[Iter.  280]  loss:12.940506  pct:-4.850037403\n",
      "[Iter.  290]  loss:12.323233  pct:-4.770086514\n",
      "[Iter.  300]  loss:11.745193  pct:-4.690645594\n",
      "[Iter.  310]  loss:11.203602  pct:-4.611176863\n",
      "[Iter.  320]  loss:10.695844  pct:-4.532097337\n",
      "[Iter.  330]  loss:10.219587  pct:-4.452723731\n",
      "[Iter.  340]  loss:9.772689  pct:-4.372959946\n",
      "[Iter.  350]  loss:9.353125  pct:-4.293232425\n",
      "[Iter.  360]  loss:8.959138  pct:-4.212353818\n",
      "[Iter.  370]  loss:8.589011  pct:-4.131276108\n",
      "[Iter.  380]  loss:8.241226  pct:-4.049185503\n",
      "[Iter.  390]  loss:7.914351  pct:-3.966347712\n",
      "[Iter.  400]  loss:7.607021  pct:-3.883194892\n",
      "[Iter.  410]  loss:7.318065  pct:-3.798545723\n",
      "[Iter.  420]  loss:7.046308  pct:-3.713510844\n",
      "[Iter.  430]  loss:6.790690  pct:-3.627675054\n",
      "[Iter.  440]  loss:6.550220  pct:-3.541170597\n",
      "[Iter.  450]  loss:6.323980  pct:-3.453931947\n",
      "[Iter.  460]  loss:6.111082  pct:-3.366530572\n",
      "[Iter.  470]  loss:5.910744  pct:-3.278272161\n",
      "[Iter.  480]  loss:5.722188  pct:-3.190042570\n",
      "[Iter.  490]  loss:5.544712  pct:-3.101547720\n",
      "[Iter.  500]  loss:5.377670  pct:-3.012632154\n",
      "[Iter.  510]  loss:5.220452  pct:-2.923533259\n",
      "[Iter.  520]  loss:5.072448  pct:-2.835090201\n",
      "[Iter.  530]  loss:4.933134  pct:-2.746488102\n",
      "[Iter.  540]  loss:4.801972  pct:-2.658780878\n",
      "[Iter.  550]  loss:4.678484  pct:-2.571618831\n",
      "[Iter.  560]  loss:4.562227  pct:-2.484922782\n",
      "[Iter.  570]  loss:4.452782  pct:-2.398950467\n",
      "[Iter.  580]  loss:4.349745  pct:-2.313989051\n",
      "[Iter.  590]  loss:4.252747  pct:-2.229963881\n",
      "[Iter.  600]  loss:4.161444  pct:-2.146915169\n",
      "[Iter.  610]  loss:4.075474  pct:-2.065879005\n",
      "[Iter.  620]  loss:3.994537  pct:-1.985956760\n",
      "[Iter.  630]  loss:3.918350  pct:-1.907265484\n",
      "[Iter.  640]  loss:3.846630  pct:-1.830359229\n",
      "[Iter.  650]  loss:3.779110  pct:-1.755306222\n",
      "[Iter.  660]  loss:3.715558  pct:-1.681675748\n",
      "[Iter.  670]  loss:3.655713  pct:-1.610646284\n",
      "[Iter.  680]  loss:3.599367  pct:-1.541324813\n",
      "[Iter.  690]  loss:3.546335  pct:-1.473361410\n",
      "[Iter.  700]  loss:3.496428  pct:-1.407289621\n",
      "[Iter.  710]  loss:3.449442  pct:-1.343817921\n",
      "[Iter.  720]  loss:3.405217  pct:-1.282090626\n",
      "[Iter.  730]  loss:3.363601  pct:-1.222137527\n",
      "[Iter.  740]  loss:3.324428  pct:-1.164604534\n",
      "[Iter.  750]  loss:3.287571  pct:-1.108682848\n",
      "[Iter.  760]  loss:3.252888  pct:-1.054973098\n",
      "[Iter.  770]  loss:3.220248  pct:-1.003430106\n",
      "[Iter.  780]  loss:3.189520  pct:-0.954208124\n",
      "[Iter.  790]  loss:3.160609  pct:-0.906426049\n",
      "[Iter.  800]  loss:3.133399  pct:-0.860917486\n",
      "[Iter.  810]  loss:3.107793  pct:-0.817185441\n",
      "[Iter.  820]  loss:3.083704  pct:-0.775134447\n",
      "[Iter.  830]  loss:3.061043  pct:-0.734854442\n",
      "[Iter.  840]  loss:3.039736  pct:-0.696054130\n",
      "[Iter.  850]  loss:3.019680  pct:-0.659810074\n",
      "[Iter.  860]  loss:3.000815  pct:-0.624730813\n",
      "[Iter.  870]  loss:2.983043  pct:-0.592245714\n",
      "[Iter.  880]  loss:2.966333  pct:-0.560167739\n",
      "[Iter.  890]  loss:2.950598  pct:-0.530425864\n",
      "[Iter.  900]  loss:2.935784  pct:-0.502096567\n",
      "[Iter.  910]  loss:2.921839  pct:-0.475004444\n",
      "[Iter.  920]  loss:2.908714  pct:-0.449201511\n",
      "[Iter.  930]  loss:2.896370  pct:-0.424351450\n",
      "[Iter.  940]  loss:2.884761  pct:-0.400814668\n",
      "[Iter.  950]  loss:2.873834  pct:-0.378798711\n",
      "[Iter.  960]  loss:2.863540  pct:-0.358204384\n",
      "[Iter.  970]  loss:2.853858  pct:-0.338102582\n",
      "[Iter.  980]  loss:2.844759  pct:-0.318815086\n",
      "[Iter.  990]  loss:2.836215  pct:-0.300365763\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.692569  pct:100.000000000\n",
      "[Iter.    2]  loss:2.692584  pct:0.000531281\n",
      "[Iter.    4]  loss:2.692596  pct:0.000469296\n",
      "[Iter.    6]  loss:2.692607  pct:0.000398457\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.692607\n",
      "Best loss: 2.692607 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 15%|█▌        | 1542/10000 [00:15<01:26, 97.32it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:795.325134  pct:100.000000000\n",
      "[Iter.   10]  loss:131.472809  pct:-83.469296557\n",
      "[Iter.   20]  loss:118.656166  pct:-9.748512163\n",
      "[Iter.   30]  loss:108.945457  pct:-8.183905598\n",
      "[Iter.   40]  loss:100.716751  pct:-7.553051363\n",
      "[Iter.   50]  loss:93.616051  pct:-7.050168220\n",
      "[Iter.   60]  loss:87.394485  pct:-6.645831776\n",
      "[Iter.   70]  loss:81.877716  pct:-6.312491434\n",
      "[Iter.   80]  loss:76.939415  pct:-6.031312699\n",
      "[Iter.   90]  loss:72.484428  pct:-5.790252725\n",
      "[Iter.  100]  loss:68.438843  pct:-5.581316872\n",
      "[Iter.  110]  loss:64.744064  pct:-5.398657097\n",
      "[Iter.  120]  loss:61.353172  pct:-5.237379000\n",
      "[Iter.  130]  loss:58.227810  pct:-5.094051830\n",
      "[Iter.  140]  loss:55.336308  pct:-4.965844302\n",
      "[Iter.  150]  loss:52.652180  pct:-4.850572667\n",
      "[Iter.  160]  loss:50.153309  pct:-4.745996962\n",
      "[Iter.  170]  loss:47.820702  pct:-4.650953889\n",
      "[Iter.  180]  loss:45.638165  pct:-4.564000539\n",
      "[Iter.  190]  loss:43.591663  pct:-4.484188138\n",
      "[Iter.  200]  loss:41.669018  pct:-4.410580879\n",
      "[Iter.  210]  loss:39.859589  pct:-4.342384977\n",
      "[Iter.  220]  loss:38.154041  pct:-4.278888447\n",
      "[Iter.  230]  loss:36.543941  pct:-4.219998034\n",
      "[Iter.  240]  loss:35.022141  pct:-4.164304485\n",
      "[Iter.  250]  loss:33.581928  pct:-4.112290765\n",
      "[Iter.  260]  loss:32.217453  pct:-4.063123594\n",
      "[Iter.  270]  loss:30.923361  pct:-4.016742659\n",
      "[Iter.  280]  loss:29.694891  pct:-3.972627217\n",
      "[Iter.  290]  loss:28.527819  pct:-3.930212430\n",
      "[Iter.  300]  loss:27.418081  pct:-3.890018402\n",
      "[Iter.  310]  loss:26.362146  pct:-3.851235596\n",
      "[Iter.  320]  loss:25.356739  pct:-3.813829568\n",
      "[Iter.  330]  loss:24.398771  pct:-3.777961182\n",
      "[Iter.  340]  loss:23.485472  pct:-3.743219484\n",
      "[Iter.  350]  loss:22.614271  pct:-3.709529754\n",
      "[Iter.  360]  loss:21.782888  pct:-3.676363237\n",
      "[Iter.  370]  loss:20.989117  pct:-3.644015104\n",
      "[Iter.  380]  loss:20.230879  pct:-3.612528582\n",
      "[Iter.  390]  loss:19.506371  pct:-3.581200261\n",
      "[Iter.  400]  loss:18.813732  pct:-3.550831743\n",
      "[Iter.  410]  loss:18.151400  pct:-3.520473926\n",
      "[Iter.  420]  loss:17.517776  pct:-3.490767306\n",
      "[Iter.  430]  loss:16.911518  pct:-3.460818174\n",
      "[Iter.  440]  loss:16.331270  pct:-3.431080969\n",
      "[Iter.  450]  loss:15.775709  pct:-3.401823975\n",
      "[Iter.  460]  loss:15.243648  pct:-3.372663452\n",
      "[Iter.  470]  loss:14.734042  pct:-3.343067368\n",
      "[Iter.  480]  loss:14.245814  pct:-3.313604228\n",
      "[Iter.  490]  loss:13.777958  pct:-3.284167521\n",
      "[Iter.  500]  loss:13.329613  pct:-3.254075728\n",
      "[Iter.  510]  loss:12.899812  pct:-3.224407159\n",
      "[Iter.  520]  loss:12.487800  pct:-3.193938860\n",
      "[Iter.  530]  loss:12.092717  pct:-3.163747698\n",
      "[Iter.  540]  loss:11.713821  pct:-3.133255779\n",
      "[Iter.  550]  loss:11.350418  pct:-3.102346430\n",
      "[Iter.  560]  loss:11.001842  pct:-3.071045867\n",
      "[Iter.  570]  loss:10.667394  pct:-3.039926174\n",
      "[Iter.  580]  loss:10.346543  pct:-3.007767237\n",
      "[Iter.  590]  loss:10.038696  pct:-2.975361082\n",
      "[Iter.  600]  loss:9.743287  pct:-2.942704850\n",
      "[Iter.  610]  loss:9.459794  pct:-2.909624231\n",
      "[Iter.  620]  loss:9.187742  pct:-2.875874569\n",
      "[Iter.  630]  loss:8.926635  pct:-2.841910865\n",
      "[Iter.  640]  loss:8.675961  pct:-2.808160675\n",
      "[Iter.  650]  loss:8.435347  pct:-2.773340615\n",
      "[Iter.  660]  loss:8.204375  pct:-2.738136881\n",
      "[Iter.  670]  loss:7.982660  pct:-2.702405035\n",
      "[Iter.  680]  loss:7.769763  pct:-2.666991063\n",
      "[Iter.  690]  loss:7.565395  pct:-2.630294358\n",
      "[Iter.  700]  loss:7.369161  pct:-2.593839674\n",
      "[Iter.  710]  loss:7.180780  pct:-2.556345176\n",
      "[Iter.  720]  loss:6.999839  pct:-2.519797383\n",
      "[Iter.  730]  loss:6.826099  pct:-2.482055864\n",
      "[Iter.  740]  loss:6.659274  pct:-2.443926167\n",
      "[Iter.  750]  loss:6.499121  pct:-2.404967667\n",
      "[Iter.  760]  loss:6.345312  pct:-2.366606200\n",
      "[Iter.  770]  loss:6.197608  pct:-2.327760161\n",
      "[Iter.  780]  loss:6.055786  pct:-2.288347443\n",
      "[Iter.  790]  loss:5.919609  pct:-2.248702214\n",
      "[Iter.  800]  loss:5.788826  pct:-2.209311505\n",
      "[Iter.  810]  loss:5.663251  pct:-2.169266368\n",
      "[Iter.  820]  loss:5.542635  pct:-2.129808965\n",
      "[Iter.  830]  loss:5.426791  pct:-2.090057355\n",
      "[Iter.  840]  loss:5.315549  pct:-2.049854997\n",
      "[Iter.  850]  loss:5.208729  pct:-2.009577919\n",
      "[Iter.  860]  loss:5.106175  pct:-1.968884140\n",
      "[Iter.  870]  loss:5.007675  pct:-1.929041672\n",
      "[Iter.  880]  loss:4.913086  pct:-1.888875643\n",
      "[Iter.  890]  loss:4.822250  pct:-1.848868864\n",
      "[Iter.  900]  loss:4.735045  pct:-1.808386856\n",
      "[Iter.  910]  loss:4.651279  pct:-1.769064166\n",
      "[Iter.  920]  loss:4.570852  pct:-1.729140958\n",
      "[Iter.  930]  loss:4.493619  pct:-1.689671076\n",
      "[Iter.  940]  loss:4.419452  pct:-1.650512007\n",
      "[Iter.  950]  loss:4.348241  pct:-1.611294545\n",
      "[Iter.  960]  loss:4.279851  pct:-1.572817316\n",
      "[Iter.  970]  loss:4.214163  pct:-1.534833885\n",
      "[Iter.  980]  loss:4.151086  pct:-1.496774062\n",
      "[Iter.  990]  loss:4.090538  pct:-1.458613498\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.630371  pct:100.000000000\n",
      "[Iter.    2]  loss:2.630370  pct:-0.000036256\n",
      "[Iter.    4]  loss:2.630371  pct:0.000063448\n",
      "[Iter.    6]  loss:2.630368  pct:-0.000126897\n",
      "[Iter.    8]  loss:2.630368  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.630368\n",
      "Best loss: 2.630368 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1783/10000 [00:17<01:19, 102.76it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:176.540024  pct:100.000000000\n",
      "[Iter.   10]  loss:36.190990  pct:-79.499836081\n",
      "[Iter.   20]  loss:27.865101  pct:-23.005420643\n",
      "[Iter.   30]  loss:22.915613  pct:-17.762317499\n",
      "[Iter.   40]  loss:19.310032  pct:-15.734168910\n",
      "[Iter.   50]  loss:16.546368  pct:-14.312064637\n",
      "[Iter.   60]  loss:14.356180  pct:-13.236666205\n",
      "[Iter.   70]  loss:12.579995  pct:-12.372267637\n",
      "[Iter.   80]  loss:11.115713  pct:-11.639766294\n",
      "[Iter.   90]  loss:9.893960  pct:-10.991225730\n",
      "[Iter.  100]  loss:8.865249  pct:-10.397366869\n",
      "[Iter.  110]  loss:7.993123  pct:-9.837581933\n",
      "[Iter.  120]  loss:7.249816  pct:-9.299321420\n",
      "[Iter.  130]  loss:6.613561  pct:-8.776156907\n",
      "[Iter.  140]  loss:6.067118  pct:-8.262469337\n",
      "[Iter.  150]  loss:5.596625  pct:-7.754799952\n",
      "[Iter.  160]  loss:5.190658  pct:-7.253778295\n",
      "[Iter.  170]  loss:4.839695  pct:-6.761428563\n",
      "[Iter.  180]  loss:4.536065  pct:-6.273758995\n",
      "[Iter.  190]  loss:4.273192  pct:-5.795170873\n",
      "[Iter.  200]  loss:4.045359  pct:-5.331677069\n",
      "[Iter.  210]  loss:3.847643  pct:-4.887471117\n",
      "[Iter.  220]  loss:3.676134  pct:-4.457514618\n",
      "[Iter.  230]  loss:3.527234  pct:-4.050451578\n",
      "[Iter.  240]  loss:3.398061  pct:-3.662175926\n",
      "[Iter.  250]  loss:3.285821  pct:-3.303034787\n",
      "[Iter.  260]  loss:3.188364  pct:-2.966005584\n",
      "[Iter.  270]  loss:3.103677  pct:-2.656112140\n",
      "[Iter.  280]  loss:3.030174  pct:-2.368278858\n",
      "[Iter.  290]  loss:2.966231  pct:-2.110190441\n",
      "[Iter.  300]  loss:2.910614  pct:-1.875024867\n",
      "[Iter.  310]  loss:2.862252  pct:-1.661550076\n",
      "[Iter.  320]  loss:2.820108  pct:-1.472425681\n",
      "[Iter.  330]  loss:2.783430  pct:-1.300582860\n",
      "[Iter.  340]  loss:2.751671  pct:-1.140986064\n",
      "[Iter.  350]  loss:2.723986  pct:-1.006121816\n",
      "[Iter.  360]  loss:2.699804  pct:-0.887746115\n",
      "[Iter.  370]  loss:2.678694  pct:-0.781910735\n",
      "[Iter.  380]  loss:2.660238  pct:-0.689000706\n",
      "[Iter.  390]  loss:2.644034  pct:-0.609105019\n",
      "[Iter.  400]  loss:2.629989  pct:-0.531195866\n",
      "[Iter.  410]  loss:2.617688  pct:-0.467728416\n",
      "[Iter.  420]  loss:2.606797  pct:-0.416061783\n",
      "[Iter.  430]  loss:2.596991  pct:-0.376167331\n",
      "[Iter.  440]  loss:2.588616  pct:-0.322476727\n",
      "[Iter.  450]  loss:2.581438  pct:-0.277302860\n",
      "[Iter.  460]  loss:2.575245  pct:-0.239911299\n",
      "[Iter.  470]  loss:2.569565  pct:-0.220555580\n",
      "[Iter.  480]  loss:2.564854  pct:-0.183325762\n",
      "[Iter.  490]  loss:2.560810  pct:-0.157681261\n",
      "[Iter.  500]  loss:2.557316  pct:-0.136432850\n",
      "[Iter.  510]  loss:2.554244  pct:-0.120117533\n",
      "[Iter.  520]  loss:2.551525  pct:-0.106475358\n",
      "[Iter.  530]  loss:2.549038  pct:-0.097440915\n",
      "[Iter.  540]  loss:2.546858  pct:-0.085526349\n",
      "[Iter.  550]  loss:2.545034  pct:-0.071623166\n",
      "[Iter.  560]  loss:2.543418  pct:-0.063486877\n",
      "[Iter.  570]  loss:2.541982  pct:-0.056468630\n",
      "[Iter.  580]  loss:2.540702  pct:-0.050375892\n",
      "[Iter.  590]  loss:2.539591  pct:-0.043729282\n",
      "[Iter.  600]  loss:2.538493  pct:-0.043222681\n",
      "[Iter.  610]  loss:2.537714  pct:-0.030702876\n",
      "[Iter.  620]  loss:2.536955  pct:-0.029876149\n",
      "[Iter.  630]  loss:2.536142  pct:-0.032046577\n",
      "[Iter.  640]  loss:2.535568  pct:-0.022665415\n",
      "[Iter.  650]  loss:2.535065  pct:-0.019821455\n",
      "[Iter.  660]  loss:2.534594  pct:-0.018574542\n",
      "[Iter.  670]  loss:2.533777  pct:-0.032226938\n",
      "[Iter.  680]  loss:2.533350  pct:-0.016880842\n",
      "[Iter.  690]  loss:2.533088  pct:-0.010333497\n",
      "[Iter.  700]  loss:2.532817  pct:-0.010682815\n",
      "[Iter.  710]  loss:2.532447  pct:-0.014599839\n",
      "[Iter.  720]  loss:2.531783  pct:-0.026247772\n",
      "[Iter.  730]  loss:2.531416  pct:-0.014483383\n",
      "[Iter.  740]  loss:2.530993  pct:-0.016698802\n",
      "[Iter.  750]  loss:2.530543  pct:-0.017803727\n",
      "[Iter.  760]  loss:2.530163  pct:-0.015008670\n",
      "[Iter.  770]  loss:2.529902  pct:-0.010308820\n",
      "[Iter.  780]  loss:2.529703  pct:-0.007859636\n",
      "[Iter.  790]  loss:2.529519  pct:-0.007275919\n",
      "[Iter.  800]  loss:2.529338  pct:-0.007153917\n",
      "[Iter.  810]  loss:2.529113  pct:-0.008917114\n",
      "[Iter.  820]  loss:2.528841  pct:-0.010756168\n",
      "[Iter.  830]  loss:2.528642  pct:-0.007834651\n",
      "[Iter.  840]  loss:2.528467  pct:-0.006948966\n",
      "[Iter.  850]  loss:2.528178  pct:-0.011418972\n",
      "[Iter.  860]  loss:2.527677  pct:-0.019822807\n",
      "[Iter.  870]  loss:2.527691  pct:0.000547075\n",
      "[Iter.  880]  loss:2.527731  pct:0.001584621\n",
      "[Iter.  890]  loss:2.527831  pct:0.003952058\n",
      "[Iter.  900]  loss:2.527583  pct:-0.009780721\n",
      "[Iter.  910]  loss:2.526739  pct:-0.033401082\n",
      "[Iter.  920]  loss:2.526407  pct:-0.013134663\n",
      "[Iter.  930]  loss:2.526179  pct:-0.009021830\n",
      "[Iter.  940]  loss:2.526011  pct:-0.006672604\n",
      "[Iter.  950]  loss:2.525915  pct:-0.003775417\n",
      "[Iter.  960]  loss:2.525836  pct:-0.003133714\n",
      "[Iter.  970]  loss:2.525747  pct:-0.003539698\n",
      "[Iter.  980]  loss:2.525636  pct:-0.004379941\n",
      "[Iter.  990]  loss:2.525529  pct:-0.004238534\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.524023  pct:100.000000000\n",
      "[Iter.    2]  loss:2.524015  pct:-0.000321163\n",
      "[Iter.    4]  loss:2.524011  pct:-0.000151136\n",
      "[Iter.    6]  loss:2.524009  pct:-0.000103906\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.524009\n",
      "Best loss: 2.524009 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  5%|▌         | 531/10000 [00:05<01:30, 104.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:2329.282959  pct:100.000000000\n",
      "[Iter.   10]  loss:354.167542  pct:-84.794999719\n",
      "[Iter.   20]  loss:331.854248  pct:-6.300208473\n",
      "[Iter.   30]  loss:318.396515  pct:-4.055314414\n",
      "[Iter.   40]  loss:306.250305  pct:-3.814806114\n",
      "[Iter.   50]  loss:294.971100  pct:-3.683002149\n",
      "[Iter.   60]  loss:284.472290  pct:-3.559267270\n",
      "[Iter.   70]  loss:274.668884  pct:-3.446172476\n",
      "[Iter.   80]  loss:265.484192  pct:-3.343914403\n",
      "[Iter.   90]  loss:256.854462  pct:-3.250562741\n",
      "[Iter.  100]  loss:248.726196  pct:-3.164541246\n",
      "[Iter.  110]  loss:241.053192  pct:-3.084919990\n",
      "[Iter.  120]  loss:233.794769  pct:-3.011129115\n",
      "[Iter.  130]  loss:226.915283  pct:-2.942532078\n",
      "[Iter.  140]  loss:220.382935  pct:-2.878760981\n",
      "[Iter.  150]  loss:214.169464  pct:-2.819397278\n",
      "[Iter.  160]  loss:208.250000  pct:-2.763916012\n",
      "[Iter.  170]  loss:202.602280  pct:-2.711990558\n",
      "[Iter.  180]  loss:197.206192  pct:-2.663389403\n",
      "[Iter.  190]  loss:192.043671  pct:-2.617829242\n",
      "[Iter.  200]  loss:187.098618  pct:-2.574962811\n",
      "[Iter.  210]  loss:182.356201  pct:-2.534714817\n",
      "[Iter.  220]  loss:177.803360  pct:-2.496674726\n",
      "[Iter.  230]  loss:173.427963  pct:-2.460806550\n",
      "[Iter.  240]  loss:169.219040  pct:-2.426900057\n",
      "[Iter.  250]  loss:165.166336  pct:-2.394945545\n",
      "[Iter.  260]  loss:161.260925  pct:-2.364531938\n",
      "[Iter.  270]  loss:157.494171  pct:-2.335813306\n",
      "[Iter.  280]  loss:153.858215  pct:-2.308628811\n",
      "[Iter.  290]  loss:150.346405  pct:-2.282497750\n",
      "[Iter.  300]  loss:146.952026  pct:-2.257705238\n",
      "[Iter.  310]  loss:143.668854  pct:-2.234179881\n",
      "[Iter.  320]  loss:140.491241  pct:-2.211761437\n",
      "[Iter.  330]  loss:137.414078  pct:-2.190288636\n",
      "[Iter.  340]  loss:134.432480  pct:-2.169790715\n",
      "[Iter.  350]  loss:131.541901  pct:-2.150208957\n",
      "[Iter.  360]  loss:128.738068  pct:-2.131513225\n",
      "[Iter.  370]  loss:126.017227  pct:-2.113470013\n",
      "[Iter.  380]  loss:123.375542  pct:-2.096289170\n",
      "[Iter.  390]  loss:120.809532  pct:-2.079836478\n",
      "[Iter.  400]  loss:118.316078  pct:-2.063954669\n",
      "[Iter.  410]  loss:115.891899  pct:-2.048900804\n",
      "[Iter.  420]  loss:113.534126  pct:-2.034458703\n",
      "[Iter.  430]  loss:111.240471  pct:-2.020234330\n",
      "[Iter.  440]  loss:109.008270  pct:-2.006644349\n",
      "[Iter.  450]  loss:106.835121  pct:-1.993563519\n",
      "[Iter.  460]  loss:104.718483  pct:-1.981219435\n",
      "[Iter.  470]  loss:102.656448  pct:-1.969121924\n",
      "[Iter.  480]  loss:100.646851  pct:-1.957595271\n",
      "[Iter.  490]  loss:98.688034  pct:-1.946227345\n",
      "[Iter.  500]  loss:96.778076  pct:-1.935349006\n",
      "[Iter.  510]  loss:94.915291  pct:-1.924800960\n",
      "[Iter.  520]  loss:93.097900  pct:-1.914749906\n",
      "[Iter.  530]  loss:91.324463  pct:-1.904916752\n",
      "[Iter.  540]  loss:89.593697  pct:-1.895183658\n",
      "[Iter.  550]  loss:87.903946  pct:-1.886015128\n",
      "[Iter.  560]  loss:86.253891  pct:-1.877111334\n",
      "[Iter.  570]  loss:84.642365  pct:-1.868352222\n",
      "[Iter.  580]  loss:83.068016  pct:-1.860000555\n",
      "[Iter.  590]  loss:81.529648  pct:-1.851938084\n",
      "[Iter.  600]  loss:80.026291  pct:-1.843938952\n",
      "[Iter.  610]  loss:78.556580  pct:-1.836535578\n",
      "[Iter.  620]  loss:77.119835  pct:-1.828929795\n",
      "[Iter.  630]  loss:75.714966  pct:-1.821670238\n",
      "[Iter.  640]  loss:74.341011  pct:-1.814640947\n",
      "[Iter.  650]  loss:72.997009  pct:-1.807887398\n",
      "[Iter.  660]  loss:71.682083  pct:-1.801342494\n",
      "[Iter.  670]  loss:70.395599  pct:-1.794707559\n",
      "[Iter.  680]  loss:69.136803  pct:-1.788175260\n",
      "[Iter.  690]  loss:67.904732  pct:-1.782076803\n",
      "[Iter.  700]  loss:66.698753  pct:-1.775985800\n",
      "[Iter.  710]  loss:65.518127  pct:-1.770086930\n",
      "[Iter.  720]  loss:64.362091  pct:-1.764452713\n",
      "[Iter.  730]  loss:63.230106  pct:-1.758775534\n",
      "[Iter.  740]  loss:62.121517  pct:-1.753261597\n",
      "[Iter.  750]  loss:61.035664  pct:-1.747950832\n",
      "[Iter.  760]  loss:59.972004  pct:-1.742685514\n",
      "[Iter.  770]  loss:58.929768  pct:-1.737871440\n",
      "[Iter.  780]  loss:57.908401  pct:-1.733192172\n",
      "[Iter.  790]  loss:56.907822  pct:-1.727866438\n",
      "[Iter.  800]  loss:55.927288  pct:-1.723020793\n",
      "[Iter.  810]  loss:54.966408  pct:-1.718088455\n",
      "[Iter.  820]  loss:54.024509  pct:-1.713589052\n",
      "[Iter.  830]  loss:53.101219  pct:-1.709021077\n",
      "[Iter.  840]  loss:52.196049  pct:-1.704613293\n",
      "[Iter.  850]  loss:51.308704  pct:-1.700022093\n",
      "[Iter.  860]  loss:50.438652  pct:-1.695720732\n",
      "[Iter.  870]  loss:49.585499  pct:-1.691467147\n",
      "[Iter.  880]  loss:48.748810  pct:-1.687366297\n",
      "[Iter.  890]  loss:47.928276  pct:-1.683187252\n",
      "[Iter.  900]  loss:47.123573  pct:-1.678972884\n",
      "[Iter.  910]  loss:46.334190  pct:-1.675133865\n",
      "[Iter.  920]  loss:45.559864  pct:-1.671176982\n",
      "[Iter.  930]  loss:44.800316  pct:-1.667143226\n",
      "[Iter.  940]  loss:44.055099  pct:-1.663417669\n",
      "[Iter.  950]  loss:43.323971  pct:-1.659577895\n",
      "[Iter.  960]  loss:42.606400  pct:-1.656291530\n",
      "[Iter.  970]  loss:41.902264  pct:-1.652652894\n",
      "[Iter.  980]  loss:41.211441  pct:-1.648652224\n",
      "[Iter.  990]  loss:40.533524  pct:-1.644973977\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.653944  pct:100.000000000\n",
      "[Iter.    2]  loss:2.653615  pct:-0.012406291\n",
      "[Iter.    4]  loss:2.653562  pct:-0.001985612\n",
      "[Iter.    6]  loss:2.653539  pct:-0.000880515\n",
      "[Iter.    8]  loss:2.653534  pct:-0.000188684\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.653534\n",
      "Best loss: 2.653534 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 28%|██▊       | 2753/10000 [00:41<01:49, 66.15it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:446.747375  pct:100.000000000\n",
      "[Iter.   10]  loss:84.134041  pct:-81.167425301\n",
      "[Iter.   20]  loss:72.253922  pct:-14.120466824\n",
      "[Iter.   30]  loss:63.690243  pct:-11.852199248\n",
      "[Iter.   40]  loss:56.952328  pct:-10.579195095\n",
      "[Iter.   50]  loss:51.447136  pct:-9.666315711\n",
      "[Iter.   60]  loss:46.829407  pct:-8.975677856\n",
      "[Iter.   70]  loss:42.880093  pct:-8.433406256\n",
      "[Iter.   80]  loss:39.451866  pct:-7.994913867\n",
      "[Iter.   90]  loss:36.440933  pct:-7.631915081\n",
      "[Iter.  100]  loss:33.771553  pct:-7.325224553\n",
      "[Iter.  110]  loss:31.386572  pct:-7.062100913\n",
      "[Iter.  120]  loss:29.241863  pct:-6.833204471\n",
      "[Iter.  130]  loss:27.302679  pct:-6.631534291\n",
      "[Iter.  140]  loss:25.541197  pct:-6.451682763\n",
      "[Iter.  150]  loss:23.934868  pct:-6.289168731\n",
      "[Iter.  160]  loss:22.464922  pct:-6.141441500\n",
      "[Iter.  170]  loss:21.115828  pct:-6.005337538\n",
      "[Iter.  180]  loss:19.874454  pct:-5.878874785\n",
      "[Iter.  190]  loss:18.729624  pct:-5.760312585\n",
      "[Iter.  200]  loss:17.671700  pct:-5.648401069\n",
      "[Iter.  210]  loss:16.692442  pct:-5.541388831\n",
      "[Iter.  220]  loss:15.784582  pct:-5.438747701\n",
      "[Iter.  230]  loss:14.941730  pct:-5.339720653\n",
      "[Iter.  240]  loss:14.158299  pct:-5.243235712\n",
      "[Iter.  250]  loss:13.429304  pct:-5.148890416\n",
      "[Iter.  260]  loss:12.750299  pct:-5.056141874\n",
      "[Iter.  270]  loss:12.117386  pct:-4.963911568\n",
      "[Iter.  280]  loss:11.526943  pct:-4.872690068\n",
      "[Iter.  290]  loss:10.975778  pct:-4.781541566\n",
      "[Iter.  300]  loss:10.460965  pct:-4.690441871\n",
      "[Iter.  310]  loss:9.979830  pct:-4.599340129\n",
      "[Iter.  320]  loss:9.529973  pct:-4.507659626\n",
      "[Iter.  330]  loss:9.109178  pct:-4.415494560\n",
      "[Iter.  340]  loss:8.715333  pct:-4.323602220\n",
      "[Iter.  350]  loss:8.346556  pct:-4.231361851\n",
      "[Iter.  360]  loss:8.001097  pct:-4.138940617\n",
      "[Iter.  370]  loss:7.677691  pct:-4.042017660\n",
      "[Iter.  380]  loss:7.374708  pct:-3.946281306\n",
      "[Iter.  390]  loss:7.090796  pct:-3.849802821\n",
      "[Iter.  400]  loss:6.824677  pct:-3.753026887\n",
      "[Iter.  410]  loss:6.575161  pct:-3.656071553\n",
      "[Iter.  420]  loss:6.341197  pct:-3.558314447\n",
      "[Iter.  430]  loss:6.121823  pct:-3.459500139\n",
      "[Iter.  440]  loss:5.916091  pct:-3.360623094\n",
      "[Iter.  450]  loss:5.723169  pct:-3.260972505\n",
      "[Iter.  460]  loss:5.542191  pct:-3.162212486\n",
      "[Iter.  470]  loss:5.372424  pct:-3.063164727\n",
      "[Iter.  480]  loss:5.213136  pct:-2.964926124\n",
      "[Iter.  490]  loss:5.063715  pct:-2.866235338\n",
      "[Iter.  500]  loss:4.923536  pct:-2.768306624\n",
      "[Iter.  510]  loss:4.792039  pct:-2.670781954\n",
      "[Iter.  520]  loss:4.668684  pct:-2.574173348\n",
      "[Iter.  530]  loss:4.552873  pct:-2.480589451\n",
      "[Iter.  540]  loss:4.444256  pct:-2.385676848\n",
      "[Iter.  550]  loss:4.342474  pct:-2.290188707\n",
      "[Iter.  560]  loss:4.246934  pct:-2.200129397\n",
      "[Iter.  570]  loss:4.157306  pct:-2.110410571\n",
      "[Iter.  580]  loss:4.073256  pct:-2.021757634\n",
      "[Iter.  590]  loss:3.994448  pct:-1.934756895\n",
      "[Iter.  600]  loss:3.920552  pct:-1.849977996\n",
      "[Iter.  610]  loss:3.851275  pct:-1.767016937\n",
      "[Iter.  620]  loss:3.786280  pct:-1.687605786\n",
      "[Iter.  630]  loss:3.725352  pct:-1.609199933\n",
      "[Iter.  640]  loss:3.668203  pct:-1.534041968\n",
      "[Iter.  650]  loss:3.614592  pct:-1.461506858\n",
      "[Iter.  660]  loss:3.564291  pct:-1.391611389\n",
      "[Iter.  670]  loss:3.517071  pct:-1.324794043\n",
      "[Iter.  680]  loss:3.472801  pct:-1.258739097\n",
      "[Iter.  690]  loss:3.431272  pct:-1.195820508\n",
      "[Iter.  700]  loss:3.392395  pct:-1.133034168\n",
      "[Iter.  710]  loss:3.355939  pct:-1.074628208\n",
      "[Iter.  720]  loss:3.321705  pct:-1.020117536\n",
      "[Iter.  730]  loss:3.289596  pct:-0.966642906\n",
      "[Iter.  740]  loss:3.259510  pct:-0.914574486\n",
      "[Iter.  750]  loss:3.231350  pct:-0.863921881\n",
      "[Iter.  760]  loss:3.204956  pct:-0.816806873\n",
      "[Iter.  770]  loss:3.180128  pct:-0.774673814\n",
      "[Iter.  780]  loss:3.156802  pct:-0.733497396\n",
      "[Iter.  790]  loss:3.134914  pct:-0.693375551\n",
      "[Iter.  800]  loss:3.114386  pct:-0.654821158\n",
      "[Iter.  810]  loss:3.095095  pct:-0.619390508\n",
      "[Iter.  820]  loss:3.076948  pct:-0.586322159\n",
      "[Iter.  830]  loss:3.059903  pct:-0.553966416\n",
      "[Iter.  840]  loss:3.043908  pct:-0.522722051\n",
      "[Iter.  850]  loss:3.028923  pct:-0.492305364\n",
      "[Iter.  860]  loss:3.014893  pct:-0.463192436\n",
      "[Iter.  870]  loss:3.001696  pct:-0.437740967\n",
      "[Iter.  880]  loss:2.989193  pct:-0.416520254\n",
      "[Iter.  890]  loss:2.977402  pct:-0.394446011\n",
      "[Iter.  900]  loss:2.966254  pct:-0.374411541\n",
      "[Iter.  910]  loss:2.955753  pct:-0.354044525\n",
      "[Iter.  920]  loss:2.945848  pct:-0.335088416\n",
      "[Iter.  930]  loss:2.936497  pct:-0.317422214\n",
      "[Iter.  940]  loss:2.927688  pct:-0.300010644\n",
      "[Iter.  950]  loss:2.919389  pct:-0.283461725\n",
      "[Iter.  960]  loss:2.911587  pct:-0.267248051\n",
      "[Iter.  970]  loss:2.904261  pct:-0.251611519\n",
      "[Iter.  980]  loss:2.897336  pct:-0.238421791\n",
      "[Iter.  990]  loss:2.890726  pct:-0.228170611\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.772140  pct:100.000000000\n",
      "[Iter.    2]  loss:2.772160  pct:0.000705243\n",
      "[Iter.    4]  loss:2.772170  pct:0.000352619\n",
      "[Iter.    6]  loss:2.772176  pct:0.000215011\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.772176\n",
      "Best loss: 2.772176 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 14%|█▎        | 1355/10000 [00:13<01:29, 96.81it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:4902.158691  pct:100.000000000\n",
      "[Iter.   10]  loss:800.292786  pct:-83.674682540\n",
      "[Iter.   20]  loss:775.834167  pct:-3.056208753\n",
      "[Iter.   30]  loss:753.997375  pct:-2.814621076\n",
      "[Iter.   40]  loss:734.125732  pct:-2.635505602\n",
      "[Iter.   50]  loss:715.417725  pct:-2.548338382\n",
      "[Iter.   60]  loss:697.706787  pct:-2.475607871\n",
      "[Iter.   70]  loss:680.905396  pct:-2.408087740\n",
      "[Iter.   80]  loss:664.944763  pct:-2.344030820\n",
      "[Iter.   90]  loss:649.764648  pct:-2.282913647\n",
      "[Iter.  100]  loss:635.304749  pct:-2.225405758\n",
      "[Iter.  110]  loss:621.513672  pct:-2.170781297\n",
      "[Iter.  120]  loss:608.345459  pct:-2.118732618\n",
      "[Iter.  130]  loss:595.758240  pct:-2.069090687\n",
      "[Iter.  140]  loss:583.713562  pct:-2.021739177\n",
      "[Iter.  150]  loss:572.173157  pct:-1.977066497\n",
      "[Iter.  160]  loss:561.103821  pct:-1.934612941\n",
      "[Iter.  170]  loss:550.474426  pct:-1.894372153\n",
      "[Iter.  180]  loss:540.255615  pct:-1.856364355\n",
      "[Iter.  190]  loss:530.421509  pct:-1.820269178\n",
      "[Iter.  200]  loss:520.947510  pct:-1.786126480\n",
      "[Iter.  210]  loss:511.810974  pct:-1.753830371\n",
      "[Iter.  220]  loss:502.991516  pct:-1.723186577\n",
      "[Iter.  230]  loss:494.470764  pct:-1.694015044\n",
      "[Iter.  240]  loss:486.231049  pct:-1.666370628\n",
      "[Iter.  250]  loss:478.256348  pct:-1.640105244\n",
      "[Iter.  260]  loss:470.531860  pct:-1.615135344\n",
      "[Iter.  270]  loss:463.044189  pct:-1.591320701\n",
      "[Iter.  280]  loss:455.781525  pct:-1.568460411\n",
      "[Iter.  290]  loss:448.732147  pct:-1.546657128\n",
      "[Iter.  300]  loss:441.884949  pct:-1.525898808\n",
      "[Iter.  310]  loss:435.229706  pct:-1.506103102\n",
      "[Iter.  320]  loss:428.758026  pct:-1.486957255\n",
      "[Iter.  330]  loss:422.461060  pct:-1.468652753\n",
      "[Iter.  340]  loss:416.331024  pct:-1.451029689\n",
      "[Iter.  350]  loss:410.360260  pct:-1.434138657\n",
      "[Iter.  360]  loss:404.541168  pct:-1.418044670\n",
      "[Iter.  370]  loss:398.867401  pct:-1.402519085\n",
      "[Iter.  380]  loss:393.332916  pct:-1.387550060\n",
      "[Iter.  390]  loss:387.931763  pct:-1.373176091\n",
      "[Iter.  400]  loss:382.658722  pct:-1.359270181\n",
      "[Iter.  410]  loss:377.508575  pct:-1.345885038\n",
      "[Iter.  420]  loss:372.476624  pct:-1.332937112\n",
      "[Iter.  430]  loss:367.558197  pct:-1.320465823\n",
      "[Iter.  440]  loss:362.748474  pct:-1.308560913\n",
      "[Iter.  450]  loss:358.043854  pct:-1.296937326\n",
      "[Iter.  460]  loss:353.440643  pct:-1.285655486\n",
      "[Iter.  470]  loss:348.934753  pct:-1.274864670\n",
      "[Iter.  480]  loss:344.522949  pct:-1.264363654\n",
      "[Iter.  490]  loss:340.202606  pct:-1.254007324\n",
      "[Iter.  500]  loss:335.970245  pct:-1.244070669\n",
      "[Iter.  510]  loss:331.822754  pct:-1.234481777\n",
      "[Iter.  520]  loss:327.757263  pct:-1.225199500\n",
      "[Iter.  530]  loss:323.770996  pct:-1.216225401\n",
      "[Iter.  540]  loss:319.861755  pct:-1.207409178\n",
      "[Iter.  550]  loss:316.026642  pct:-1.198990958\n",
      "[Iter.  560]  loss:312.263702  pct:-1.190703237\n",
      "[Iter.  570]  loss:308.570557  pct:-1.182700943\n",
      "[Iter.  580]  loss:304.945404  pct:-1.174821288\n",
      "[Iter.  590]  loss:301.385803  pct:-1.167291188\n",
      "[Iter.  600]  loss:297.890289  pct:-1.159813727\n",
      "[Iter.  610]  loss:294.456451  pct:-1.152718975\n",
      "[Iter.  620]  loss:291.082855  pct:-1.145702930\n",
      "[Iter.  630]  loss:287.767914  pct:-1.138830868\n",
      "[Iter.  640]  loss:284.509735  pct:-1.132224461\n",
      "[Iter.  650]  loss:281.307007  pct:-1.125700767\n",
      "[Iter.  660]  loss:278.158112  pct:-1.119380317\n",
      "[Iter.  670]  loss:275.061249  pct:-1.113346210\n",
      "[Iter.  680]  loss:272.015533  pct:-1.107286230\n",
      "[Iter.  690]  loss:269.019470  pct:-1.101430935\n",
      "[Iter.  700]  loss:266.071747  pct:-1.095728642\n",
      "[Iter.  710]  loss:263.171265  pct:-1.090112803\n",
      "[Iter.  720]  loss:260.316559  pct:-1.084733097\n",
      "[Iter.  730]  loss:257.506409  pct:-1.079512636\n",
      "[Iter.  740]  loss:254.740112  pct:-1.074263123\n",
      "[Iter.  750]  loss:252.016708  pct:-1.069091124\n",
      "[Iter.  760]  loss:249.334763  pct:-1.064193647\n",
      "[Iter.  770]  loss:246.693314  pct:-1.059398596\n",
      "[Iter.  780]  loss:244.091660  pct:-1.054610688\n",
      "[Iter.  790]  loss:241.528503  pct:-1.050079357\n",
      "[Iter.  800]  loss:239.003647  pct:-1.045365881\n",
      "[Iter.  810]  loss:236.515594  pct:-1.041010211\n",
      "[Iter.  820]  loss:234.063400  pct:-1.036800224\n",
      "[Iter.  830]  loss:231.646484  pct:-1.032590269\n",
      "[Iter.  840]  loss:229.263885  pct:-1.028549552\n",
      "[Iter.  850]  loss:226.915512  pct:-1.024310221\n",
      "[Iter.  860]  loss:224.600082  pct:-1.020392862\n",
      "[Iter.  870]  loss:222.316956  pct:-1.016529828\n",
      "[Iter.  880]  loss:220.065643  pct:-1.012658819\n",
      "[Iter.  890]  loss:217.845062  pct:-1.009053945\n",
      "[Iter.  900]  loss:215.654602  pct:-1.005512901\n",
      "[Iter.  910]  loss:213.494156  pct:-1.001808515\n",
      "[Iter.  920]  loss:211.362762  pct:-0.998338069\n",
      "[Iter.  930]  loss:209.259827  pct:-0.994941477\n",
      "[Iter.  940]  loss:207.185028  pct:-0.991493980\n",
      "[Iter.  950]  loss:205.137497  pct:-0.988262109\n",
      "[Iter.  960]  loss:203.117081  pct:-0.984908313\n",
      "[Iter.  970]  loss:201.123047  pct:-0.981716460\n",
      "[Iter.  980]  loss:199.154800  pct:-0.978628004\n",
      "[Iter.  990]  loss:197.212158  pct:-0.975443327\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.796037  pct:100.000000000\n",
      "[Iter.    2]  loss:2.794663  pct:-0.049132683\n",
      "[Iter.    4]  loss:2.794172  pct:-0.017557231\n",
      "[Iter.    6]  loss:2.793981  pct:-0.006851765\n",
      "[Iter.    8]  loss:2.793923  pct:-0.002056524\n",
      "[Iter.   10]  loss:2.793851  pct:-0.002577108\n",
      "[Iter.   12]  loss:2.793821  pct:-0.001100846\n",
      "[Iter.   14]  loss:2.793822  pct:0.000059736\n",
      "[Iter.   16]  loss:2.793780  pct:-0.001510479\n",
      "[Iter.   18]  loss:2.793797  pct:0.000605907\n",
      "[Iter.   20]  loss:2.793755  pct:-0.001519026\n",
      "[Iter.   22]  loss:2.793741  pct:-0.000486437\n",
      "[Iter.   24]  loss:2.793701  pct:-0.001442250\n",
      "[Iter.   26]  loss:2.793718  pct:0.000622993\n",
      "[Iter.   28]  loss:2.793650  pct:-0.002440751\n",
      "[Iter.   30]  loss:2.793623  pct:-0.000964376\n",
      "[Iter.   32]  loss:2.793665  pct:0.001519121\n",
      "[Iter.   34]  loss:2.793599  pct:-0.002363989\n",
      "[Iter.   36]  loss:2.793581  pct:-0.000665688\n",
      "[Iter.   38]  loss:2.793557  pct:-0.000844917\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.793557\n",
      "Best loss: 2.793557 (trial 0)\n",
      "iteration 0\n",
      "Switched off factor 1\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [03:03<00:00, 54.49it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:333.064972  pct:100.000000000\n",
      "[Iter.   10]  loss:70.461365  pct:-78.844558664\n",
      "[Iter.   20]  loss:58.949814  pct:-16.337394180\n",
      "[Iter.   30]  loss:51.054604  pct:-13.393104662\n",
      "[Iter.   40]  loss:45.018253  pct:-11.823322144\n",
      "[Iter.   50]  loss:40.188667  pct:-10.728061780\n",
      "[Iter.   60]  loss:36.203064  pct:-9.917231898\n",
      "[Iter.   70]  loss:32.839779  pct:-9.290056411\n",
      "[Iter.   80]  loss:29.953772  pct:-8.788144761\n",
      "[Iter.   90]  loss:27.444996  pct:-8.375491892\n",
      "[Iter.  100]  loss:25.241776  pct:-8.027767164\n",
      "[Iter.  110]  loss:23.290836  pct:-7.729009306\n",
      "[Iter.  120]  loss:21.551481  pct:-7.467980378\n",
      "[Iter.  130]  loss:19.992060  pct:-7.235797491\n",
      "[Iter.  140]  loss:18.587355  pct:-7.026314788\n",
      "[Iter.  150]  loss:17.316906  pct:-6.835016106\n",
      "[Iter.  160]  loss:16.164001  pct:-6.657681875\n",
      "[Iter.  170]  loss:15.114712  pct:-6.491521952\n",
      "[Iter.  180]  loss:14.157329  pct:-6.334114543\n",
      "[Iter.  190]  loss:13.281878  pct:-6.183724053\n",
      "[Iter.  200]  loss:12.479850  pct:-6.038518254\n",
      "[Iter.  210]  loss:11.743895  pct:-5.897148197\n",
      "[Iter.  220]  loss:11.067601  pct:-5.758680552\n",
      "[Iter.  230]  loss:10.445341  pct:-5.622357386\n",
      "[Iter.  240]  loss:9.872184  pct:-5.487205295\n",
      "[Iter.  250]  loss:9.343737  pct:-5.352890119\n",
      "[Iter.  260]  loss:8.856091  pct:-5.218962405\n",
      "[Iter.  270]  loss:8.405804  pct:-5.084488047\n",
      "[Iter.  280]  loss:7.989736  pct:-4.949771526\n",
      "[Iter.  290]  loss:7.605062  pct:-4.814603278\n",
      "[Iter.  300]  loss:7.249259  pct:-4.678502750\n",
      "[Iter.  310]  loss:6.920007  pct:-4.541868184\n",
      "[Iter.  320]  loss:6.615217  pct:-4.404475760\n",
      "[Iter.  330]  loss:6.332971  pct:-4.266619285\n",
      "[Iter.  340]  loss:6.071536  pct:-4.128150449\n",
      "[Iter.  350]  loss:5.829333  pct:-3.989159136\n",
      "[Iter.  360]  loss:5.604874  pct:-3.850504012\n",
      "[Iter.  370]  loss:5.396822  pct:-3.711977763\n",
      "[Iter.  380]  loss:5.203944  pct:-3.573922396\n",
      "[Iter.  390]  loss:5.025148  pct:-3.435784172\n",
      "[Iter.  400]  loss:4.859358  pct:-3.299198491\n",
      "[Iter.  410]  loss:4.705610  pct:-3.163967389\n",
      "[Iter.  420]  loss:4.563052  pct:-3.029535043\n",
      "[Iter.  430]  loss:4.430836  pct:-2.897534049\n",
      "[Iter.  440]  loss:4.308211  pct:-2.767533751\n",
      "[Iter.  450]  loss:4.194478  pct:-2.639908276\n",
      "[Iter.  460]  loss:4.089001  pct:-2.514659878\n",
      "[Iter.  470]  loss:3.991166  pct:-2.392633830\n",
      "[Iter.  480]  loss:3.900414  pct:-2.273818687\n",
      "[Iter.  490]  loss:3.816239  pct:-2.158119026\n",
      "[Iter.  500]  loss:3.738153  pct:-2.046154348\n",
      "[Iter.  510]  loss:3.665722  pct:-1.937617120\n",
      "[Iter.  520]  loss:3.598537  pct:-1.832775496\n",
      "[Iter.  530]  loss:3.536221  pct:-1.731702002\n",
      "[Iter.  540]  loss:3.478416  pct:-1.634662975\n",
      "[Iter.  550]  loss:3.424796  pct:-1.541495431\n",
      "[Iter.  560]  loss:3.375066  pct:-1.452058970\n",
      "[Iter.  570]  loss:3.328943  pct:-1.366595522\n",
      "[Iter.  580]  loss:3.286160  pct:-1.285190621\n",
      "[Iter.  590]  loss:3.246480  pct:-1.207481424\n",
      "[Iter.  600]  loss:3.209680  pct:-1.133539986\n",
      "[Iter.  610]  loss:3.175560  pct:-1.063014750\n",
      "[Iter.  620]  loss:3.143912  pct:-0.996624063\n",
      "[Iter.  630]  loss:3.114554  pct:-0.933786786\n",
      "[Iter.  640]  loss:3.087323  pct:-0.874313769\n",
      "[Iter.  650]  loss:3.062066  pct:-0.818106299\n",
      "[Iter.  660]  loss:3.038639  pct:-0.765064254\n",
      "[Iter.  670]  loss:3.016918  pct:-0.714822845\n",
      "[Iter.  680]  loss:2.996778  pct:-0.667558515\n",
      "[Iter.  690]  loss:2.978095  pct:-0.623450602\n",
      "[Iter.  700]  loss:2.960757  pct:-0.582177491\n",
      "[Iter.  710]  loss:2.944666  pct:-0.543495592\n",
      "[Iter.  720]  loss:2.929737  pct:-0.506978361\n",
      "[Iter.  730]  loss:2.915899  pct:-0.472330910\n",
      "[Iter.  740]  loss:2.903071  pct:-0.439936792\n",
      "[Iter.  750]  loss:2.891172  pct:-0.409876153\n",
      "[Iter.  760]  loss:2.880135  pct:-0.381727452\n",
      "[Iter.  770]  loss:2.869898  pct:-0.355458780\n",
      "[Iter.  780]  loss:2.860394  pct:-0.331156115\n",
      "[Iter.  790]  loss:2.851577  pct:-0.308226117\n",
      "[Iter.  800]  loss:2.843395  pct:-0.286938996\n",
      "[Iter.  810]  loss:2.835811  pct:-0.266735133\n",
      "[Iter.  820]  loss:2.828778  pct:-0.247985329\n",
      "[Iter.  830]  loss:2.822256  pct:-0.230556785\n",
      "[Iter.  840]  loss:2.816198  pct:-0.214658606\n",
      "[Iter.  850]  loss:2.810581  pct:-0.199449864\n",
      "[Iter.  860]  loss:2.805363  pct:-0.185665066\n",
      "[Iter.  870]  loss:2.800509  pct:-0.173032951\n",
      "[Iter.  880]  loss:2.795998  pct:-0.161065061\n",
      "[Iter.  890]  loss:2.791813  pct:-0.149685357\n",
      "[Iter.  900]  loss:2.787925  pct:-0.139251930\n",
      "[Iter.  910]  loss:2.784315  pct:-0.129508887\n",
      "[Iter.  920]  loss:2.780964  pct:-0.120343250\n",
      "[Iter.  930]  loss:2.777852  pct:-0.111906441\n",
      "[Iter.  940]  loss:2.774961  pct:-0.104058353\n",
      "[Iter.  950]  loss:2.772282  pct:-0.096545838\n",
      "[Iter.  960]  loss:2.769797  pct:-0.089630071\n",
      "[Iter.  970]  loss:2.767482  pct:-0.083581726\n",
      "[Iter.  980]  loss:2.765329  pct:-0.077810674\n",
      "[Iter.  990]  loss:2.763326  pct:-0.072422346\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.733457  pct:100.000000000\n",
      "[Iter.    2]  loss:2.733460  pct:0.000130834\n",
      "[Iter.    4]  loss:2.733461  pct:0.000043611\n",
      "[Iter.    6]  loss:2.733462  pct:0.000034889\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.733462\n",
      "Best loss: 2.733462 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 11%|█         | 1078/10000 [00:13<01:50, 80.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:260.783630  pct:100.000000000\n",
      "[Iter.   10]  loss:56.869953  pct:-78.192669049\n",
      "[Iter.   20]  loss:46.409828  pct:-18.393060639\n",
      "[Iter.   30]  loss:39.648190  pct:-14.569411061\n",
      "[Iter.   40]  loss:34.567783  pct:-12.813715449\n",
      "[Iter.   50]  loss:30.553856  pct:-11.611758320\n",
      "[Iter.   60]  loss:27.277328  pct:-10.723777110\n",
      "[Iter.   70]  loss:24.539633  pct:-10.036524269\n",
      "[Iter.   80]  loss:22.212023  pct:-9.485105320\n",
      "[Iter.   90]  loss:20.206488  pct:-9.029052174\n",
      "[Iter.  100]  loss:18.460327  pct:-8.641583520\n",
      "[Iter.  110]  loss:16.927223  pct:-8.304857929\n",
      "[Iter.  120]  loss:15.572250  pct:-8.004696476\n",
      "[Iter.  130]  loss:14.368083  pct:-7.732776816\n",
      "[Iter.  140]  loss:13.293052  pct:-7.482078719\n",
      "[Iter.  150]  loss:12.329663  pct:-7.247308318\n",
      "[Iter.  160]  loss:11.463564  pct:-7.024517525\n",
      "[Iter.  170]  loss:10.682745  pct:-6.811310555\n",
      "[Iter.  180]  loss:9.977338  pct:-6.603238624\n",
      "[Iter.  190]  loss:9.338774  pct:-6.400145211\n",
      "[Iter.  200]  loss:8.759770  pct:-6.199993178\n",
      "[Iter.  210]  loss:8.234007  pct:-6.002023889\n",
      "[Iter.  220]  loss:7.756009  pct:-5.805171936\n",
      "[Iter.  230]  loss:7.320975  pct:-5.608984497\n",
      "[Iter.  240]  loss:6.924765  pct:-5.411992993\n",
      "[Iter.  250]  loss:6.563627  pct:-5.215157616\n",
      "[Iter.  260]  loss:6.234254  pct:-5.018153388\n",
      "[Iter.  270]  loss:5.933690  pct:-4.821182271\n",
      "[Iter.  280]  loss:5.659299  pct:-4.624276621\n",
      "[Iter.  290]  loss:5.408739  pct:-4.427416610\n",
      "[Iter.  300]  loss:5.179885  pct:-4.231175549\n",
      "[Iter.  310]  loss:4.970847  pct:-4.035576890\n",
      "[Iter.  320]  loss:4.779822  pct:-3.842911522\n",
      "[Iter.  330]  loss:4.605186  pct:-3.653596618\n",
      "[Iter.  340]  loss:4.445548  pct:-3.466502019\n",
      "[Iter.  350]  loss:4.299666  pct:-3.281522696\n",
      "[Iter.  360]  loss:4.166331  pct:-3.101057518\n",
      "[Iter.  370]  loss:4.044484  pct:-2.924567398\n",
      "[Iter.  380]  loss:3.933135  pct:-2.753092852\n",
      "[Iter.  390]  loss:3.831320  pct:-2.588659049\n",
      "[Iter.  400]  loss:3.738228  pct:-2.429768534\n",
      "[Iter.  410]  loss:3.653088  pct:-2.277536897\n",
      "[Iter.  420]  loss:3.575318  pct:-2.128883639\n",
      "[Iter.  430]  loss:3.504253  pct:-1.987647215\n",
      "[Iter.  440]  loss:3.439188  pct:-1.856761345\n",
      "[Iter.  450]  loss:3.379884  pct:-1.724360605\n",
      "[Iter.  460]  loss:3.325776  pct:-1.600887681\n",
      "[Iter.  470]  loss:3.276363  pct:-1.485750482\n",
      "[Iter.  480]  loss:3.230887  pct:-1.388008583\n",
      "[Iter.  490]  loss:3.189475  pct:-1.281756959\n",
      "[Iter.  500]  loss:3.151676  pct:-1.185105638\n",
      "[Iter.  510]  loss:3.117150  pct:-1.095476564\n",
      "[Iter.  520]  loss:3.085593  pct:-1.012369711\n",
      "[Iter.  530]  loss:3.056695  pct:-0.936538372\n",
      "[Iter.  540]  loss:3.030309  pct:-0.863228391\n",
      "[Iter.  550]  loss:3.006086  pct:-0.799344644\n",
      "[Iter.  560]  loss:2.983833  pct:-0.740266044\n",
      "[Iter.  570]  loss:2.962983  pct:-0.698779651\n",
      "[Iter.  580]  loss:2.943747  pct:-0.649205550\n",
      "[Iter.  590]  loss:2.926230  pct:-0.595052875\n",
      "[Iter.  600]  loss:2.910147  pct:-0.549631364\n",
      "[Iter.  610]  loss:2.895302  pct:-0.510099918\n",
      "[Iter.  620]  loss:2.881628  pct:-0.472299516\n",
      "[Iter.  630]  loss:2.869043  pct:-0.436704924\n",
      "[Iter.  640]  loss:2.857467  pct:-0.403477252\n",
      "[Iter.  650]  loss:2.846731  pct:-0.375733610\n",
      "[Iter.  660]  loss:2.836711  pct:-0.351966553\n",
      "[Iter.  670]  loss:2.827522  pct:-0.323935979\n",
      "[Iter.  680]  loss:2.819034  pct:-0.300215321\n",
      "[Iter.  690]  loss:2.811184  pct:-0.278461798\n",
      "[Iter.  700]  loss:2.803936  pct:-0.257816179\n",
      "[Iter.  710]  loss:2.797244  pct:-0.238653602\n",
      "[Iter.  720]  loss:2.791069  pct:-0.220754447\n",
      "[Iter.  730]  loss:2.785374  pct:-0.204047412\n",
      "[Iter.  740]  loss:2.780105  pct:-0.189185622\n",
      "[Iter.  750]  loss:2.775209  pct:-0.176105800\n",
      "[Iter.  760]  loss:2.770681  pct:-0.163134772\n",
      "[Iter.  770]  loss:2.766503  pct:-0.150803540\n",
      "[Iter.  780]  loss:2.762640  pct:-0.139629622\n",
      "[Iter.  790]  loss:2.759053  pct:-0.129831205\n",
      "[Iter.  800]  loss:2.755736  pct:-0.120252579\n",
      "[Iter.  810]  loss:2.752670  pct:-0.111261142\n",
      "[Iter.  820]  loss:2.749845  pct:-0.102619775\n",
      "[Iter.  830]  loss:2.747210  pct:-0.095806327\n",
      "[Iter.  840]  loss:2.744762  pct:-0.089111562\n",
      "[Iter.  850]  loss:2.742494  pct:-0.082624190\n",
      "[Iter.  860]  loss:2.740395  pct:-0.076537520\n",
      "[Iter.  870]  loss:2.738458  pct:-0.070697442\n",
      "[Iter.  880]  loss:2.736675  pct:-0.065123183\n",
      "[Iter.  890]  loss:2.735052  pct:-0.059276322\n",
      "[Iter.  900]  loss:2.733632  pct:-0.051936772\n",
      "[Iter.  910]  loss:2.732410  pct:-0.044698602\n",
      "[Iter.  920]  loss:2.731100  pct:-0.047955780\n",
      "[Iter.  930]  loss:2.729863  pct:-0.045263832\n",
      "[Iter.  940]  loss:2.728683  pct:-0.043258107\n",
      "[Iter.  950]  loss:2.727531  pct:-0.042210853\n",
      "[Iter.  960]  loss:2.726535  pct:-0.036494459\n",
      "[Iter.  970]  loss:2.725610  pct:-0.033928190\n",
      "[Iter.  980]  loss:2.724749  pct:-0.031586669\n",
      "[Iter.  990]  loss:2.723949  pct:-0.029382871\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.711388  pct:100.000000000\n",
      "[Iter.    2]  loss:2.711367  pct:-0.000756218\n",
      "[Iter.    4]  loss:2.711364  pct:-0.000131899\n",
      "[Iter.    6]  loss:2.711363  pct:-0.000026380\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.711363\n",
      "Best loss: 2.711363 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 20%|██        | 2028/10000 [00:27<01:48, 73.21it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:409.759491  pct:100.000000000\n",
      "[Iter.   10]  loss:67.359451  pct:-83.561218706\n",
      "[Iter.   20]  loss:58.082382  pct:-13.772483168\n",
      "[Iter.   30]  loss:51.253010  pct:-11.758079037\n",
      "[Iter.   40]  loss:45.754284  pct:-10.728591185\n",
      "[Iter.   50]  loss:41.202091  pct:-9.949216334\n",
      "[Iter.   60]  loss:37.354496  pct:-9.338349344\n",
      "[Iter.   70]  loss:34.050102  pct:-8.846040295\n",
      "[Iter.   80]  loss:31.176420  pct:-8.439569439\n",
      "[Iter.   90]  loss:28.652046  pct:-8.097061789\n",
      "[Iter.  100]  loss:26.416149  pct:-7.803620894\n",
      "[Iter.  110]  loss:24.422279  pct:-7.547919914\n",
      "[Iter.  120]  loss:22.634209  pct:-7.321473367\n",
      "[Iter.  130]  loss:21.022976  pct:-7.118573397\n",
      "[Iter.  140]  loss:19.565161  pct:-6.934390144\n",
      "[Iter.  150]  loss:18.241631  pct:-6.764729480\n",
      "[Iter.  160]  loss:17.036392  pct:-6.607075715\n",
      "[Iter.  170]  loss:15.936044  pct:-6.458811578\n",
      "[Iter.  180]  loss:14.929215  pct:-6.317931380\n",
      "[Iter.  190]  loss:14.006156  pct:-6.182906716\n",
      "[Iter.  200]  loss:13.158520  pct:-6.051883363\n",
      "[Iter.  210]  loss:12.378941  pct:-5.924520217\n",
      "[Iter.  220]  loss:11.661014  pct:-5.799583367\n",
      "[Iter.  230]  loss:10.999128  pct:-5.676052563\n",
      "[Iter.  240]  loss:10.388280  pct:-5.553607594\n",
      "[Iter.  250]  loss:9.824011  pct:-5.431785344\n",
      "[Iter.  260]  loss:9.302443  pct:-5.309117695\n",
      "[Iter.  270]  loss:8.819955  pct:-5.186677326\n",
      "[Iter.  280]  loss:8.373353  pct:-5.063539147\n",
      "[Iter.  290]  loss:7.959779  pct:-4.939170945\n",
      "[Iter.  300]  loss:7.576406  pct:-4.816374851\n",
      "[Iter.  310]  loss:7.221196  pct:-4.688374728\n",
      "[Iter.  320]  loss:6.891984  pct:-4.558970598\n",
      "[Iter.  330]  loss:6.586647  pct:-4.430320412\n",
      "[Iter.  340]  loss:6.303380  pct:-4.300611321\n",
      "[Iter.  350]  loss:6.040544  pct:-4.169769804\n",
      "[Iter.  360]  loss:5.796685  pct:-4.037033964\n",
      "[Iter.  370]  loss:5.570333  pct:-3.904864299\n",
      "[Iter.  380]  loss:5.360213  pct:-3.772121740\n",
      "[Iter.  390]  loss:5.165144  pct:-3.639190580\n",
      "[Iter.  400]  loss:4.984025  pct:-3.506561474\n",
      "[Iter.  410]  loss:4.815818  pct:-3.374925937\n",
      "[Iter.  420]  loss:4.659595  pct:-3.243961609\n",
      "[Iter.  430]  loss:4.514492  pct:-3.114068441\n",
      "[Iter.  440]  loss:4.379734  pct:-2.985019608\n",
      "[Iter.  450]  loss:4.254608  pct:-2.856929154\n",
      "[Iter.  460]  loss:4.138387  pct:-2.731649071\n",
      "[Iter.  470]  loss:4.030464  pct:-2.607841199\n",
      "[Iter.  480]  loss:3.930250  pct:-2.486425312\n",
      "[Iter.  490]  loss:3.837180  pct:-2.368025502\n",
      "[Iter.  500]  loss:3.750792  pct:-2.251343504\n",
      "[Iter.  510]  loss:3.670607  pct:-2.137813317\n",
      "[Iter.  520]  loss:3.596180  pct:-2.027651589\n",
      "[Iter.  530]  loss:3.527102  pct:-1.920890500\n",
      "[Iter.  540]  loss:3.463007  pct:-1.817208310\n",
      "[Iter.  550]  loss:3.403566  pct:-1.716445198\n",
      "[Iter.  560]  loss:3.348409  pct:-1.620556375\n",
      "[Iter.  570]  loss:3.297289  pct:-1.526717666\n",
      "[Iter.  580]  loss:3.249872  pct:-1.438036372\n",
      "[Iter.  590]  loss:3.205741  pct:-1.357953475\n",
      "[Iter.  600]  loss:3.164747  pct:-1.278743904\n",
      "[Iter.  610]  loss:3.126763  pct:-1.200226339\n",
      "[Iter.  620]  loss:3.091598  pct:-1.124670558\n",
      "[Iter.  630]  loss:3.059040  pct:-1.053103624\n",
      "[Iter.  640]  loss:3.028838  pct:-0.987300382\n",
      "[Iter.  650]  loss:3.000834  pct:-0.924592488\n",
      "[Iter.  660]  loss:2.974821  pct:-0.866847793\n",
      "[Iter.  670]  loss:2.950740  pct:-0.809485832\n",
      "[Iter.  680]  loss:2.928493  pct:-0.753965179\n",
      "[Iter.  690]  loss:2.907912  pct:-0.702768740\n",
      "[Iter.  700]  loss:2.888836  pct:-0.655998889\n",
      "[Iter.  710]  loss:2.871047  pct:-0.615796915\n",
      "[Iter.  720]  loss:2.854536  pct:-0.575068533\n",
      "[Iter.  730]  loss:2.839265  pct:-0.534987951\n",
      "[Iter.  740]  loss:2.825101  pct:-0.498868898\n",
      "[Iter.  750]  loss:2.811929  pct:-0.466237292\n",
      "[Iter.  760]  loss:2.799621  pct:-0.437702458\n",
      "[Iter.  770]  loss:2.788271  pct:-0.405409019\n",
      "[Iter.  780]  loss:2.777531  pct:-0.385203513\n",
      "[Iter.  790]  loss:2.767446  pct:-0.363070361\n",
      "[Iter.  800]  loss:2.758029  pct:-0.340296899\n",
      "[Iter.  810]  loss:2.749147  pct:-0.322025973\n",
      "[Iter.  820]  loss:2.740937  pct:-0.298627350\n",
      "[Iter.  830]  loss:2.733310  pct:-0.278271535\n",
      "[Iter.  840]  loss:2.726202  pct:-0.260049995\n",
      "[Iter.  850]  loss:2.719566  pct:-0.243429524\n",
      "[Iter.  860]  loss:2.713372  pct:-0.227761157\n",
      "[Iter.  870]  loss:2.707583  pct:-0.213334744\n",
      "[Iter.  880]  loss:2.702173  pct:-0.199807563\n",
      "[Iter.  890]  loss:2.697128  pct:-0.186716895\n",
      "[Iter.  900]  loss:2.692399  pct:-0.175335870\n",
      "[Iter.  910]  loss:2.687946  pct:-0.165389460\n",
      "[Iter.  920]  loss:2.683764  pct:-0.155560613\n",
      "[Iter.  930]  loss:2.679846  pct:-0.146004219\n",
      "[Iter.  940]  loss:2.676172  pct:-0.137089662\n",
      "[Iter.  950]  loss:2.672725  pct:-0.128814363\n",
      "[Iter.  960]  loss:2.669491  pct:-0.120996723\n",
      "[Iter.  970]  loss:2.666455  pct:-0.113739306\n",
      "[Iter.  980]  loss:2.663602  pct:-0.107001819\n",
      "[Iter.  990]  loss:2.660926  pct:-0.100465854\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.615214  pct:100.000000000\n",
      "[Iter.    2]  loss:2.615167  pct:-0.001805087\n",
      "[Iter.    4]  loss:2.615156  pct:-0.000419371\n",
      "[Iter.    6]  loss:2.615152  pct:-0.000127635\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.615152\n",
      "Best loss: 2.615152 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 20%|██        | 2033/10000 [00:34<02:16, 58.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:329.095795  pct:100.000000000\n",
      "[Iter.   10]  loss:73.834587  pct:-77.564406498\n",
      "[Iter.   20]  loss:60.415661  pct:-18.174309313\n",
      "[Iter.   30]  loss:52.004814  pct:-13.921633217\n",
      "[Iter.   40]  loss:45.664219  pct:-12.192323632\n",
      "[Iter.   50]  loss:40.638535  pct:-11.005738141\n",
      "[Iter.   60]  loss:36.519684  pct:-10.135332767\n",
      "[Iter.   70]  loss:33.062504  pct:-9.466620901\n",
      "[Iter.   80]  loss:30.108488  pct:-8.934640124\n",
      "[Iter.   90]  loss:27.549545  pct:-8.499074373\n",
      "[Iter.  100]  loss:25.308699  pct:-8.133878837\n",
      "[Iter.  110]  loss:23.329140  pct:-7.821654411\n",
      "[Iter.  120]  loss:21.567892  pct:-7.549561007\n",
      "[Iter.  130]  loss:19.991562  pct:-7.308689136\n",
      "[Iter.  140]  loss:18.573826  pct:-7.091672283\n",
      "[Iter.  150]  loss:17.293436  pct:-6.893516700\n",
      "[Iter.  160]  loss:16.132998  pct:-6.710283221\n",
      "[Iter.  170]  loss:15.078066  pct:-6.538968594\n",
      "[Iter.  180]  loss:14.116541  pct:-6.376978132\n",
      "[Iter.  190]  loss:13.238200  pct:-6.222067621\n",
      "[Iter.  200]  loss:12.434265  pct:-6.072842528\n",
      "[Iter.  210]  loss:11.697137  pct:-5.928201222\n",
      "[Iter.  220]  loss:11.020228  pct:-5.786958809\n",
      "[Iter.  230]  loss:10.397852  pct:-5.647582066\n",
      "[Iter.  240]  loss:9.824965  pct:-5.509670880\n",
      "[Iter.  250]  loss:9.297193  pct:-5.371744076\n",
      "[Iter.  260]  loss:8.810570  pct:-5.234083370\n",
      "[Iter.  270]  loss:8.361529  pct:-5.096610378\n",
      "[Iter.  280]  loss:7.946929  pct:-4.958433222\n",
      "[Iter.  290]  loss:7.563929  pct:-4.819470780\n",
      "[Iter.  300]  loss:7.209950  pct:-4.679825160\n",
      "[Iter.  310]  loss:6.882714  pct:-4.538674705\n",
      "[Iter.  320]  loss:6.580151  pct:-4.395979880\n",
      "[Iter.  330]  loss:6.300327  pct:-4.252550640\n",
      "[Iter.  340]  loss:6.041609  pct:-4.106422111\n",
      "[Iter.  350]  loss:5.802286  pct:-3.961240621\n",
      "[Iter.  360]  loss:5.581004  pct:-3.813703765\n",
      "[Iter.  370]  loss:5.376455  pct:-3.665098741\n",
      "[Iter.  380]  loss:5.187424  pct:-3.515897617\n",
      "[Iter.  390]  loss:5.012893  pct:-3.364511049\n",
      "[Iter.  400]  loss:4.851853  pct:-3.212503465\n",
      "[Iter.  410]  loss:4.703410  pct:-3.059515833\n",
      "[Iter.  420]  loss:4.566763  pct:-2.905279789\n",
      "[Iter.  430]  loss:4.441212  pct:-2.749228480\n",
      "[Iter.  440]  loss:4.326167  pct:-2.590397983\n",
      "[Iter.  450]  loss:4.220352  pct:-2.445928027\n",
      "[Iter.  460]  loss:4.122952  pct:-2.307857440\n",
      "[Iter.  470]  loss:4.033257  pct:-2.175515107\n",
      "[Iter.  480]  loss:3.950185  pct:-2.059668117\n",
      "[Iter.  490]  loss:3.873245  pct:-1.947764276\n",
      "[Iter.  500]  loss:3.801584  pct:-1.850166264\n",
      "[Iter.  510]  loss:3.734775  pct:-1.757397634\n",
      "[Iter.  520]  loss:3.672385  pct:-1.670499043\n",
      "[Iter.  530]  loss:3.614033  pct:-1.588946381\n",
      "[Iter.  540]  loss:3.559416  pct:-1.511252575\n",
      "[Iter.  550]  loss:3.508265  pct:-1.437062705\n",
      "[Iter.  560]  loss:3.460325  pct:-1.366494755\n",
      "[Iter.  570]  loss:3.415379  pct:-1.298873550\n",
      "[Iter.  580]  loss:3.373211  pct:-1.234661667\n",
      "[Iter.  590]  loss:3.333641  pct:-1.173076118\n",
      "[Iter.  600]  loss:3.296480  pct:-1.114702000\n",
      "[Iter.  610]  loss:3.261593  pct:-1.058320069\n",
      "[Iter.  620]  loss:3.228832  pct:-1.004436148\n",
      "[Iter.  630]  loss:3.198065  pct:-0.952889397\n",
      "[Iter.  640]  loss:3.169165  pct:-0.903683395\n",
      "[Iter.  650]  loss:3.142025  pct:-0.856388981\n",
      "[Iter.  660]  loss:3.116517  pct:-0.811808168\n",
      "[Iter.  670]  loss:3.092556  pct:-0.768848791\n",
      "[Iter.  680]  loss:3.070049  pct:-0.727770617\n",
      "[Iter.  690]  loss:3.048894  pct:-0.689072994\n",
      "[Iter.  700]  loss:3.029006  pct:-0.652299540\n",
      "[Iter.  710]  loss:3.010313  pct:-0.617147808\n",
      "[Iter.  720]  loss:2.992746  pct:-0.583573734\n",
      "[Iter.  730]  loss:2.976238  pct:-0.551603925\n",
      "[Iter.  740]  loss:2.960725  pct:-0.521218641\n",
      "[Iter.  750]  loss:2.946137  pct:-0.492713103\n",
      "[Iter.  760]  loss:2.932421  pct:-0.465542024\n",
      "[Iter.  770]  loss:2.919535  pct:-0.439449937\n",
      "[Iter.  780]  loss:2.907418  pct:-0.415012408\n",
      "[Iter.  790]  loss:2.896029  pct:-0.391730862\n",
      "[Iter.  800]  loss:2.885324  pct:-0.369668556\n",
      "[Iter.  810]  loss:2.875264  pct:-0.348630515\n",
      "[Iter.  820]  loss:2.865809  pct:-0.328854710\n",
      "[Iter.  830]  loss:2.856912  pct:-0.310439021\n",
      "[Iter.  840]  loss:2.848542  pct:-0.292979271\n",
      "[Iter.  850]  loss:2.840669  pct:-0.276397404\n",
      "[Iter.  860]  loss:2.833265  pct:-0.260629181\n",
      "[Iter.  870]  loss:2.826306  pct:-0.245624718\n",
      "[Iter.  880]  loss:2.819761  pct:-0.231593586\n",
      "[Iter.  890]  loss:2.813606  pct:-0.218256077\n",
      "[Iter.  900]  loss:2.807813  pct:-0.205904185\n",
      "[Iter.  910]  loss:2.802365  pct:-0.194016705\n",
      "[Iter.  920]  loss:2.797237  pct:-0.183010475\n",
      "[Iter.  930]  loss:2.792415  pct:-0.172359405\n",
      "[Iter.  940]  loss:2.787878  pct:-0.162505222\n",
      "[Iter.  950]  loss:2.783610  pct:-0.153080344\n",
      "[Iter.  960]  loss:2.779594  pct:-0.144253178\n",
      "[Iter.  970]  loss:2.775816  pct:-0.135926997\n",
      "[Iter.  980]  loss:2.772257  pct:-0.128210006\n",
      "[Iter.  990]  loss:2.768911  pct:-0.120720453\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.712241  pct:100.000000000\n",
      "[Iter.    2]  loss:2.712229  pct:-0.000465895\n",
      "[Iter.    4]  loss:2.712225  pct:-0.000114277\n",
      "[Iter.    6]  loss:2.712224  pct:-0.000052743\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.712224\n",
      "Best loss: 2.712224 (trial 0)\n",
      "iteration 0\n",
      "Switched off factor 1\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  9%|▉         | 908/10000 [00:16<02:45, 54.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:466.084808  pct:100.000000000\n",
      "[Iter.   10]  loss:92.789688  pct:-80.091669647\n",
      "[Iter.   20]  loss:79.006073  pct:-14.854684171\n",
      "[Iter.   30]  loss:69.806107  pct:-11.644631965\n",
      "[Iter.   40]  loss:62.563217  pct:-10.375724647\n",
      "[Iter.   50]  loss:56.629280  pct:-9.484705777\n",
      "[Iter.   60]  loss:51.644066  pct:-8.803244939\n",
      "[Iter.   70]  loss:47.375847  pct:-8.264684283\n",
      "[Iter.   80]  loss:43.667210  pct:-7.828118088\n",
      "[Iter.   90]  loss:40.406750  pct:-7.466609220\n",
      "[Iter.  100]  loss:37.512798  pct:-7.162049498\n",
      "[Iter.  110]  loss:34.923901  pct:-6.901371856\n",
      "[Iter.  120]  loss:32.592731  pct:-6.674996458\n",
      "[Iter.  130]  loss:30.481981  pct:-6.476137785\n",
      "[Iter.  140]  loss:28.561800  pct:-6.299397854\n",
      "[Iter.  150]  loss:26.807896  pct:-6.140734626\n",
      "[Iter.  160]  loss:25.200245  pct:-5.996930073\n",
      "[Iter.  170]  loss:23.722204  pct:-5.865183854\n",
      "[Iter.  180]  loss:22.359619  pct:-5.743922680\n",
      "[Iter.  190]  loss:21.100498  pct:-5.631227139\n",
      "[Iter.  200]  loss:19.934546  pct:-5.525711628\n",
      "[Iter.  210]  loss:18.852867  pct:-5.426150245\n",
      "[Iter.  220]  loss:17.847868  pct:-5.330749716\n",
      "[Iter.  230]  loss:16.912819  pct:-5.238995822\n",
      "[Iter.  240]  loss:16.041616  pct:-5.151136978\n",
      "[Iter.  250]  loss:15.228989  pct:-5.065747554\n",
      "[Iter.  260]  loss:14.470205  pct:-4.982493309\n",
      "[Iter.  270]  loss:13.761076  pct:-4.900616947\n",
      "[Iter.  280]  loss:13.097805  pct:-4.819906173\n",
      "[Iter.  290]  loss:12.476914  pct:-4.740417316\n",
      "[Iter.  300]  loss:11.895188  pct:-4.662419371\n",
      "[Iter.  310]  loss:11.350097  pct:-4.582454803\n",
      "[Iter.  320]  loss:10.838909  pct:-4.503816723\n",
      "[Iter.  330]  loss:10.359352  pct:-4.424403146\n",
      "[Iter.  340]  loss:9.909256  pct:-4.344828958\n",
      "[Iter.  350]  loss:9.486671  pct:-4.264543519\n",
      "[Iter.  360]  loss:9.089785  pct:-4.183625708\n",
      "[Iter.  370]  loss:8.716938  pct:-4.101819998\n",
      "[Iter.  380]  loss:8.366594  pct:-4.019114321\n",
      "[Iter.  390]  loss:8.037275  pct:-3.936117706\n",
      "[Iter.  400]  loss:7.727541  pct:-3.853723213\n",
      "[Iter.  410]  loss:7.436130  pct:-3.771075443\n",
      "[Iter.  420]  loss:7.162277  pct:-3.682726958\n",
      "[Iter.  430]  loss:6.904878  pct:-3.593816243\n",
      "[Iter.  440]  loss:6.662821  pct:-3.505591869\n",
      "[Iter.  450]  loss:6.435158  pct:-3.416916501\n",
      "[Iter.  460]  loss:6.221066  pct:-3.326915147\n",
      "[Iter.  470]  loss:6.019626  pct:-3.238027977\n",
      "[Iter.  480]  loss:5.830080  pct:-3.148801999\n",
      "[Iter.  490]  loss:5.651740  pct:-3.058962436\n",
      "[Iter.  500]  loss:5.483976  pct:-2.968355017\n",
      "[Iter.  510]  loss:5.326227  pct:-2.876556031\n",
      "[Iter.  520]  loss:5.177885  pct:-2.785126143\n",
      "[Iter.  530]  loss:5.038376  pct:-2.694309718\n",
      "[Iter.  540]  loss:4.907133  pct:-2.604880960\n",
      "[Iter.  550]  loss:4.783678  pct:-2.515828632\n",
      "[Iter.  560]  loss:4.667590  pct:-2.426740406\n",
      "[Iter.  570]  loss:4.558423  pct:-2.338842369\n",
      "[Iter.  580]  loss:4.455563  pct:-2.256471303\n",
      "[Iter.  590]  loss:4.358678  pct:-2.174466526\n",
      "[Iter.  600]  loss:4.267531  pct:-2.091171123\n",
      "[Iter.  610]  loss:4.181696  pct:-2.011349927\n",
      "[Iter.  620]  loss:4.100860  pct:-1.933086969\n",
      "[Iter.  630]  loss:4.024754  pct:-1.855856315\n",
      "[Iter.  640]  loss:3.953063  pct:-1.781246679\n",
      "[Iter.  650]  loss:3.885424  pct:-1.711049629\n",
      "[Iter.  660]  loss:3.821705  pct:-1.639968850\n",
      "[Iter.  670]  loss:3.761577  pct:-1.573316167\n",
      "[Iter.  680]  loss:3.704882  pct:-1.507219470\n",
      "[Iter.  690]  loss:3.651364  pct:-1.444528010\n",
      "[Iter.  700]  loss:3.600942  pct:-1.380900859\n",
      "[Iter.  710]  loss:3.553364  pct:-1.321268007\n",
      "[Iter.  720]  loss:3.508480  pct:-1.263133401\n",
      "[Iter.  730]  loss:3.466200  pct:-1.205092571\n",
      "[Iter.  740]  loss:3.426325  pct:-1.150389289\n",
      "[Iter.  750]  loss:3.388869  pct:-1.093190901\n",
      "[Iter.  760]  loss:3.353503  pct:-1.043587755\n",
      "[Iter.  770]  loss:3.320116  pct:-0.995598414\n",
      "[Iter.  780]  loss:3.288609  pct:-0.948959088\n",
      "[Iter.  790]  loss:3.258871  pct:-0.904271354\n",
      "[Iter.  800]  loss:3.230776  pct:-0.862101257\n",
      "[Iter.  810]  loss:3.204345  pct:-0.818117938\n",
      "[Iter.  820]  loss:3.179344  pct:-0.780208566\n",
      "[Iter.  830]  loss:3.155619  pct:-0.746216612\n",
      "[Iter.  840]  loss:3.133345  pct:-0.705867580\n",
      "[Iter.  850]  loss:3.112467  pct:-0.666326744\n",
      "[Iter.  860]  loss:3.092646  pct:-0.636816161\n",
      "[Iter.  870]  loss:3.073740  pct:-0.611301841\n",
      "[Iter.  880]  loss:3.055794  pct:-0.583872328\n",
      "[Iter.  890]  loss:3.038836  pct:-0.554922365\n",
      "[Iter.  900]  loss:3.022840  pct:-0.526400688\n",
      "[Iter.  910]  loss:3.007895  pct:-0.494411498\n",
      "[Iter.  920]  loss:2.993949  pct:-0.463632563\n",
      "[Iter.  930]  loss:2.980503  pct:-0.449116877\n",
      "[Iter.  940]  loss:2.967758  pct:-0.427609173\n",
      "[Iter.  950]  loss:2.955723  pct:-0.405513349\n",
      "[Iter.  960]  loss:2.944396  pct:-0.383239702\n",
      "[Iter.  970]  loss:2.933811  pct:-0.359474581\n",
      "[Iter.  980]  loss:2.924036  pct:-0.333197972\n",
      "[Iter.  990]  loss:2.914573  pct:-0.323630480\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.725604  pct:100.000000000\n",
      "[Iter.    2]  loss:2.725573  pct:-0.001128410\n",
      "[Iter.    4]  loss:2.725575  pct:0.000052485\n",
      "[Iter.    6]  loss:2.725587  pct:0.000437373\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.725587\n",
      "Best loss: 2.725587 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 21%|██▏       | 2148/10000 [00:21<01:19, 98.79it/s] \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:783.773193  pct:100.000000000\n",
      "[Iter.   10]  loss:138.720718  pct:-82.300912522\n",
      "[Iter.   20]  loss:125.354836  pct:-9.635102117\n",
      "[Iter.   30]  loss:114.498886  pct:-8.660176018\n",
      "[Iter.   40]  loss:105.407349  pct:-7.940284648\n",
      "[Iter.   50]  loss:97.631973  pct:-7.376502177\n",
      "[Iter.   60]  loss:90.874809  pct:-6.921056469\n",
      "[Iter.   70]  loss:84.926872  pct:-6.545198895\n",
      "[Iter.   80]  loss:79.635818  pct:-6.230129088\n",
      "[Iter.   90]  loss:74.887703  pct:-5.962286356\n",
      "[Iter.  100]  loss:70.595131  pct:-5.732011870\n",
      "[Iter.  110]  loss:66.689957  pct:-5.531789805\n",
      "[Iter.  120]  loss:63.117847  pct:-5.356292613\n",
      "[Iter.  130]  loss:59.834972  pct:-5.201183491\n",
      "[Iter.  140]  loss:56.805698  pct:-5.062714774\n",
      "[Iter.  150]  loss:54.000244  pct:-4.938684557\n",
      "[Iter.  160]  loss:51.393730  pct:-4.826855912\n",
      "[Iter.  170]  loss:48.965172  pct:-4.725398102\n",
      "[Iter.  180]  loss:46.696651  pct:-4.632926366\n",
      "[Iter.  190]  loss:44.572750  pct:-4.548294794\n",
      "[Iter.  200]  loss:42.580101  pct:-4.470554485\n",
      "[Iter.  210]  loss:40.707161  pct:-4.398627572\n",
      "[Iter.  220]  loss:38.943649  pct:-4.332190250\n",
      "[Iter.  230]  loss:37.280777  pct:-4.269944766\n",
      "[Iter.  240]  loss:35.710537  pct:-4.211929439\n",
      "[Iter.  250]  loss:34.225937  pct:-4.157316562\n",
      "[Iter.  260]  loss:32.820644  pct:-4.105928540\n",
      "[Iter.  270]  loss:31.488914  pct:-4.057598241\n",
      "[Iter.  280]  loss:30.225660  pct:-4.011742501\n",
      "[Iter.  290]  loss:29.026379  pct:-3.967760107\n",
      "[Iter.  300]  loss:27.886759  pct:-3.926152283\n",
      "[Iter.  310]  loss:26.803036  pct:-3.886156422\n",
      "[Iter.  320]  loss:25.771694  pct:-3.847853515\n",
      "[Iter.  330]  loss:24.789694  pct:-3.810383376\n",
      "[Iter.  340]  loss:23.853956  pct:-3.774704182\n",
      "[Iter.  350]  loss:22.961893  pct:-3.739686334\n",
      "[Iter.  360]  loss:22.110929  pct:-3.705981861\n",
      "[Iter.  370]  loss:21.298826  pct:-3.672859035\n",
      "[Iter.  380]  loss:20.523453  pct:-3.640451595\n",
      "[Iter.  390]  loss:19.782837  pct:-3.608631810\n",
      "[Iter.  400]  loss:19.075163  pct:-3.577212053\n",
      "[Iter.  410]  loss:18.398720  pct:-3.546198289\n",
      "[Iter.  420]  loss:17.751873  pct:-3.515716195\n",
      "[Iter.  430]  loss:17.133192  pct:-3.485158740\n",
      "[Iter.  440]  loss:16.541237  pct:-3.455019840\n",
      "[Iter.  450]  loss:15.974694  pct:-3.425031814\n",
      "[Iter.  460]  loss:15.432360  pct:-3.394960480\n",
      "[Iter.  470]  loss:14.913086  pct:-3.364837058\n",
      "[Iter.  480]  loss:14.415722  pct:-3.335084678\n",
      "[Iter.  490]  loss:13.939329  pct:-3.304674920\n",
      "[Iter.  500]  loss:13.482893  pct:-3.274448522\n",
      "[Iter.  510]  loss:13.045523  pct:-3.243890615\n",
      "[Iter.  520]  loss:12.626306  pct:-3.213494159\n",
      "[Iter.  530]  loss:12.224526  pct:-3.182080239\n",
      "[Iter.  540]  loss:11.839331  pct:-3.151007404\n",
      "[Iter.  550]  loss:11.469954  pct:-3.119907643\n",
      "[Iter.  560]  loss:11.115796  pct:-3.087705376\n",
      "[Iter.  570]  loss:10.776098  pct:-3.055991987\n",
      "[Iter.  580]  loss:10.450281  pct:-3.023516495\n",
      "[Iter.  590]  loss:10.137730  pct:-2.990842965\n",
      "[Iter.  600]  loss:9.837911  pct:-2.957456976\n",
      "[Iter.  610]  loss:9.550271  pct:-2.923787663\n",
      "[Iter.  620]  loss:9.274295  pct:-2.889720931\n",
      "[Iter.  630]  loss:9.009464  pct:-2.855533423\n",
      "[Iter.  640]  loss:8.755328  pct:-2.820768006\n",
      "[Iter.  650]  loss:8.511460  pct:-2.785365313\n",
      "[Iter.  660]  loss:8.277429  pct:-2.749606635\n",
      "[Iter.  670]  loss:8.052823  pct:-2.713470214\n",
      "[Iter.  680]  loss:7.837248  pct:-2.677014215\n",
      "[Iter.  690]  loss:7.630335  pct:-2.640117061\n",
      "[Iter.  700]  loss:7.431746  pct:-2.602622814\n",
      "[Iter.  710]  loss:7.241141  pct:-2.564742541\n",
      "[Iter.  720]  loss:7.058167  pct:-2.526878116\n",
      "[Iter.  730]  loss:6.882472  pct:-2.489236625\n",
      "[Iter.  740]  loss:6.713841  pct:-2.450145806\n",
      "[Iter.  750]  loss:6.551960  pct:-2.411159816\n",
      "[Iter.  760]  loss:6.396555  pct:-2.371879070\n",
      "[Iter.  770]  loss:6.247366  pct:-2.332340804\n",
      "[Iter.  780]  loss:6.104146  pct:-2.292492960\n",
      "[Iter.  790]  loss:5.966649  pct:-2.252509721\n",
      "[Iter.  800]  loss:5.834650  pct:-2.212280538\n",
      "[Iter.  810]  loss:5.707930  pct:-2.171859968\n",
      "[Iter.  820]  loss:5.586281  pct:-2.131216043\n",
      "[Iter.  830]  loss:5.469506  pct:-2.090398002\n",
      "[Iter.  840]  loss:5.357370  pct:-2.050201454\n",
      "[Iter.  850]  loss:5.249705  pct:-2.009662651\n",
      "[Iter.  860]  loss:5.146385  pct:-1.968103886\n",
      "[Iter.  870]  loss:5.047167  pct:-1.927914215\n",
      "[Iter.  880]  loss:4.951901  pct:-1.887511541\n",
      "[Iter.  890]  loss:4.860432  pct:-1.847154857\n",
      "[Iter.  900]  loss:4.772626  pct:-1.806551808\n",
      "[Iter.  910]  loss:4.688316  pct:-1.766533898\n",
      "[Iter.  920]  loss:4.607370  pct:-1.726536651\n",
      "[Iter.  930]  loss:4.529673  pct:-1.686379595\n",
      "[Iter.  940]  loss:4.455088  pct:-1.646575599\n",
      "[Iter.  950]  loss:4.383471  pct:-1.607524866\n",
      "[Iter.  960]  loss:4.314724  pct:-1.568324187\n",
      "[Iter.  970]  loss:4.248711  pct:-1.529965908\n",
      "[Iter.  980]  loss:4.185365  pct:-1.490944303\n",
      "[Iter.  990]  loss:4.124537  pct:-1.453332249\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.665247  pct:100.000000000\n",
      "[Iter.    2]  loss:2.665263  pct:0.000599346\n",
      "[Iter.    4]  loss:2.665272  pct:0.000366762\n",
      "[Iter.    6]  loss:2.665282  pct:0.000348870\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.665282\n",
      "Best loss: 2.665282 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    }
   ],
   "source": [
    "N = 20\n",
    "k = 3\n",
    "p = 500\n",
    "sigma = 2.0\n",
    "lam = 0.1\n",
    "n_top = 20\n",
    "rhos = []\n",
    "models = []\n",
    "corr_group = []\n",
    "results = []\n",
    "model_kwargs = dict(a=0.3, c=0.3)\n",
    "\n",
    "for rho in [0.25,0.5,0.7, 0.85,0.9,0.95,0.99]:\n",
    "    for trial in range(10):\n",
    "        np.random.seed(trial)\n",
    "        data, A_star,theta_star = simulate_corr_data(rho,N,k,p,sigma)\n",
    "        A = construct_A_matrix_w(theta_star)\n",
    "        nmf = NMF(n_components=k)\n",
    "        Y_nmf = nmf.fit_transform(data)\n",
    "        nmf_theta = nmf.components_/nmf.components_.sum(axis = 0, keepdims =True)\n",
    "        theta = theta_star/theta_star.sum(axis = 1, keepdims =True)\n",
    "        \n",
    "        lst = []\n",
    "        for i in range(k):\n",
    "            lst.append(list(theta.argsort(axis = 0)[-1*n_top:,:][:,i]))\n",
    "        \n",
    "\n",
    "        model = spc.SPECTRA_Model(X = data, labels = None, L = k, adj_matrix = A, use_weights = False, lam = lam, delta=0.0,use_cell_types = False, kappa = 0.0, rho = 0.0)\n",
    "        #initialize eta\n",
    "        model.internal_model.theta = nn.Parameter(torch.log(torch.Tensor(nmf_theta.T) + 0.000001))\n",
    "        model.internal_model.eta = nn.Parameter(torch.eye(k)*10)\n",
    "        model.train(X = data)\n",
    "        spectra_theta = torch.softmax(model.internal_model.theta, dim = 1).detach().numpy().T\n",
    "        #spectra_theta = theta*(model.internal_model.gene_scaling.exp().detach()/(1.0 + model.internal_model.gene_scaling.exp().detach()) + model.internal_model.delta).numpy().reshape(1,-1)\n",
    "        \n",
    "        theta = theta_star/theta_star.sum(axis = 1, keepdims =True)\n",
    "        \n",
    "        \n",
    "        model = run_trials(scipy.sparse.coo_matrix(data), vcells=None, nfactors=k, \n",
    "                        ntrials=1, min_iter=20,\n",
    "                        max_iter=1000, check_freq=10,\n",
    "                        epsilon=0.001,\n",
    "                        better_than_n_ago=5, dtype=np.float32,\n",
    "                        verbose=True, model_kwargs=model_kwargs,\n",
    "                        return_all=False, reproject=True,\n",
    "                        batchsize=0,\n",
    "                        beta_theta_simultaneous=True,\n",
    "                        loss_smoothing=1\n",
    "                        )\n",
    "        schpf_theta = model.beta.e_x.T / (model.beta.e_x.T.sum(axis = 0, keepdims = True))\n",
    "        \n",
    "        I = create_mask(np.array(lst), G_input = k, D= p).T.astype(int)\n",
    "        terms = np.array(range(k)).astype(str)\n",
    "        FA = slalom.initFA(Y = data.astype(float), terms = terms, I = I, noise='gauss', \n",
    "            nHidden=0, nHiddenSparse=0,do_preTrain=False, minGenes = 1, pruneGenes = False)\n",
    "        FA.train()\n",
    "        temp = (FA.getW()*FA.getZ()).T\n",
    "        slalom_theta = temp/temp.sum(axis = 0, keepdims = True)\n",
    "        \n",
    "        best_schpf = best_permutation(theta,schpf_theta)\n",
    "        best_nmf = best_permutation(theta,nmf_theta)\n",
    "        best_spade = best_permutation(theta,spectra_theta)\n",
    "        best_slalom = best_permutation(theta, slalom_theta)\n",
    "        \n",
    "        nfactors = [np.corrcoef(nmf_theta[best_nmf[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        sfactors = [np.corrcoef(spectra_theta[best_spade[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        hfactors = [np.corrcoef(schpf_theta[best_schpf[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        slfactors = [np.corrcoef(slalom_theta[best_slalom[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(nfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"NMF\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(nfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"NMF\")\n",
    "        corr_group.append(\"C\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(sfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"Spectra\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(sfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"Spectra\")\n",
    "        corr_group.append(\"C\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(hfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"scHPF\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(hfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"scHPF\")\n",
    "        corr_group.append(\"C\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(slfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"slalom\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(slfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"slalom\")\n",
    "        corr_group.append(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "26720569",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2 = pd.DataFrame()\n",
    "results_df2[\"rho\"] = rhos\n",
    "results_df2[\"model\"] = models\n",
    "results_df2[\"corr\"] = corr_group\n",
    "results_df2[\"value\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "62211848",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 722.25x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.catplot(x=\"rho\", y=\"value\", hue=\"model\", col=\"corr\",\n",
    "                capsize=.2,  height=6, aspect=.75,\n",
    "                kind=\"point\", data=results_df2)\n",
    "g.despine(left=True)\n",
    "plt.savefig(\"simulation_correlated_factors.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 155,
   "id": "ff89a4a8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rho</th>\n",
       "      <th>model</th>\n",
       "      <th>corr</th>\n",
       "      <th>value</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>64</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>U</td>\n",
       "      <td>0.637732</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>73</th>\n",
       "      <td>0.25</td>\n",
       "      <td>NMF</td>\n",
       "      <td>C</td>\n",
       "      <td>0.646530</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     rho model corr     value\n",
       "0   0.25   NMF    U  0.637732\n",
       "1   0.25   NMF    C  0.646530\n",
       "8   0.25   NMF    U  0.637732\n",
       "9   0.25   NMF    C  0.646530\n",
       "16  0.25   NMF    U  0.637732\n",
       "17  0.25   NMF    C  0.646530\n",
       "24  0.25   NMF    U  0.637732\n",
       "25  0.25   NMF    C  0.646530\n",
       "32  0.25   NMF    U  0.637732\n",
       "33  0.25   NMF    C  0.646530\n",
       "40  0.25   NMF    U  0.637732\n",
       "41  0.25   NMF    C  0.646530\n",
       "48  0.25   NMF    U  0.637732\n",
       "49  0.25   NMF    C  0.646530\n",
       "56  0.25   NMF    U  0.637732\n",
       "57  0.25   NMF    C  0.646530\n",
       "64  0.25   NMF    U  0.637732\n",
       "65  0.25   NMF    C  0.646530\n",
       "72  0.25   NMF    U  0.637732\n",
       "73  0.25   NMF    C  0.646530"
      ]
     },
     "execution_count": 155,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results_df2[(results_df2.rho == 0.25)&(results_df2.model == \"NMF\")]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 162,
   "id": "597334ea",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor(0.)"
      ]
     },
     "execution_count": 162,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.internal_model.weights.min()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "80ee1150",
   "metadata": {},
   "source": [
    "## Same code for scHPF"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "87668152",
   "metadata": {},
   "source": [
    "## "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "id": "21eed69c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:162.682297  pct:100.000000000\n",
      "[Iter.   10]  loss:40.581615  pct:-75.054680151\n",
      "[Iter.   20]  loss:28.153906  pct:-30.623989317\n",
      "[Iter.   30]  loss:21.512732  pct:-23.588820491\n",
      "[Iter.   40]  loss:16.980190  pct:-21.069110931\n",
      "[Iter.   50]  loss:13.731272  pct:-19.133581428\n",
      "[Iter.   60]  loss:11.348175  pct:-17.355251133\n",
      "[Iter.   70]  loss:9.537597  pct:-15.954797476\n",
      "[Iter.   80]  loss:8.132697  pct:-14.730121654\n",
      "[Iter.   90]  loss:7.027909  pct:-13.584519529\n",
      "[Iter.  100]  loss:6.153367  pct:-12.443846408\n",
      "[Iter.  110]  loss:5.455604  pct:-11.339538795\n",
      "[Iter.  120]  loss:4.897664  pct:-10.226916162\n",
      "[Iter.  130]  loss:4.449148  pct:-9.157742394\n",
      "[Iter.  140]  loss:4.087934  pct:-8.118715530\n",
      "[Iter.  150]  loss:3.797028  pct:-7.116232086\n",
      "[Iter.  160]  loss:3.561892  pct:-6.192628003\n",
      "[Iter.  170]  loss:3.371762  pct:-5.337886941\n",
      "[Iter.  180]  loss:3.218355  pct:-4.549753415\n",
      "[Iter.  190]  loss:3.094132  pct:-3.859843423\n",
      "[Iter.  200]  loss:2.994543  pct:-3.218621686\n",
      "[Iter.  210]  loss:2.914437  pct:-2.675074293\n",
      "[Iter.  220]  loss:2.848788  pct:-2.252562268\n",
      "[Iter.  230]  loss:2.795468  pct:-1.871662655\n",
      "[Iter.  240]  loss:2.752895  pct:-1.522911389\n",
      "[Iter.  250]  loss:2.718290  pct:-1.257041107\n",
      "[Iter.  260]  loss:2.690087  pct:-1.037527532\n",
      "[Iter.  270]  loss:2.666834  pct:-0.864403259\n",
      "[Iter.  280]  loss:2.647718  pct:-0.716811111\n",
      "[Iter.  290]  loss:2.632185  pct:-0.586673035\n",
      "[Iter.  300]  loss:2.618418  pct:-0.522989567\n",
      "[Iter.  310]  loss:2.607260  pct:-0.426152906\n",
      "[Iter.  320]  loss:2.598916  pct:-0.320036109\n",
      "[Iter.  330]  loss:2.591934  pct:-0.268626355\n",
      "[Iter.  340]  loss:2.586544  pct:-0.207959243\n",
      "[Iter.  350]  loss:2.582048  pct:-0.173817212\n",
      "[Iter.  360]  loss:2.577313  pct:-0.183399730\n",
      "[Iter.  370]  loss:2.573664  pct:-0.141590674\n",
      "[Iter.  380]  loss:2.571123  pct:-0.098724118\n",
      "[Iter.  390]  loss:2.569089  pct:-0.079107417\n",
      "[Iter.  400]  loss:2.567415  pct:-0.065147547\n",
      "[Iter.  410]  loss:2.566036  pct:-0.053712116\n",
      "[Iter.  420]  loss:2.564879  pct:-0.045109348\n",
      "[Iter.  430]  loss:2.563891  pct:-0.038511302\n",
      "[Iter.  440]  loss:2.563060  pct:-0.032416635\n",
      "[Iter.  450]  loss:2.562350  pct:-0.027701676\n",
      "[Iter.  460]  loss:2.561728  pct:-0.024248009\n",
      "[Iter.  470]  loss:2.561194  pct:-0.020847550\n",
      "[Iter.  480]  loss:2.560734  pct:-0.017966143\n",
      "[Iter.  490]  loss:2.560341  pct:-0.015353106\n",
      "[Iter.  500]  loss:2.560009  pct:-0.012962283\n",
      "[Iter.  510]  loss:2.559716  pct:-0.011445913\n",
      "[Iter.  520]  loss:2.559441  pct:-0.010748654\n",
      "[Iter.  530]  loss:2.559199  pct:-0.009464304\n",
      "[Iter.  540]  loss:2.558987  pct:-0.008263417\n",
      "[Iter.  550]  loss:2.558793  pct:-0.007602599\n",
      "[Iter.  560]  loss:2.558611  pct:-0.007118661\n",
      "[Iter.  570]  loss:2.558445  pct:-0.006476207\n",
      "[Iter.  580]  loss:2.558294  pct:-0.005898855\n",
      "[Iter.  590]  loss:2.558155  pct:-0.005451870\n",
      "[Iter.  600]  loss:2.558026  pct:-0.005032770\n",
      "[Iter.  610]  loss:2.557907  pct:-0.004641566\n",
      "[Iter.  620]  loss:2.557795  pct:-0.004380798\n",
      "[Iter.  630]  loss:2.557690  pct:-0.004092031\n",
      "[Iter.  640]  loss:2.557591  pct:-0.003887122\n",
      "[Iter.  650]  loss:2.557498  pct:-0.003654223\n",
      "[Iter.  660]  loss:2.557408  pct:-0.003495877\n",
      "[Iter.  670]  loss:2.557323  pct:-0.003337514\n",
      "[Iter.  680]  loss:2.557241  pct:-0.003197781\n",
      "[Iter.  690]  loss:2.557162  pct:-0.003095327\n",
      "[Iter.  700]  loss:2.557085  pct:-0.003011511\n",
      "[Iter.  710]  loss:2.557011  pct:-0.002899715\n",
      "[Iter.  720]  loss:2.556938  pct:-0.002825206\n",
      "[Iter.  730]  loss:2.556867  pct:-0.002778664\n",
      "[Iter.  740]  loss:2.556798  pct:-0.002722794\n",
      "[Iter.  750]  loss:2.556730  pct:-0.002666919\n",
      "[Iter.  760]  loss:2.556662  pct:-0.002629689\n",
      "[Iter.  770]  loss:2.556596  pct:-0.002592457\n",
      "[Iter.  780]  loss:2.556531  pct:-0.002555221\n",
      "[Iter.  790]  loss:2.556466  pct:-0.002536635\n",
      "[Iter.  800]  loss:2.556401  pct:-0.002518047\n",
      "[Iter.  810]  loss:2.556338  pct:-0.002490132\n",
      "[Iter.  820]  loss:2.556275  pct:-0.002471540\n",
      "[Iter.  830]  loss:2.556212  pct:-0.002452948\n",
      "[Iter.  840]  loss:2.556149  pct:-0.002443681\n",
      "[Iter.  850]  loss:2.556087  pct:-0.002443741\n",
      "[Iter.  860]  loss:2.556025  pct:-0.002415818\n",
      "[Iter.  870]  loss:2.555964  pct:-0.002415876\n",
      "[Iter.  880]  loss:2.555902  pct:-0.002406607\n",
      "[Iter.  890]  loss:2.555840  pct:-0.002406665\n",
      "[Iter.  900]  loss:2.555779  pct:-0.002397394\n",
      "[Iter.  910]  loss:2.555718  pct:-0.002388123\n",
      "[Iter.  920]  loss:2.555657  pct:-0.002388180\n",
      "[Iter.  930]  loss:2.555596  pct:-0.002378908\n",
      "[Iter.  940]  loss:2.555536  pct:-0.002369636\n",
      "[Iter.  950]  loss:2.555475  pct:-0.002360362\n",
      "[Iter.  960]  loss:2.555415  pct:-0.002369748\n",
      "[Iter.  970]  loss:2.555355  pct:-0.002360474\n",
      "[Iter.  980]  loss:2.555294  pct:-0.002360530\n",
      "[Iter.  990]  loss:2.555234  pct:-0.002360585\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.554628  pct:100.000000000\n",
      "[Iter.    2]  loss:2.554630  pct:0.000065330\n",
      "[Iter.    4]  loss:2.554631  pct:0.000037331\n",
      "[Iter.    6]  loss:2.554631  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.554631\n",
      "Best loss: 2.554631 (trial 0)\n"
     ]
    }
   ],
   "source": [
    "model = run_trials(scipy.sparse.coo_matrix(data), vcells=None, nfactors=k, \n",
    "                        ntrials=1, min_iter=20,\n",
    "                        max_iter=1000, check_freq=10,\n",
    "                        epsilon=0.001,\n",
    "                        better_than_n_ago=5, dtype=np.float32,\n",
    "                        verbose=True, model_kwargs=model_kwargs,\n",
    "                        return_all=False, reproject=True,\n",
    "                        batchsize=0,\n",
    "                        beta_theta_simultaneous=True,\n",
    "                        loss_smoothing=1\n",
    "                        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "id": "c2cacdc8",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.40838942e-03, 5.35525939e-03, 1.04719661e-03],\n",
       "       [1.43997046e-02, 4.71373838e-05, 4.63942455e-02],\n",
       "       [2.01888923e-03, 5.20275874e-05, 9.77493818e-03],\n",
       "       ...,\n",
       "       [1.58313156e-03, 4.93269273e-05, 7.33240666e-04],\n",
       "       [1.49959582e-03, 5.97432272e-05, 1.27868612e-03],\n",
       "       [5.58313347e-05, 1.15564407e-02, 3.06465401e-03]])"
      ]
     },
     "execution_count": 119,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "c0b715eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2.40838942e-03, 5.35525939e-03, 1.04719661e-03],\n",
       "       [1.43997046e-02, 4.71373838e-05, 4.63942455e-02],\n",
       "       [2.01888923e-03, 5.20275874e-05, 9.77493818e-03],\n",
       "       ...,\n",
       "       [1.58313156e-03, 4.93269273e-05, 7.33240666e-04],\n",
       "       [1.49959582e-03, 5.97432272e-05, 1.27868612e-03],\n",
       "       [5.58313347e-05, 1.15564407e-02, 3.06465401e-03]])"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.beta.e_x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "id": "96ec2738",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[  7,   0,   5, ...,   2,   1,  44],\n",
       "       [  6,   0,  14, ...,   2,   6,  50],\n",
       "       [  3,   2,   8, ...,   0,   5,  23],\n",
       "       ...,\n",
       "       [  3,   0,   3, ...,   2,   4,  11],\n",
       "       [  4,   0,   7, ...,   0,   0,  11],\n",
       "       [ 14,   3,  29, ...,   4,   4, 115]])"
      ]
     },
     "execution_count": 136,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "id": "ea84ff0a",
   "metadata": {},
   "outputs": [],
   "source": [
    "I = create_mask(np.array(lst), G_input = k, D= p).T.astype(int)\n",
    "terms = np.array(range(k)).astype(str)\n",
    "FA = slalom.initFA(Y = data.astype(float), terms = terms, I = I, noise='gauss', \n",
    "    nHidden=0, nHiddenSparse=0,do_preTrain=False, minGenes = 1, pruneGenes = False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "id": "6e85146d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    }
   ],
   "source": [
    "FA.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "d5d60758",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100, 3)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_nmf.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae32d541",
   "metadata": {},
   "source": [
    "## Other settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "f22f51db",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 23%|██▎       | 2252/10000 [00:29<01:39, 77.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:468.482788  pct:100.000000000\n",
      "[Iter.   10]  loss:80.517685  pct:-82.813093193\n",
      "[Iter.   20]  loss:76.477486  pct:-5.017778744\n",
      "[Iter.   30]  loss:72.764748  pct:-4.854681094\n",
      "[Iter.   40]  loss:69.475121  pct:-4.520907696\n",
      "[Iter.   50]  loss:66.728424  pct:-3.953496519\n",
      "[Iter.   60]  loss:64.276558  pct:-3.674395408\n",
      "[Iter.   70]  loss:62.033810  pct:-3.489216494\n",
      "[Iter.   80]  loss:59.955269  pct:-3.350657993\n",
      "[Iter.   90]  loss:58.029957  pct:-3.211247450\n",
      "[Iter.  100]  loss:56.234745  pct:-3.093594913\n",
      "[Iter.  110]  loss:54.552372  pct:-2.991696763\n",
      "[Iter.  120]  loss:52.970615  pct:-2.899519369\n",
      "[Iter.  130]  loss:51.479179  pct:-2.815591236\n",
      "[Iter.  140]  loss:50.068848  pct:-2.739615788\n",
      "[Iter.  150]  loss:48.732044  pct:-2.669930503\n",
      "[Iter.  160]  loss:47.462189  pct:-2.605791568\n",
      "[Iter.  170]  loss:46.253563  pct:-2.546502439\n",
      "[Iter.  180]  loss:45.101135  pct:-2.491543571\n",
      "[Iter.  190]  loss:44.000458  pct:-2.440465155\n",
      "[Iter.  200]  loss:42.947647  pct:-2.392726627\n",
      "[Iter.  210]  loss:41.939182  pct:-2.348125873\n",
      "[Iter.  220]  loss:40.971905  pct:-2.306381465\n",
      "[Iter.  230]  loss:40.043034  pct:-2.267092927\n",
      "[Iter.  240]  loss:39.150028  pct:-2.230114182\n",
      "[Iter.  250]  loss:38.290615  pct:-2.195178869\n",
      "[Iter.  260]  loss:37.462708  pct:-2.162168355\n",
      "[Iter.  270]  loss:36.664448  pct:-2.130811647\n",
      "[Iter.  280]  loss:35.894051  pct:-2.101210390\n",
      "[Iter.  290]  loss:35.149948  pct:-2.073052402\n",
      "[Iter.  300]  loss:34.430649  pct:-2.046373764\n",
      "[Iter.  310]  loss:33.734879  pct:-2.020787548\n",
      "[Iter.  320]  loss:33.061371  pct:-1.996472848\n",
      "[Iter.  330]  loss:32.409058  pct:-1.973037462\n",
      "[Iter.  340]  loss:31.776878  pct:-1.950625247\n",
      "[Iter.  350]  loss:31.163794  pct:-1.929342417\n",
      "[Iter.  360]  loss:30.568926  pct:-1.908842404\n",
      "[Iter.  370]  loss:29.991446  pct:-1.889108956\n",
      "[Iter.  380]  loss:29.430536  pct:-1.870230865\n",
      "[Iter.  390]  loss:28.885479  pct:-1.852012793\n",
      "[Iter.  400]  loss:28.355595  pct:-1.834431546\n",
      "[Iter.  410]  loss:27.840223  pct:-1.817529589\n",
      "[Iter.  420]  loss:27.338795  pct:-1.801094045\n",
      "[Iter.  430]  loss:26.850723  pct:-1.785270517\n",
      "[Iter.  440]  loss:26.375441  pct:-1.770092613\n",
      "[Iter.  450]  loss:25.912445  pct:-1.755403962\n",
      "[Iter.  460]  loss:25.461329  pct:-1.740926264\n",
      "[Iter.  470]  loss:25.021622  pct:-1.726959386\n",
      "[Iter.  480]  loss:24.592865  pct:-1.713544865\n",
      "[Iter.  490]  loss:24.174656  pct:-1.700530118\n",
      "[Iter.  500]  loss:23.766659  pct:-1.687706054\n",
      "[Iter.  510]  loss:23.368511  pct:-1.675235828\n",
      "[Iter.  520]  loss:22.979872  pct:-1.663090330\n",
      "[Iter.  530]  loss:22.600391  pct:-1.651359790\n",
      "[Iter.  540]  loss:22.229742  pct:-1.640012916\n",
      "[Iter.  550]  loss:21.867685  pct:-1.628704154\n",
      "[Iter.  560]  loss:21.513906  pct:-1.617815667\n",
      "[Iter.  570]  loss:21.168140  pct:-1.607174726\n",
      "[Iter.  580]  loss:20.830132  pct:-1.596781172\n",
      "[Iter.  590]  loss:20.499674  pct:-1.586440714\n",
      "[Iter.  600]  loss:20.176502  pct:-1.576471987\n",
      "[Iter.  610]  loss:19.860418  pct:-1.566594172\n",
      "[Iter.  620]  loss:19.551189  pct:-1.557010996\n",
      "[Iter.  630]  loss:19.248589  pct:-1.547736325\n",
      "[Iter.  640]  loss:18.952471  pct:-1.538386992\n",
      "[Iter.  650]  loss:18.662601  pct:-1.529453926\n",
      "[Iter.  660]  loss:18.378855  pct:-1.520402822\n",
      "[Iter.  670]  loss:18.101021  pct:-1.511704306\n",
      "[Iter.  680]  loss:17.828968  pct:-1.502969184\n",
      "[Iter.  690]  loss:17.562532  pct:-1.494397334\n",
      "[Iter.  700]  loss:17.301565  pct:-1.485931803\n",
      "[Iter.  710]  loss:17.045887  pct:-1.477774839\n",
      "[Iter.  720]  loss:16.795387  pct:-1.469561105\n",
      "[Iter.  730]  loss:16.549902  pct:-1.461623372\n",
      "[Iter.  740]  loss:16.309341  pct:-1.453546566\n",
      "[Iter.  750]  loss:16.073559  pct:-1.445690645\n",
      "[Iter.  760]  loss:15.842413  pct:-1.438050288\n",
      "[Iter.  770]  loss:15.615829  pct:-1.430239416\n",
      "[Iter.  780]  loss:15.393656  pct:-1.422740631\n",
      "[Iter.  790]  loss:15.175791  pct:-1.415290776\n",
      "[Iter.  800]  loss:14.962170  pct:-1.407644205\n",
      "[Iter.  810]  loss:14.752620  pct:-1.400531533\n",
      "[Iter.  820]  loss:14.547074  pct:-1.393280848\n",
      "[Iter.  830]  loss:14.345465  pct:-1.385911745\n",
      "[Iter.  840]  loss:14.147663  pct:-1.378844074\n",
      "[Iter.  850]  loss:13.953592  pct:-1.371751748\n",
      "[Iter.  860]  loss:13.763159  pct:-1.364763267\n",
      "[Iter.  870]  loss:13.576300  pct:-1.357676196\n",
      "[Iter.  880]  loss:13.392900  pct:-1.350884693\n",
      "[Iter.  890]  loss:13.212932  pct:-1.343755921\n",
      "[Iter.  900]  loss:13.036297  pct:-1.336832683\n",
      "[Iter.  910]  loss:12.862895  pct:-1.330146395\n",
      "[Iter.  920]  loss:12.692689  pct:-1.323232988\n",
      "[Iter.  930]  loss:12.525580  pct:-1.316573159\n",
      "[Iter.  940]  loss:12.361494  pct:-1.310009888\n",
      "[Iter.  950]  loss:12.200455  pct:-1.302749907\n",
      "[Iter.  960]  loss:12.042268  pct:-1.296565712\n",
      "[Iter.  970]  loss:11.886951  pct:-1.289759997\n",
      "[Iter.  980]  loss:11.734443  pct:-1.282992838\n",
      "[Iter.  990]  loss:11.584655  pct:-1.276480754\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.762573  pct:100.000000000\n",
      "[Iter.    2]  loss:2.762607  pct:0.001234134\n",
      "[Iter.    4]  loss:2.762599  pct:-0.000267536\n",
      "[Iter.    6]  loss:2.762596  pct:-0.000120823\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.762596\n",
      "Best loss: 2.762596 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 11%|█         | 1081/10000 [00:17<02:24, 61.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3938.545654  pct:100.000000000\n",
      "[Iter.   10]  loss:580.299683  pct:-85.266146149\n",
      "[Iter.   20]  loss:567.408203  pct:-2.221521031\n",
      "[Iter.   30]  loss:560.970825  pct:-1.134523240\n",
      "[Iter.   40]  loss:554.886597  pct:-1.084589116\n",
      "[Iter.   50]  loss:549.048584  pct:-1.052109157\n",
      "[Iter.   60]  loss:543.396484  pct:-1.029435240\n",
      "[Iter.   70]  loss:537.901794  pct:-1.011175099\n",
      "[Iter.   80]  loss:532.554810  pct:-0.994044809\n",
      "[Iter.   90]  loss:527.354614  pct:-0.976461994\n",
      "[Iter.  100]  loss:522.291199  pct:-0.960153830\n",
      "[Iter.  110]  loss:517.354797  pct:-0.945143510\n",
      "[Iter.  120]  loss:512.540100  pct:-0.930637406\n",
      "[Iter.  130]  loss:507.841858  pct:-0.916658460\n",
      "[Iter.  140]  loss:503.255127  pct:-0.903180958\n",
      "[Iter.  150]  loss:498.774933  pct:-0.890243110\n",
      "[Iter.  160]  loss:494.397186  pct:-0.877699799\n",
      "[Iter.  170]  loss:490.117218  pct:-0.865694300\n",
      "[Iter.  180]  loss:485.931305  pct:-0.854063667\n",
      "[Iter.  190]  loss:481.835968  pct:-0.842781042\n",
      "[Iter.  200]  loss:477.827271  pct:-0.831963111\n",
      "[Iter.  210]  loss:473.902039  pct:-0.821475076\n",
      "[Iter.  220]  loss:470.057037  pct:-0.811349373\n",
      "[Iter.  230]  loss:466.289398  pct:-0.801528083\n",
      "[Iter.  240]  loss:462.596161  pct:-0.792048311\n",
      "[Iter.  250]  loss:458.974823  pct:-0.782829214\n",
      "[Iter.  260]  loss:455.422729  pct:-0.773919032\n",
      "[Iter.  270]  loss:451.937469  pct:-0.765280208\n",
      "[Iter.  280]  loss:448.516693  pct:-0.756913644\n",
      "[Iter.  290]  loss:445.158234  pct:-0.748792525\n",
      "[Iter.  300]  loss:441.860138  pct:-0.740881658\n",
      "[Iter.  310]  loss:438.620056  pct:-0.733282211\n",
      "[Iter.  320]  loss:435.436432  pct:-0.725827336\n",
      "[Iter.  330]  loss:432.307526  pct:-0.718567860\n",
      "[Iter.  340]  loss:429.231598  pct:-0.711513807\n",
      "[Iter.  350]  loss:426.206879  pct:-0.704682333\n",
      "[Iter.  360]  loss:423.231781  pct:-0.698040742\n",
      "[Iter.  370]  loss:420.304871  pct:-0.691562055\n",
      "[Iter.  380]  loss:417.424896  pct:-0.685210800\n",
      "[Iter.  390]  loss:414.590363  pct:-0.679052380\n",
      "[Iter.  400]  loss:411.800079  pct:-0.673021723\n",
      "[Iter.  410]  loss:409.052460  pct:-0.667221734\n",
      "[Iter.  420]  loss:406.346527  pct:-0.661512369\n",
      "[Iter.  430]  loss:403.681335  pct:-0.655891332\n",
      "[Iter.  440]  loss:401.055573  pct:-0.650454383\n",
      "[Iter.  450]  loss:398.468018  pct:-0.645186131\n",
      "[Iter.  460]  loss:395.917786  pct:-0.640009190\n",
      "[Iter.  470]  loss:393.403839  pct:-0.634966810\n",
      "[Iter.  480]  loss:390.925201  pct:-0.630049188\n",
      "[Iter.  490]  loss:388.481384  pct:-0.625136760\n",
      "[Iter.  500]  loss:386.070892  pct:-0.620490979\n",
      "[Iter.  510]  loss:383.693268  pct:-0.615851793\n",
      "[Iter.  520]  loss:381.347626  pct:-0.611332616\n",
      "[Iter.  530]  loss:379.032990  pct:-0.606962276\n",
      "[Iter.  540]  loss:376.748779  pct:-0.602641529\n",
      "[Iter.  550]  loss:374.494263  pct:-0.598413777\n",
      "[Iter.  560]  loss:372.268677  pct:-0.594291064\n",
      "[Iter.  570]  loss:370.071503  pct:-0.590211911\n",
      "[Iter.  580]  loss:367.901917  pct:-0.586261348\n",
      "[Iter.  590]  loss:365.759277  pct:-0.582394128\n",
      "[Iter.  600]  loss:363.643097  pct:-0.578571905\n",
      "[Iter.  610]  loss:361.552948  pct:-0.574780312\n",
      "[Iter.  620]  loss:359.487915  pct:-0.571156443\n",
      "[Iter.  630]  loss:357.447693  pct:-0.567535676\n",
      "[Iter.  640]  loss:355.431396  pct:-0.564081522\n",
      "[Iter.  650]  loss:353.438873  pct:-0.560592906\n",
      "[Iter.  660]  loss:351.469421  pct:-0.557225606\n",
      "[Iter.  670]  loss:349.522644  pct:-0.553896648\n",
      "[Iter.  680]  loss:347.598297  pct:-0.550564307\n",
      "[Iter.  690]  loss:345.695587  pct:-0.547387596\n",
      "[Iter.  700]  loss:343.813995  pct:-0.544291529\n",
      "[Iter.  710]  loss:341.953278  pct:-0.541198962\n",
      "[Iter.  720]  loss:340.112823  pct:-0.538218003\n",
      "[Iter.  730]  loss:338.292480  pct:-0.535217402\n",
      "[Iter.  740]  loss:336.491699  pct:-0.532314891\n",
      "[Iter.  750]  loss:334.710236  pct:-0.529422755\n",
      "[Iter.  760]  loss:332.947662  pct:-0.526596756\n",
      "[Iter.  770]  loss:331.203552  pct:-0.523839121\n",
      "[Iter.  780]  loss:329.477661  pct:-0.521096800\n",
      "[Iter.  790]  loss:327.769897  pct:-0.518324570\n",
      "[Iter.  800]  loss:326.079315  pct:-0.515783264\n",
      "[Iter.  810]  loss:324.405945  pct:-0.513178936\n",
      "[Iter.  820]  loss:322.749329  pct:-0.510661484\n",
      "[Iter.  830]  loss:321.109375  pct:-0.508119915\n",
      "[Iter.  840]  loss:319.485779  pct:-0.505620925\n",
      "[Iter.  850]  loss:317.878174  pct:-0.503185145\n",
      "[Iter.  860]  loss:316.286316  pct:-0.500776096\n",
      "[Iter.  870]  loss:314.709869  pct:-0.498423882\n",
      "[Iter.  880]  loss:313.148438  pct:-0.496149640\n",
      "[Iter.  890]  loss:311.601929  pct:-0.493858057\n",
      "[Iter.  900]  loss:310.069855  pct:-0.491676666\n",
      "[Iter.  910]  loss:308.552399  pct:-0.489391675\n",
      "[Iter.  920]  loss:307.049011  pct:-0.487238945\n",
      "[Iter.  930]  loss:305.559753  pct:-0.485022833\n",
      "[Iter.  940]  loss:304.084045  pct:-0.482952349\n",
      "[Iter.  950]  loss:302.621796  pct:-0.480870265\n",
      "[Iter.  960]  loss:301.173004  pct:-0.478746582\n",
      "[Iter.  970]  loss:299.736969  pct:-0.476814036\n",
      "[Iter.  980]  loss:298.313873  pct:-0.474781509\n",
      "[Iter.  990]  loss:296.903320  pct:-0.472841897\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.794372  pct:100.000000000\n",
      "[Iter.    2]  loss:2.793932  pct:-0.015758786\n",
      "[Iter.    4]  loss:2.793847  pct:-0.003029372\n",
      "[Iter.    6]  loss:2.793814  pct:-0.001177651\n",
      "[Iter.    8]  loss:2.793817  pct:0.000102406\n",
      "[Iter.   10]  loss:2.793816  pct:-0.000034135\n",
      "[Iter.   12]  loss:2.793813  pct:-0.000119473\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.793813\n",
      "Best loss: 2.793813 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 23%|██▎       | 2334/10000 [00:51<02:48, 45.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:435.486115  pct:100.000000000\n",
      "[Iter.   10]  loss:71.844780  pct:-83.502393026\n",
      "[Iter.   20]  loss:67.575333  pct:-5.942599210\n",
      "[Iter.   30]  loss:64.664230  pct:-4.307936315\n",
      "[Iter.   40]  loss:62.045902  pct:-4.049113521\n",
      "[Iter.   50]  loss:59.665844  pct:-3.835963701\n",
      "[Iter.   60]  loss:57.485897  pct:-3.653592666\n",
      "[Iter.   70]  loss:55.476601  pct:-3.495285835\n",
      "[Iter.   80]  loss:53.614727  pct:-3.356142238\n",
      "[Iter.   90]  loss:51.881588  pct:-3.232580178\n",
      "[Iter.  100]  loss:50.261772  pct:-3.122140030\n",
      "[Iter.  110]  loss:48.742481  pct:-3.022756379\n",
      "[Iter.  120]  loss:47.312950  pct:-2.932823815\n",
      "[Iter.  130]  loss:45.964123  pct:-2.850862942\n",
      "[Iter.  140]  loss:44.688091  pct:-2.776146736\n",
      "[Iter.  150]  loss:43.478191  pct:-2.707432490\n",
      "[Iter.  160]  loss:42.328518  pct:-2.644253189\n",
      "[Iter.  170]  loss:41.234093  pct:-2.585550488\n",
      "[Iter.  180]  loss:40.190350  pct:-2.531262518\n",
      "[Iter.  190]  loss:39.193375  pct:-2.480632678\n",
      "[Iter.  200]  loss:38.239658  pct:-2.433360962\n",
      "[Iter.  210]  loss:37.326061  pct:-2.389135118\n",
      "[Iter.  220]  loss:36.449783  pct:-2.347630300\n",
      "[Iter.  230]  loss:35.608307  pct:-2.308591063\n",
      "[Iter.  240]  loss:34.799374  pct:-2.271754343\n",
      "[Iter.  250]  loss:34.020885  pct:-2.237075206\n",
      "[Iter.  260]  loss:33.270985  pct:-2.204236626\n",
      "[Iter.  270]  loss:32.547985  pct:-2.173063347\n",
      "[Iter.  280]  loss:31.850346  pct:-2.143418291\n",
      "[Iter.  290]  loss:31.176594  pct:-2.115367410\n",
      "[Iter.  300]  loss:30.525486  pct:-2.088450691\n",
      "[Iter.  310]  loss:29.895823  pct:-2.062746741\n",
      "[Iter.  320]  loss:29.286423  pct:-2.038411203\n",
      "[Iter.  330]  loss:28.696278  pct:-2.015080901\n",
      "[Iter.  340]  loss:28.124479  pct:-1.992587095\n",
      "[Iter.  350]  loss:27.570141  pct:-1.971017665\n",
      "[Iter.  360]  loss:27.032396  pct:-1.950459830\n",
      "[Iter.  370]  loss:26.510529  pct:-1.930527157\n",
      "[Iter.  380]  loss:26.003815  pct:-1.911368406\n",
      "[Iter.  390]  loss:25.511549  pct:-1.893051873\n",
      "[Iter.  400]  loss:25.033157  pct:-1.875196396\n",
      "[Iter.  410]  loss:24.568047  pct:-1.857978889\n",
      "[Iter.  420]  loss:24.115631  pct:-1.841479195\n",
      "[Iter.  430]  loss:23.675421  pct:-1.825414979\n",
      "[Iter.  440]  loss:23.246954  pct:-1.809753673\n",
      "[Iter.  450]  loss:22.829758  pct:-1.794627694\n",
      "[Iter.  460]  loss:22.423378  pct:-1.780043859\n",
      "[Iter.  470]  loss:22.027428  pct:-1.765792458\n",
      "[Iter.  480]  loss:21.641493  pct:-1.752064905\n",
      "[Iter.  490]  loss:21.265230  pct:-1.738616959\n",
      "[Iter.  500]  loss:20.898315  pct:-1.725421009\n",
      "[Iter.  510]  loss:20.540422  pct:-1.712544685\n",
      "[Iter.  520]  loss:20.191179  pct:-1.700272548\n",
      "[Iter.  530]  loss:19.850346  pct:-1.688032478\n",
      "[Iter.  540]  loss:19.517645  pct:-1.676045021\n",
      "[Iter.  550]  loss:19.192783  pct:-1.664450442\n",
      "[Iter.  560]  loss:18.875483  pct:-1.653229709\n",
      "[Iter.  570]  loss:18.565571  pct:-1.641874463\n",
      "[Iter.  580]  loss:18.262770  pct:-1.630982074\n",
      "[Iter.  590]  loss:17.966862  pct:-1.620279832\n",
      "[Iter.  600]  loss:17.677601  pct:-1.609968779\n",
      "[Iter.  610]  loss:17.394835  pct:-1.599574198\n",
      "[Iter.  620]  loss:17.118366  pct:-1.589369975\n",
      "[Iter.  630]  loss:16.848000  pct:-1.579395282\n",
      "[Iter.  640]  loss:16.583544  pct:-1.569656944\n",
      "[Iter.  650]  loss:16.324858  pct:-1.559896179\n",
      "[Iter.  660]  loss:16.071722  pct:-1.550614931\n",
      "[Iter.  670]  loss:15.824013  pct:-1.541274008\n",
      "[Iter.  680]  loss:15.581580  pct:-1.532055099\n",
      "[Iter.  690]  loss:15.344290  pct:-1.522890361\n",
      "[Iter.  700]  loss:15.111980  pct:-1.513979107\n",
      "[Iter.  710]  loss:14.884533  pct:-1.505080758\n",
      "[Iter.  720]  loss:14.661812  pct:-1.496325756\n",
      "[Iter.  730]  loss:14.443710  pct:-1.487548088\n",
      "[Iter.  740]  loss:14.230088  pct:-1.478997352\n",
      "[Iter.  750]  loss:14.020833  pct:-1.470512446\n",
      "[Iter.  760]  loss:13.815820  pct:-1.462204670\n",
      "[Iter.  770]  loss:13.614974  pct:-1.453737253\n",
      "[Iter.  780]  loss:13.418167  pct:-1.445518055\n",
      "[Iter.  790]  loss:13.225311  pct:-1.437274058\n",
      "[Iter.  800]  loss:13.036291  pct:-1.429230306\n",
      "[Iter.  810]  loss:12.851028  pct:-1.421130276\n",
      "[Iter.  820]  loss:12.669423  pct:-1.413158020\n",
      "[Iter.  830]  loss:12.491408  pct:-1.405073884\n",
      "[Iter.  840]  loss:12.316870  pct:-1.397269287\n",
      "[Iter.  850]  loss:12.145751  pct:-1.389303776\n",
      "[Iter.  860]  loss:11.977954  pct:-1.381529134\n",
      "[Iter.  870]  loss:11.813420  pct:-1.373637070\n",
      "[Iter.  880]  loss:11.652057  pct:-1.365934654\n",
      "[Iter.  890]  loss:11.493780  pct:-1.358357259\n",
      "[Iter.  900]  loss:11.338538  pct:-1.350660657\n",
      "[Iter.  910]  loss:11.186244  pct:-1.343155146\n",
      "[Iter.  920]  loss:11.036887  pct:-1.335183122\n",
      "[Iter.  930]  loss:10.890336  pct:-1.327830302\n",
      "[Iter.  940]  loss:10.746559  pct:-1.320224584\n",
      "[Iter.  950]  loss:10.605511  pct:-1.312498536\n",
      "[Iter.  960]  loss:10.467087  pct:-1.305207485\n",
      "[Iter.  970]  loss:10.331271  pct:-1.297549386\n",
      "[Iter.  980]  loss:10.198010  pct:-1.289877351\n",
      "[Iter.  990]  loss:10.067214  pct:-1.282568136\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.753112  pct:100.000000000\n",
      "[Iter.    2]  loss:2.753168  pct:0.002026432\n",
      "[Iter.    4]  loss:2.753169  pct:0.000017320\n",
      "[Iter.    6]  loss:2.753168  pct:-0.000017320\n",
      "[Iter.    8]  loss:2.753168  pct:-0.000017320\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.753168\n",
      "Best loss: 2.753168 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 281/10000 [00:03<02:13, 72.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5054.274902  pct:100.000000000\n",
      "[Iter.   10]  loss:797.372559  pct:-84.223799180\n",
      "[Iter.   20]  loss:786.297302  pct:-1.388968836\n",
      "[Iter.   30]  loss:777.633179  pct:-1.101889007\n",
      "[Iter.   40]  loss:769.340942  pct:-1.066342918\n",
      "[Iter.   50]  loss:761.310669  pct:-1.043786051\n",
      "[Iter.   60]  loss:753.525513  pct:-1.022599126\n",
      "[Iter.   70]  loss:745.971863  pct:-1.002441162\n",
      "[Iter.   80]  loss:738.636841  pct:-0.983284000\n",
      "[Iter.   90]  loss:731.509155  pct:-0.964978343\n",
      "[Iter.  100]  loss:724.578308  pct:-0.947472375\n",
      "[Iter.  110]  loss:717.834229  pct:-0.930759245\n",
      "[Iter.  120]  loss:711.268372  pct:-0.914675934\n",
      "[Iter.  130]  loss:704.871948  pct:-0.899298155\n",
      "[Iter.  140]  loss:698.636719  pct:-0.884590387\n",
      "[Iter.  150]  loss:692.555664  pct:-0.870417275\n",
      "[Iter.  160]  loss:686.621582  pct:-0.856838279\n",
      "[Iter.  170]  loss:680.827881  pct:-0.843798291\n",
      "[Iter.  180]  loss:675.168518  pct:-0.831247214\n",
      "[Iter.  190]  loss:669.638062  pct:-0.819122396\n",
      "[Iter.  200]  loss:664.230896  pct:-0.807475835\n",
      "[Iter.  210]  loss:658.942078  pct:-0.796231911\n",
      "[Iter.  220]  loss:653.766602  pct:-0.785422004\n",
      "[Iter.  230]  loss:648.700684  pct:-0.774881733\n",
      "[Iter.  240]  loss:643.739746  pct:-0.764749849\n",
      "[Iter.  250]  loss:638.879700  pct:-0.754970687\n",
      "[Iter.  260]  loss:634.117004  pct:-0.745476075\n",
      "[Iter.  270]  loss:629.447754  pct:-0.736338949\n",
      "[Iter.  280]  loss:624.868652  pct:-0.727479212\n",
      "[Iter.  290]  loss:620.376953  pct:-0.718822940\n",
      "[Iter.  300]  loss:615.968445  pct:-0.710617678\n",
      "[Iter.  310]  loss:611.640747  pct:-0.702584327\n",
      "[Iter.  320]  loss:607.386597  pct:-0.695530900\n",
      "[Iter.  330]  loss:603.208923  pct:-0.687811249\n",
      "[Iter.  340]  loss:599.106384  pct:-0.680119094\n",
      "[Iter.  350]  loss:595.074341  pct:-0.673009596\n",
      "[Iter.  360]  loss:591.109985  pct:-0.666194994\n",
      "[Iter.  370]  loss:587.210999  pct:-0.659604289\n",
      "[Iter.  380]  loss:583.374329  pct:-0.653371604\n",
      "[Iter.  390]  loss:579.597839  pct:-0.647352664\n",
      "[Iter.  400]  loss:575.878418  pct:-0.641724509\n",
      "[Iter.  410]  loss:572.213684  pct:-0.636372848\n",
      "[Iter.  420]  loss:568.599854  pct:-0.631552629\n",
      "[Iter.  430]  loss:565.034180  pct:-0.627097212\n",
      "[Iter.  440]  loss:561.516968  pct:-0.622477726\n",
      "[Iter.  450]  loss:558.050842  pct:-0.617278851\n",
      "[Iter.  460]  loss:554.633850  pct:-0.612308401\n",
      "[Iter.  470]  loss:551.268005  pct:-0.606858872\n",
      "[Iter.  480]  loss:547.953491  pct:-0.601252771\n",
      "[Iter.  490]  loss:544.693848  pct:-0.594875953\n",
      "[Iter.  500]  loss:541.483032  pct:-0.589471580\n",
      "[Iter.  510]  loss:538.319214  pct:-0.584287627\n",
      "[Iter.  520]  loss:535.201111  pct:-0.579229377\n",
      "[Iter.  530]  loss:532.127380  pct:-0.574313171\n",
      "[Iter.  540]  loss:529.096497  pct:-0.569578620\n",
      "[Iter.  550]  loss:526.110535  pct:-0.564351103\n",
      "[Iter.  560]  loss:523.163574  pct:-0.560140931\n",
      "[Iter.  570]  loss:520.259338  pct:-0.555129597\n",
      "[Iter.  580]  loss:517.394104  pct:-0.550731945\n",
      "[Iter.  590]  loss:514.560608  pct:-0.547647542\n",
      "[Iter.  600]  loss:511.766693  pct:-0.542970984\n",
      "[Iter.  610]  loss:509.012207  pct:-0.538230823\n",
      "[Iter.  620]  loss:506.293640  pct:-0.534086778\n",
      "[Iter.  630]  loss:503.612335  pct:-0.529594828\n",
      "[Iter.  640]  loss:500.964264  pct:-0.525815415\n",
      "[Iter.  650]  loss:498.347565  pct:-0.522332511\n",
      "[Iter.  660]  loss:495.762421  pct:-0.518743188\n",
      "[Iter.  670]  loss:493.207550  pct:-0.515341724\n",
      "[Iter.  680]  loss:490.683777  pct:-0.511706115\n",
      "[Iter.  690]  loss:488.189362  pct:-0.508354953\n",
      "[Iter.  700]  loss:485.722778  pct:-0.505251332\n",
      "[Iter.  710]  loss:483.283722  pct:-0.502149890\n",
      "[Iter.  720]  loss:480.871704  pct:-0.499089399\n",
      "[Iter.  730]  loss:478.486725  pct:-0.495969970\n",
      "[Iter.  740]  loss:476.128113  pct:-0.492931557\n",
      "[Iter.  750]  loss:473.794891  pct:-0.490040679\n",
      "[Iter.  760]  loss:471.486511  pct:-0.487210852\n",
      "[Iter.  770]  loss:469.202484  pct:-0.484431059\n",
      "[Iter.  780]  loss:466.941223  pct:-0.481937130\n",
      "[Iter.  790]  loss:464.703369  pct:-0.479258179\n",
      "[Iter.  800]  loss:462.489471  pct:-0.476410943\n",
      "[Iter.  810]  loss:460.297974  pct:-0.473848150\n",
      "[Iter.  820]  loss:458.128876  pct:-0.471237769\n",
      "[Iter.  830]  loss:455.980865  pct:-0.468865939\n",
      "[Iter.  840]  loss:453.836426  pct:-0.470291598\n",
      "[Iter.  850]  loss:451.681244  pct:-0.474880764\n",
      "[Iter.  860]  loss:449.590942  pct:-0.462782447\n",
      "[Iter.  870]  loss:447.525024  pct:-0.459510585\n",
      "[Iter.  880]  loss:445.479980  pct:-0.456967507\n",
      "[Iter.  890]  loss:443.454468  pct:-0.454680970\n",
      "[Iter.  900]  loss:441.431763  pct:-0.456124636\n",
      "[Iter.  910]  loss:439.411072  pct:-0.457758387\n",
      "[Iter.  920]  loss:437.438873  pct:-0.448827673\n",
      "[Iter.  930]  loss:435.487457  pct:-0.446100275\n",
      "[Iter.  940]  loss:433.554840  pct:-0.443782514\n",
      "[Iter.  950]  loss:431.640717  pct:-0.441495137\n",
      "[Iter.  960]  loss:429.742920  pct:-0.439670438\n",
      "[Iter.  970]  loss:427.862213  pct:-0.437635316\n",
      "[Iter.  980]  loss:425.998383  pct:-0.435614670\n",
      "[Iter.  990]  loss:424.150818  pct:-0.433702280\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:3.064456  pct:100.000000000\n",
      "[Iter.    2]  loss:3.063271  pct:-0.038667239\n",
      "[Iter.    4]  loss:3.062701  pct:-0.018601700\n",
      "[Iter.    6]  loss:3.062441  pct:-0.008485199\n",
      "[Iter.    8]  loss:3.062253  pct:-0.006150345\n",
      "[Iter.   10]  loss:3.062156  pct:-0.003161005\n",
      "[Iter.   12]  loss:3.062113  pct:-0.001409261\n",
      "[Iter.   14]  loss:3.062063  pct:-0.001627291\n",
      "[Iter.   16]  loss:3.062071  pct:0.000264731\n",
      "[Iter.   18]  loss:3.062024  pct:-0.001541665\n",
      "[Iter.   20]  loss:3.062020  pct:-0.000116795\n",
      "[Iter.   22]  loss:3.062031  pct:0.000365957\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 3.062031\n",
      "Best loss: 3.062031 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:11<00:00, 76.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:323.579529  pct:100.000000000\n",
      "[Iter.   10]  loss:69.557198  pct:-78.503834839\n",
      "[Iter.   20]  loss:63.372360  pct:-8.891728760\n",
      "[Iter.   30]  loss:58.870975  pct:-7.103072568\n",
      "[Iter.   40]  loss:55.714100  pct:-5.362363344\n",
      "[Iter.   50]  loss:52.969143  pct:-4.926862277\n",
      "[Iter.   60]  loss:50.534214  pct:-4.596881807\n",
      "[Iter.   70]  loss:48.347057  pct:-4.328071030\n",
      "[Iter.   80]  loss:46.362324  pct:-4.105179696\n",
      "[Iter.   90]  loss:44.547863  pct:-3.913653603\n",
      "[Iter.  100]  loss:42.878765  pct:-3.746751893\n",
      "[Iter.  110]  loss:41.334938  pct:-3.600446638\n",
      "[Iter.  120]  loss:39.900307  pct:-3.470747545\n",
      "[Iter.  130]  loss:38.561684  pct:-3.354919191\n",
      "[Iter.  140]  loss:37.308167  pct:-3.250680551\n",
      "[Iter.  150]  loss:36.130569  pct:-3.156405571\n",
      "[Iter.  160]  loss:35.021076  pct:-3.070788178\n",
      "[Iter.  170]  loss:33.972683  pct:-2.993606603\n",
      "[Iter.  180]  loss:32.980549  pct:-2.920387818\n",
      "[Iter.  190]  loss:32.039425  pct:-2.853572772\n",
      "[Iter.  200]  loss:31.144943  pct:-2.791815589\n",
      "[Iter.  210]  loss:30.293213  pct:-2.734730772\n",
      "[Iter.  220]  loss:29.480970  pct:-2.681268939\n",
      "[Iter.  230]  loss:28.705194  pct:-2.631446317\n",
      "[Iter.  240]  loss:27.963226  pct:-2.584787069\n",
      "[Iter.  250]  loss:27.252697  pct:-2.540941876\n",
      "[Iter.  260]  loss:26.571491  pct:-2.499590223\n",
      "[Iter.  270]  loss:25.917698  pct:-2.460506748\n",
      "[Iter.  280]  loss:25.289583  pct:-2.423497267\n",
      "[Iter.  290]  loss:24.685545  pct:-2.388486333\n",
      "[Iter.  300]  loss:24.104168  pct:-2.355131435\n",
      "[Iter.  310]  loss:23.544115  pct:-2.323469008\n",
      "[Iter.  320]  loss:23.004190  pct:-2.293246614\n",
      "[Iter.  330]  loss:22.483267  pct:-2.264472709\n",
      "[Iter.  340]  loss:21.980362  pct:-2.236796351\n",
      "[Iter.  350]  loss:21.494486  pct:-2.210500831\n",
      "[Iter.  360]  loss:21.024836  pct:-2.184980240\n",
      "[Iter.  370]  loss:20.570589  pct:-2.160523535\n",
      "[Iter.  380]  loss:20.130997  pct:-2.136994522\n",
      "[Iter.  390]  loss:19.705353  pct:-2.114370824\n",
      "[Iter.  400]  loss:19.293020  pct:-2.092489992\n",
      "[Iter.  410]  loss:18.893385  pct:-2.071398411\n",
      "[Iter.  420]  loss:18.505922  pct:-2.050784533\n",
      "[Iter.  430]  loss:18.130075  pct:-2.030954504\n",
      "[Iter.  440]  loss:17.765358  pct:-2.011671073\n",
      "[Iter.  450]  loss:17.411303  pct:-1.992953957\n",
      "[Iter.  460]  loss:17.067444  pct:-1.974916682\n",
      "[Iter.  470]  loss:16.733389  pct:-1.957264075\n",
      "[Iter.  480]  loss:16.408779  pct:-1.939892501\n",
      "[Iter.  490]  loss:16.093245  pct:-1.922962025\n",
      "[Iter.  500]  loss:15.786388  pct:-1.906738908\n",
      "[Iter.  510]  loss:15.487927  pct:-1.890622180\n",
      "[Iter.  520]  loss:15.197553  pct:-1.874845792\n",
      "[Iter.  530]  loss:14.914948  pct:-1.859543949\n",
      "[Iter.  540]  loss:14.639848  pct:-1.844456738\n",
      "[Iter.  550]  loss:14.372001  pct:-1.829575455\n",
      "[Iter.  560]  loss:14.111164  pct:-1.814894160\n",
      "[Iter.  570]  loss:13.857075  pct:-1.800626467\n",
      "[Iter.  580]  loss:13.609510  pct:-1.786555391\n",
      "[Iter.  590]  loss:13.368244  pct:-1.772776853\n",
      "[Iter.  600]  loss:13.133093  pct:-1.759028994\n",
      "[Iter.  610]  loss:12.903838  pct:-1.745626294\n",
      "[Iter.  620]  loss:12.680321  pct:-1.732177784\n",
      "[Iter.  630]  loss:12.462336  pct:-1.719082330\n",
      "[Iter.  640]  loss:12.249705  pct:-1.706183166\n",
      "[Iter.  650]  loss:12.042252  pct:-1.693540558\n",
      "[Iter.  660]  loss:11.839848  pct:-1.680782209\n",
      "[Iter.  670]  loss:11.642329  pct:-1.668250774\n",
      "[Iter.  680]  loss:11.449550  pct:-1.655850281\n",
      "[Iter.  690]  loss:11.261377  pct:-1.643491192\n",
      "[Iter.  700]  loss:11.077677  pct:-1.631244173\n",
      "[Iter.  710]  loss:10.898311  pct:-1.619167226\n",
      "[Iter.  720]  loss:10.723149  pct:-1.607234067\n",
      "[Iter.  730]  loss:10.552090  pct:-1.595236657\n",
      "[Iter.  740]  loss:10.385011  pct:-1.583373310\n",
      "[Iter.  750]  loss:10.221807  pct:-1.571526923\n",
      "[Iter.  760]  loss:10.062361  pct:-1.559868121\n",
      "[Iter.  770]  loss:9.906591  pct:-1.548039787\n",
      "[Iter.  780]  loss:9.754373  pct:-1.536540797\n",
      "[Iter.  790]  loss:9.605630  pct:-1.524882039\n",
      "[Iter.  800]  loss:9.460252  pct:-1.513467768\n",
      "[Iter.  810]  loss:9.318170  pct:-1.501886178\n",
      "[Iter.  820]  loss:9.179287  pct:-1.490449767\n",
      "[Iter.  830]  loss:9.043538  pct:-1.478860655\n",
      "[Iter.  840]  loss:8.910808  pct:-1.467683142\n",
      "[Iter.  850]  loss:8.781047  pct:-1.456217527\n",
      "[Iter.  860]  loss:8.654186  pct:-1.444709503\n",
      "[Iter.  870]  loss:8.530130  pct:-1.433478075\n",
      "[Iter.  880]  loss:8.408808  pct:-1.422283439\n",
      "[Iter.  890]  loss:8.290158  pct:-1.411014334\n",
      "[Iter.  900]  loss:8.174139  pct:-1.399481713\n",
      "[Iter.  910]  loss:8.060639  pct:-1.388521055\n",
      "[Iter.  920]  loss:7.949609  pct:-1.377435418\n",
      "[Iter.  930]  loss:7.841017  pct:-1.366004662\n",
      "[Iter.  940]  loss:7.734776  pct:-1.354941876\n",
      "[Iter.  950]  loss:7.630843  pct:-1.343714855\n",
      "[Iter.  960]  loss:7.529144  pct:-1.332728282\n",
      "[Iter.  970]  loss:7.429667  pct:-1.321223375\n",
      "[Iter.  980]  loss:7.332320  pct:-1.310250558\n",
      "[Iter.  990]  loss:7.237077  pct:-1.298953826\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.738398  pct:100.000000000\n",
      "[Iter.    2]  loss:2.738451  pct:0.001950256\n",
      "[Iter.    4]  loss:2.738451  pct:0.000000000\n",
      "[Iter.    6]  loss:2.738451  pct:0.000008706\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.738451\n",
      "Best loss: 2.738451 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 15%|█▌        | 1523/10000 [00:24<02:16, 62.03it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:289.608032  pct:100.000000000\n",
      "[Iter.   10]  loss:63.736629  pct:-77.992104428\n",
      "[Iter.   20]  loss:56.787125  pct:-10.903470906\n",
      "[Iter.   30]  loss:53.321926  pct:-6.102084829\n",
      "[Iter.   40]  loss:50.365013  pct:-5.545397944\n",
      "[Iter.   50]  loss:47.782478  pct:-5.127636488\n",
      "[Iter.   60]  loss:45.493408  pct:-4.790605697\n",
      "[Iter.   70]  loss:43.441120  pct:-4.511176754\n",
      "[Iter.   80]  loss:41.584183  pct:-4.274607566\n",
      "[Iter.   90]  loss:39.890919  pct:-4.071894398\n",
      "[Iter.  100]  loss:38.336681  pct:-3.896218526\n",
      "[Iter.  110]  loss:36.902073  pct:-3.742130013\n",
      "[Iter.  120]  loss:35.571449  pct:-3.605823527\n",
      "[Iter.  130]  loss:34.332031  pct:-3.484305686\n",
      "[Iter.  140]  loss:33.173290  pct:-3.375101779\n",
      "[Iter.  150]  loss:32.086414  pct:-3.276358502\n",
      "[Iter.  160]  loss:31.063923  pct:-3.186680332\n",
      "[Iter.  170]  loss:30.099497  pct:-3.104649868\n",
      "[Iter.  180]  loss:29.187702  pct:-3.029268786\n",
      "[Iter.  190]  loss:28.323807  pct:-2.959792487\n",
      "[Iter.  200]  loss:27.503691  pct:-2.895500771\n",
      "[Iter.  210]  loss:26.723719  pct:-2.835881498\n",
      "[Iter.  220]  loss:25.980745  pct:-2.780201878\n",
      "[Iter.  230]  loss:25.271910  pct:-2.728311267\n",
      "[Iter.  240]  loss:24.594751  pct:-2.679490246\n",
      "[Iter.  250]  loss:23.946993  pct:-2.633726499\n",
      "[Iter.  260]  loss:23.326668  pct:-2.590409125\n",
      "[Iter.  270]  loss:22.731903  pct:-2.549719981\n",
      "[Iter.  280]  loss:22.161110  pct:-2.510978293\n",
      "[Iter.  290]  loss:21.612770  pct:-2.474333847\n",
      "[Iter.  300]  loss:21.085543  pct:-2.439425394\n",
      "[Iter.  310]  loss:20.578201  pct:-2.406110161\n",
      "[Iter.  320]  loss:20.089590  pct:-2.374411710\n",
      "[Iter.  330]  loss:19.618700  pct:-2.343950491\n",
      "[Iter.  340]  loss:19.164534  pct:-2.314966903\n",
      "[Iter.  350]  loss:18.726252  pct:-2.286943276\n",
      "[Iter.  360]  loss:18.303020  pct:-2.260095260\n",
      "[Iter.  370]  loss:17.894104  pct:-2.234147494\n",
      "[Iter.  380]  loss:17.498785  pct:-2.209213632\n",
      "[Iter.  390]  loss:17.116436  pct:-2.185003209\n",
      "[Iter.  400]  loss:16.746416  pct:-2.161781300\n",
      "[Iter.  410]  loss:16.388189  pct:-2.139125017\n",
      "[Iter.  420]  loss:16.041246  pct:-2.117030106\n",
      "[Iter.  430]  loss:15.705043  pct:-2.095869401\n",
      "[Iter.  440]  loss:15.379144  pct:-2.075124070\n",
      "[Iter.  450]  loss:15.063095  pct:-2.055046939\n",
      "[Iter.  460]  loss:14.756499  pct:-2.035410388\n",
      "[Iter.  470]  loss:14.458969  pct:-2.016265297\n",
      "[Iter.  480]  loss:14.170177  pct:-1.997325036\n",
      "[Iter.  490]  loss:13.889750  pct:-1.978994256\n",
      "[Iter.  500]  loss:13.617341  pct:-1.961219566\n",
      "[Iter.  510]  loss:13.352687  pct:-1.943508345\n",
      "[Iter.  520]  loss:13.095498  pct:-1.926120183\n",
      "[Iter.  530]  loss:12.845469  pct:-1.909278763\n",
      "[Iter.  540]  loss:12.602355  pct:-1.892601405\n",
      "[Iter.  550]  loss:12.365943  pct:-1.875935476\n",
      "[Iter.  560]  loss:12.135932  pct:-1.860035965\n",
      "[Iter.  570]  loss:11.912157  pct:-1.843903794\n",
      "[Iter.  580]  loss:11.694376  pct:-1.828225281\n",
      "[Iter.  590]  loss:11.482385  pct:-1.812762906\n",
      "[Iter.  600]  loss:11.275999  pct:-1.797410714\n",
      "[Iter.  610]  loss:11.075037  pct:-1.782210742\n",
      "[Iter.  620]  loss:10.879313  pct:-1.767258089\n",
      "[Iter.  630]  loss:10.688675  pct:-1.752294442\n",
      "[Iter.  640]  loss:10.502950  pct:-1.737588741\n",
      "[Iter.  650]  loss:10.321973  pct:-1.723105152\n",
      "[Iter.  660]  loss:10.145608  pct:-1.708635561\n",
      "[Iter.  670]  loss:9.973732  pct:-1.694092208\n",
      "[Iter.  680]  loss:9.806173  pct:-1.679999725\n",
      "[Iter.  690]  loss:9.642818  pct:-1.665837103\n",
      "[Iter.  700]  loss:9.483537  pct:-1.651817168\n",
      "[Iter.  710]  loss:9.328206  pct:-1.637897997\n",
      "[Iter.  720]  loss:9.176751  pct:-1.623623283\n",
      "[Iter.  730]  loss:9.028994  pct:-1.610128988\n",
      "[Iter.  740]  loss:8.884888  pct:-1.596035146\n",
      "[Iter.  750]  loss:8.744277  pct:-1.582582692\n",
      "[Iter.  760]  loss:8.607086  pct:-1.568921236\n",
      "[Iter.  770]  loss:8.473236  pct:-1.555115109\n",
      "[Iter.  780]  loss:8.342616  pct:-1.541559818\n",
      "[Iter.  790]  loss:8.215147  pct:-1.527926751\n",
      "[Iter.  800]  loss:8.090732  pct:-1.514463434\n",
      "[Iter.  810]  loss:7.969284  pct:-1.501076116\n",
      "[Iter.  820]  loss:7.850741  pct:-1.487488720\n",
      "[Iter.  830]  loss:7.735010  pct:-1.474150153\n",
      "[Iter.  840]  loss:7.622014  pct:-1.460833656\n",
      "[Iter.  850]  loss:7.511682  pct:-1.447544068\n",
      "[Iter.  860]  loss:7.403971  pct:-1.433917393\n",
      "[Iter.  870]  loss:7.298785  pct:-1.420669929\n",
      "[Iter.  880]  loss:7.196059  pct:-1.407439545\n",
      "[Iter.  890]  loss:7.095724  pct:-1.394306587\n",
      "[Iter.  900]  loss:6.997741  pct:-1.380872319\n",
      "[Iter.  910]  loss:6.902037  pct:-1.367635717\n",
      "[Iter.  920]  loss:6.808548  pct:-1.354515604\n",
      "[Iter.  930]  loss:6.717226  pct:-1.341283715\n",
      "[Iter.  940]  loss:6.628009  pct:-1.328177863\n",
      "[Iter.  950]  loss:6.540857  pct:-1.314904673\n",
      "[Iter.  960]  loss:6.455712  pct:-1.301747911\n",
      "[Iter.  970]  loss:6.372529  pct:-1.288514943\n",
      "[Iter.  980]  loss:6.291252  pct:-1.275426024\n",
      "[Iter.  990]  loss:6.211845  pct:-1.262177013\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.726088  pct:100.000000000\n",
      "[Iter.    2]  loss:2.726130  pct:0.001574247\n",
      "[Iter.    4]  loss:2.726127  pct:-0.000131185\n",
      "[Iter.    6]  loss:2.726126  pct:-0.000026237\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.726126\n",
      "Best loss: 2.726126 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 28%|██▊       | 2766/10000 [00:41<01:49, 66.14it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:612.848206  pct:100.000000000\n",
      "[Iter.   10]  loss:103.584969  pct:-83.097778537\n",
      "[Iter.   20]  loss:98.850464  pct:-4.570648392\n",
      "[Iter.   30]  loss:94.482826  pct:-4.418429073\n",
      "[Iter.   40]  loss:90.598518  pct:-4.111125816\n",
      "[Iter.   50]  loss:87.329514  pct:-3.608232100\n",
      "[Iter.   60]  loss:84.384987  pct:-3.371742900\n",
      "[Iter.   70]  loss:81.668907  pct:-3.218676464\n",
      "[Iter.   80]  loss:79.152779  pct:-3.080889199\n",
      "[Iter.   90]  loss:76.810883  pct:-2.958703532\n",
      "[Iter.  100]  loss:74.620689  pct:-2.851409986\n",
      "[Iter.  110]  loss:72.563553  pct:-2.756791116\n",
      "[Iter.  120]  loss:70.625015  pct:-2.671503146\n",
      "[Iter.  130]  loss:68.793320  pct:-2.593550670\n",
      "[Iter.  140]  loss:67.058189  pct:-2.522236632\n",
      "[Iter.  150]  loss:65.410629  pct:-2.456911131\n",
      "[Iter.  160]  loss:63.842812  pct:-2.396885193\n",
      "[Iter.  170]  loss:62.348129  pct:-2.341191240\n",
      "[Iter.  180]  loss:60.920555  pct:-2.289682424\n",
      "[Iter.  190]  loss:59.554951  pct:-2.241615163\n",
      "[Iter.  200]  loss:58.246677  pct:-2.196749892\n",
      "[Iter.  210]  loss:56.991657  pct:-2.154663918\n",
      "[Iter.  220]  loss:55.786121  pct:-2.115284845\n",
      "[Iter.  230]  loss:54.626701  pct:-2.078330569\n",
      "[Iter.  240]  loss:53.510437  pct:-2.043440873\n",
      "[Iter.  250]  loss:52.434608  pct:-2.010502273\n",
      "[Iter.  260]  loss:51.396660  pct:-1.979510554\n",
      "[Iter.  270]  loss:50.394436  pct:-1.949978795\n",
      "[Iter.  280]  loss:49.425793  pct:-1.922123289\n",
      "[Iter.  290]  loss:48.488895  pct:-1.895563484\n",
      "[Iter.  300]  loss:47.581970  pct:-1.870377111\n",
      "[Iter.  310]  loss:46.703480  pct:-1.846267492\n",
      "[Iter.  320]  loss:45.851803  pct:-1.823583479\n",
      "[Iter.  330]  loss:45.025719  pct:-1.801639382\n",
      "[Iter.  340]  loss:44.223961  pct:-1.780666330\n",
      "[Iter.  350]  loss:43.445381  pct:-1.760538171\n",
      "[Iter.  360]  loss:42.688873  pct:-1.741284927\n",
      "[Iter.  370]  loss:41.953354  pct:-1.722976861\n",
      "[Iter.  380]  loss:41.237934  pct:-1.705274318\n",
      "[Iter.  390]  loss:40.541767  pct:-1.688171358\n",
      "[Iter.  400]  loss:39.863972  pct:-1.671844762\n",
      "[Iter.  410]  loss:39.203732  pct:-1.656232796\n",
      "[Iter.  420]  loss:38.560375  pct:-1.641058894\n",
      "[Iter.  430]  loss:37.933250  pct:-1.626345135\n",
      "[Iter.  440]  loss:37.321625  pct:-1.612373484\n",
      "[Iter.  450]  loss:36.724983  pct:-1.598648356\n",
      "[Iter.  460]  loss:36.142761  pct:-1.585356708\n",
      "[Iter.  470]  loss:35.574341  pct:-1.572708866\n",
      "[Iter.  480]  loss:35.019222  pct:-1.560446513\n",
      "[Iter.  490]  loss:34.476959  pct:-1.548472513\n",
      "[Iter.  500]  loss:33.947067  pct:-1.536945194\n",
      "[Iter.  510]  loss:33.429199  pct:-1.525516293\n",
      "[Iter.  520]  loss:32.922821  pct:-1.514778055\n",
      "[Iter.  530]  loss:32.427654  pct:-1.504022933\n",
      "[Iter.  540]  loss:31.943249  pct:-1.493803757\n",
      "[Iter.  550]  loss:31.469315  pct:-1.483675556\n",
      "[Iter.  560]  loss:31.005424  pct:-1.474102890\n",
      "[Iter.  570]  loss:30.551321  pct:-1.464593622\n",
      "[Iter.  580]  loss:30.106699  pct:-1.455328362\n",
      "[Iter.  590]  loss:29.671247  pct:-1.446360851\n",
      "[Iter.  600]  loss:29.244736  pct:-1.437458148\n",
      "[Iter.  610]  loss:28.826849  pct:-1.428929767\n",
      "[Iter.  620]  loss:28.417345  pct:-1.420564339\n",
      "[Iter.  630]  loss:28.015976  pct:-1.412408845\n",
      "[Iter.  640]  loss:27.622532  pct:-1.404356079\n",
      "[Iter.  650]  loss:27.236767  pct:-1.396559436\n",
      "[Iter.  660]  loss:26.858427  pct:-1.389077382\n",
      "[Iter.  670]  loss:26.487391  pct:-1.381452938\n",
      "[Iter.  680]  loss:26.123405  pct:-1.374182411\n",
      "[Iter.  690]  loss:25.766298  pct:-1.367000803\n",
      "[Iter.  700]  loss:25.415882  pct:-1.359978758\n",
      "[Iter.  710]  loss:25.071999  pct:-1.353026084\n",
      "[Iter.  720]  loss:24.734463  pct:-1.346266261\n",
      "[Iter.  730]  loss:24.403076  pct:-1.339776690\n",
      "[Iter.  740]  loss:24.077761  pct:-1.333092079\n",
      "[Iter.  750]  loss:23.758282  pct:-1.326863377\n",
      "[Iter.  760]  loss:23.444544  pct:-1.320541078\n",
      "[Iter.  770]  loss:23.136396  pct:-1.314367354\n",
      "[Iter.  780]  loss:22.833662  pct:-1.308476781\n",
      "[Iter.  790]  loss:22.536266  pct:-1.302444197\n",
      "[Iter.  800]  loss:22.244028  pct:-1.296746459\n",
      "[Iter.  810]  loss:21.956915  pct:-1.290742794\n",
      "[Iter.  820]  loss:21.674726  pct:-1.285191553\n",
      "[Iter.  830]  loss:21.397419  pct:-1.279404889\n",
      "[Iter.  840]  loss:21.124832  pct:-1.273923845\n",
      "[Iter.  850]  loss:20.856863  pct:-1.268503009\n",
      "[Iter.  860]  loss:20.593458  pct:-1.262916892\n",
      "[Iter.  870]  loss:20.334427  pct:-1.257832917\n",
      "[Iter.  880]  loss:20.079742  pct:-1.252479107\n",
      "[Iter.  890]  loss:19.829266  pct:-1.247410608\n",
      "[Iter.  900]  loss:19.582956  pct:-1.242150292\n",
      "[Iter.  910]  loss:19.340696  pct:-1.237096051\n",
      "[Iter.  920]  loss:19.102436  pct:-1.231911535\n",
      "[Iter.  930]  loss:18.867983  pct:-1.227347133\n",
      "[Iter.  940]  loss:18.637358  pct:-1.222309530\n",
      "[Iter.  950]  loss:18.410505  pct:-1.217191946\n",
      "[Iter.  960]  loss:18.187279  pct:-1.212495495\n",
      "[Iter.  970]  loss:17.967619  pct:-1.207766199\n",
      "[Iter.  980]  loss:17.751421  pct:-1.203264429\n",
      "[Iter.  990]  loss:17.538651  pct:-1.198610874\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.728192  pct:100.000000000\n",
      "[Iter.    2]  loss:2.728137  pct:-0.002018725\n",
      "[Iter.    4]  loss:2.728114  pct:-0.000838968\n",
      "[Iter.    6]  loss:2.728107  pct:-0.000244701\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.728107\n",
      "Best loss: 2.728107 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 10%|█         | 1023/10000 [00:16<02:20, 63.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:396.358063  pct:100.000000000\n",
      "[Iter.   10]  loss:87.443855  pct:-77.938168571\n",
      "[Iter.   20]  loss:80.092506  pct:-8.406935917\n",
      "[Iter.   30]  loss:75.099693  pct:-6.233808048\n",
      "[Iter.   40]  loss:71.114174  pct:-5.306971619\n",
      "[Iter.   50]  loss:67.659187  pct:-4.858365616\n",
      "[Iter.   60]  loss:64.611320  pct:-4.504734600\n",
      "[Iter.   70]  loss:61.888542  pct:-4.214088645\n",
      "[Iter.   80]  loss:59.426998  pct:-3.977382485\n",
      "[Iter.   90]  loss:57.183571  pct:-3.775097762\n",
      "[Iter.  100]  loss:55.123848  pct:-3.601948723\n",
      "[Iter.  110]  loss:53.222260  pct:-3.449665635\n",
      "[Iter.  120]  loss:51.457771  pct:-3.315320011\n",
      "[Iter.  130]  loss:49.813057  pct:-3.196240945\n",
      "[Iter.  140]  loss:48.274059  pct:-3.089546686\n",
      "[Iter.  150]  loss:46.829025  pct:-2.993396553\n",
      "[Iter.  160]  loss:45.468109  pct:-2.906138084\n",
      "[Iter.  170]  loss:44.182930  pct:-2.826550659\n",
      "[Iter.  180]  loss:42.966198  pct:-2.753850922\n",
      "[Iter.  190]  loss:41.811771  pct:-2.686825061\n",
      "[Iter.  200]  loss:40.714199  pct:-2.625031875\n",
      "[Iter.  210]  loss:39.668739  pct:-2.567801335\n",
      "[Iter.  220]  loss:38.671215  pct:-2.514635652\n",
      "[Iter.  230]  loss:37.717941  pct:-2.465073238\n",
      "[Iter.  240]  loss:36.805614  pct:-2.418813916\n",
      "[Iter.  250]  loss:35.931404  pct:-2.375209245\n",
      "[Iter.  260]  loss:35.092579  pct:-2.334518359\n",
      "[Iter.  270]  loss:34.286816  pct:-2.296107240\n",
      "[Iter.  280]  loss:33.511971  pct:-2.259892349\n",
      "[Iter.  290]  loss:32.766090  pct:-2.225712530\n",
      "[Iter.  300]  loss:32.047466  pct:-2.193194569\n",
      "[Iter.  310]  loss:31.354427  pct:-2.162538949\n",
      "[Iter.  320]  loss:30.685549  pct:-2.133282640\n",
      "[Iter.  330]  loss:30.039480  pct:-2.105448977\n",
      "[Iter.  340]  loss:29.414957  pct:-2.079007887\n",
      "[Iter.  350]  loss:28.810848  pct:-2.053747043\n",
      "[Iter.  360]  loss:28.226118  pct:-2.029548535\n",
      "[Iter.  370]  loss:27.659788  pct:-2.006403978\n",
      "[Iter.  380]  loss:27.110994  pct:-1.984085309\n",
      "[Iter.  390]  loss:26.578867  pct:-1.962773382\n",
      "[Iter.  400]  loss:26.062605  pct:-1.942377962\n",
      "[Iter.  410]  loss:25.561506  pct:-1.922672867\n",
      "[Iter.  420]  loss:25.074911  pct:-1.903624726\n",
      "[Iter.  430]  loss:24.602158  pct:-1.885364708\n",
      "[Iter.  440]  loss:24.142700  pct:-1.867549201\n",
      "[Iter.  450]  loss:23.695911  pct:-1.850616477\n",
      "[Iter.  460]  loss:23.261332  pct:-1.833986639\n",
      "[Iter.  470]  loss:22.838446  pct:-1.817978020\n",
      "[Iter.  480]  loss:22.426792  pct:-1.802458559\n",
      "[Iter.  490]  loss:22.025909  pct:-1.787516995\n",
      "[Iter.  500]  loss:21.635414  pct:-1.772890702\n",
      "[Iter.  510]  loss:21.254911  pct:-1.758703109\n",
      "[Iter.  520]  loss:20.884020  pct:-1.744968792\n",
      "[Iter.  530]  loss:20.522432  pct:-1.731407684\n",
      "[Iter.  540]  loss:20.169798  pct:-1.718287698\n",
      "[Iter.  550]  loss:19.825785  pct:-1.705585826\n",
      "[Iter.  560]  loss:19.490086  pct:-1.693244867\n",
      "[Iter.  570]  loss:19.162437  pct:-1.681101713\n",
      "[Iter.  580]  loss:18.842590  pct:-1.669135818\n",
      "[Iter.  590]  loss:18.530273  pct:-1.657505094\n",
      "[Iter.  600]  loss:18.225264  pct:-1.646008317\n",
      "[Iter.  610]  loss:17.927303  pct:-1.634875017\n",
      "[Iter.  620]  loss:17.636173  pct:-1.623947901\n",
      "[Iter.  630]  loss:17.351673  pct:-1.613162436\n",
      "[Iter.  640]  loss:17.073576  pct:-1.602710878\n",
      "[Iter.  650]  loss:16.801727  pct:-1.592218754\n",
      "[Iter.  660]  loss:16.535889  pct:-1.582210081\n",
      "[Iter.  670]  loss:16.275900  pct:-1.572269806\n",
      "[Iter.  680]  loss:16.021618  pct:-1.562322203\n",
      "[Iter.  690]  loss:15.772861  pct:-1.552635721\n",
      "[Iter.  700]  loss:15.529458  pct:-1.543172722\n",
      "[Iter.  710]  loss:15.291266  pct:-1.533805004\n",
      "[Iter.  720]  loss:15.058168  pct:-1.524386688\n",
      "[Iter.  730]  loss:14.830004  pct:-1.515221949\n",
      "[Iter.  740]  loss:14.606623  pct:-1.506277722\n",
      "[Iter.  750]  loss:14.387910  pct:-1.497353709\n",
      "[Iter.  760]  loss:14.173769  pct:-1.488339124\n",
      "[Iter.  770]  loss:13.964024  pct:-1.479813923\n",
      "[Iter.  780]  loss:13.758599  pct:-1.471096833\n",
      "[Iter.  790]  loss:13.557384  pct:-1.462472596\n",
      "[Iter.  800]  loss:13.360246  pct:-1.454099400\n",
      "[Iter.  810]  loss:13.167114  pct:-1.445568076\n",
      "[Iter.  820]  loss:12.977877  pct:-1.437198697\n",
      "[Iter.  830]  loss:12.792447  pct:-1.428812878\n",
      "[Iter.  840]  loss:12.610725  pct:-1.420538901\n",
      "[Iter.  850]  loss:12.432606  pct:-1.412445785\n",
      "[Iter.  860]  loss:12.257998  pct:-1.404430258\n",
      "[Iter.  870]  loss:12.086826  pct:-1.396411841\n",
      "[Iter.  880]  loss:11.919030  pct:-1.388256358\n",
      "[Iter.  890]  loss:11.754518  pct:-1.380251847\n",
      "[Iter.  900]  loss:11.593206  pct:-1.372333223\n",
      "[Iter.  910]  loss:11.435031  pct:-1.364380680\n",
      "[Iter.  920]  loss:11.279888  pct:-1.356732526\n",
      "[Iter.  930]  loss:11.127745  pct:-1.348803076\n",
      "[Iter.  940]  loss:10.978503  pct:-1.341165275\n",
      "[Iter.  950]  loss:10.832111  pct:-1.333441049\n",
      "[Iter.  960]  loss:10.688509  pct:-1.325709887\n",
      "[Iter.  970]  loss:10.547645  pct:-1.317904793\n",
      "[Iter.  980]  loss:10.409444  pct:-1.310252335\n",
      "[Iter.  990]  loss:10.273847  pct:-1.302636634\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.748283  pct:100.000000000\n",
      "[Iter.    2]  loss:2.748303  pct:0.000720040\n",
      "[Iter.    4]  loss:2.748301  pct:-0.000078076\n",
      "[Iter.    6]  loss:2.748300  pct:-0.000008675\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.748300\n",
      "Best loss: 2.748300 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 20%|█▉        | 1952/10000 [00:40<02:48, 47.82it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:597.464722  pct:100.000000000\n",
      "[Iter.   10]  loss:103.905327  pct:-82.608959039\n",
      "[Iter.   20]  loss:98.990074  pct:-4.730510778\n",
      "[Iter.   30]  loss:95.030640  pct:-3.999829824\n",
      "[Iter.   40]  loss:91.479317  pct:-3.737029394\n",
      "[Iter.   50]  loss:88.256638  pct:-3.522850032\n",
      "[Iter.   60]  loss:85.304710  pct:-3.344708417\n",
      "[Iter.   70]  loss:82.581100  pct:-3.192801326\n",
      "[Iter.   80]  loss:80.054321  pct:-3.059754787\n",
      "[Iter.   90]  loss:77.699303  pct:-2.941775757\n",
      "[Iter.  100]  loss:75.494057  pct:-2.838179875\n",
      "[Iter.  110]  loss:73.423981  pct:-2.742038353\n",
      "[Iter.  120]  loss:71.474464  pct:-2.655149282\n",
      "[Iter.  130]  loss:69.632690  pct:-2.576827965\n",
      "[Iter.  140]  loss:67.888199  pct:-2.505276712\n",
      "[Iter.  150]  loss:66.231888  pct:-2.439762821\n",
      "[Iter.  160]  loss:64.656036  pct:-2.379294162\n",
      "[Iter.  170]  loss:63.153873  pct:-2.323314291\n",
      "[Iter.  180]  loss:61.719372  pct:-2.271438900\n",
      "[Iter.  190]  loss:60.347248  pct:-2.223165399\n",
      "[Iter.  200]  loss:59.032787  pct:-2.178161882\n",
      "[Iter.  210]  loss:57.771767  pct:-2.136136065\n",
      "[Iter.  220]  loss:56.560520  pct:-2.096606284\n",
      "[Iter.  230]  loss:55.395626  pct:-2.059553378\n",
      "[Iter.  240]  loss:54.274082  pct:-2.024607291\n",
      "[Iter.  250]  loss:53.193085  pct:-1.991737904\n",
      "[Iter.  260]  loss:52.150185  pct:-1.960593357\n",
      "[Iter.  270]  loss:51.143105  pct:-1.931115077\n",
      "[Iter.  280]  loss:50.169827  pct:-1.903048425\n",
      "[Iter.  290]  loss:49.228371  pct:-1.876537964\n",
      "[Iter.  300]  loss:48.316952  pct:-1.851409873\n",
      "[Iter.  310]  loss:47.434093  pct:-1.827222630\n",
      "[Iter.  320]  loss:46.578197  pct:-1.804389909\n",
      "[Iter.  330]  loss:45.747871  pct:-1.782649663\n",
      "[Iter.  340]  loss:44.941914  pct:-1.761738349\n",
      "[Iter.  350]  loss:44.159191  pct:-1.741631387\n",
      "[Iter.  360]  loss:43.398537  pct:-1.722528040\n",
      "[Iter.  370]  loss:42.659008  pct:-1.704040533\n",
      "[Iter.  380]  loss:41.939545  pct:-1.686544957\n",
      "[Iter.  390]  loss:41.239365  pct:-1.669498463\n",
      "[Iter.  400]  loss:40.557644  pct:-1.653082534\n",
      "[Iter.  410]  loss:39.893524  pct:-1.637471156\n",
      "[Iter.  420]  loss:39.246330  pct:-1.622303174\n",
      "[Iter.  430]  loss:38.615376  pct:-1.607678319\n",
      "[Iter.  440]  loss:38.000027  pct:-1.593533166\n",
      "[Iter.  450]  loss:37.399605  pct:-1.580056536\n",
      "[Iter.  460]  loss:36.813595  pct:-1.566888159\n",
      "[Iter.  470]  loss:36.241413  pct:-1.554267396\n",
      "[Iter.  480]  loss:35.682613  pct:-1.541881774\n",
      "[Iter.  490]  loss:35.136658  pct:-1.530032714\n",
      "[Iter.  500]  loss:34.603149  pct:-1.518380903\n",
      "[Iter.  510]  loss:34.081596  pct:-1.507241533\n",
      "[Iter.  520]  loss:33.571594  pct:-1.496415046\n",
      "[Iter.  530]  loss:33.072857  pct:-1.485593242\n",
      "[Iter.  540]  loss:32.584873  pct:-1.475480951\n",
      "[Iter.  550]  loss:32.107307  pct:-1.465605720\n",
      "[Iter.  560]  loss:31.639879  pct:-1.455831226\n",
      "[Iter.  570]  loss:31.182205  pct:-1.446510030\n",
      "[Iter.  580]  loss:30.733999  pct:-1.437377328\n",
      "[Iter.  590]  loss:30.294968  pct:-1.428488357\n",
      "[Iter.  600]  loss:29.864872  pct:-1.419693454\n",
      "[Iter.  610]  loss:29.443445  pct:-1.411111936\n",
      "[Iter.  620]  loss:29.030403  pct:-1.402831991\n",
      "[Iter.  630]  loss:28.625486  pct:-1.394802412\n",
      "[Iter.  640]  loss:28.228504  pct:-1.386813792\n",
      "[Iter.  650]  loss:27.839184  pct:-1.379174649\n",
      "[Iter.  660]  loss:27.457373  pct:-1.371488276\n",
      "[Iter.  670]  loss:27.082861  pct:-1.363975073\n",
      "[Iter.  680]  loss:26.715462  pct:-1.356574612\n",
      "[Iter.  690]  loss:26.354916  pct:-1.349578441\n",
      "[Iter.  700]  loss:26.001085  pct:-1.342559174\n",
      "[Iter.  710]  loss:25.653713  pct:-1.335990599\n",
      "[Iter.  720]  loss:25.312717  pct:-1.329225853\n",
      "[Iter.  730]  loss:24.977890  pct:-1.322763642\n",
      "[Iter.  740]  loss:24.649124  pct:-1.316227547\n",
      "[Iter.  750]  loss:24.326231  pct:-1.309957874\n",
      "[Iter.  760]  loss:24.009083  pct:-1.303729331\n",
      "[Iter.  770]  loss:23.697554  pct:-1.297547108\n",
      "[Iter.  780]  loss:23.391466  pct:-1.291641739\n",
      "[Iter.  790]  loss:23.090715  pct:-1.285728439\n",
      "[Iter.  800]  loss:22.795204  pct:-1.279783846\n",
      "[Iter.  810]  loss:22.504738  pct:-1.274243067\n",
      "[Iter.  820]  loss:22.219305  pct:-1.268323219\n",
      "[Iter.  830]  loss:21.938705  pct:-1.262863954\n",
      "[Iter.  840]  loss:21.662880  pct:-1.257255134\n",
      "[Iter.  850]  loss:21.391676  pct:-1.251929547\n",
      "[Iter.  860]  loss:21.125002  pct:-1.246625287\n",
      "[Iter.  870]  loss:20.862747  pct:-1.241442326\n",
      "[Iter.  880]  loss:20.604782  pct:-1.236486669\n",
      "[Iter.  890]  loss:20.351091  pct:-1.231222530\n",
      "[Iter.  900]  loss:20.101614  pct:-1.225867359\n",
      "[Iter.  910]  loss:19.856152  pct:-1.221108005\n",
      "[Iter.  920]  loss:19.614660  pct:-1.216204040\n",
      "[Iter.  930]  loss:19.377069  pct:-1.211291894\n",
      "[Iter.  940]  loss:19.143297  pct:-1.206437734\n",
      "[Iter.  950]  loss:18.913246  pct:-1.201731542\n",
      "[Iter.  960]  loss:18.686878  pct:-1.196875188\n",
      "[Iter.  970]  loss:18.464100  pct:-1.192164458\n",
      "[Iter.  980]  loss:18.244829  pct:-1.187551560\n",
      "[Iter.  990]  loss:18.028976  pct:-1.183089934\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.758398  pct:100.000000000\n",
      "[Iter.    2]  loss:2.758405  pct:0.000259301\n",
      "[Iter.    4]  loss:2.758400  pct:-0.000198797\n",
      "[Iter.    6]  loss:2.758397  pct:-0.000095077\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.758397\n",
      "Best loss: 2.758397 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▌        | 1588/10000 [00:24<02:08, 65.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:994.014526  pct:100.000000000\n",
      "[Iter.   10]  loss:174.040298  pct:-82.491173058\n",
      "[Iter.   20]  loss:165.373169  pct:-4.979955558\n",
      "[Iter.   30]  loss:159.623810  pct:-3.476597303\n",
      "[Iter.   40]  loss:154.879364  pct:-2.972266986\n",
      "[Iter.   50]  loss:150.539261  pct:-2.802247528\n",
      "[Iter.   60]  loss:146.513153  pct:-2.674456992\n",
      "[Iter.   70]  loss:142.754913  pct:-2.565121061\n",
      "[Iter.   80]  loss:139.241043  pct:-2.461470612\n",
      "[Iter.   90]  loss:135.939240  pct:-2.371286164\n",
      "[Iter.  100]  loss:132.824371  pct:-2.291367949\n",
      "[Iter.  110]  loss:129.870743  pct:-2.223709783\n",
      "[Iter.  120]  loss:127.073151  pct:-2.154135799\n",
      "[Iter.  130]  loss:124.414230  pct:-2.092432803\n",
      "[Iter.  140]  loss:121.881142  pct:-2.036012020\n",
      "[Iter.  150]  loss:119.463097  pct:-1.983936982\n",
      "[Iter.  160]  loss:117.150650  pct:-1.935699525\n",
      "[Iter.  170]  loss:114.935585  pct:-1.890783365\n",
      "[Iter.  180]  loss:112.810425  pct:-1.849001088\n",
      "[Iter.  190]  loss:110.768738  pct:-1.809838953\n",
      "[Iter.  200]  loss:108.804718  pct:-1.773081299\n",
      "[Iter.  210]  loss:106.913155  pct:-1.738493927\n",
      "[Iter.  220]  loss:105.089188  pct:-1.706026716\n",
      "[Iter.  230]  loss:103.328590  pct:-1.675336225\n",
      "[Iter.  240]  loss:101.627457  pct:-1.646334012\n",
      "[Iter.  250]  loss:99.982269  pct:-1.618841435\n",
      "[Iter.  260]  loss:98.389709  pct:-1.592842237\n",
      "[Iter.  270]  loss:96.846848  pct:-1.568113115\n",
      "[Iter.  280]  loss:95.350983  pct:-1.544567434\n",
      "[Iter.  290]  loss:93.899467  pct:-1.522286564\n",
      "[Iter.  300]  loss:92.490105  pct:-1.500927355\n",
      "[Iter.  310]  loss:91.120728  pct:-1.480566101\n",
      "[Iter.  320]  loss:89.789322  pct:-1.461144655\n",
      "[Iter.  330]  loss:88.494064  pct:-1.442551899\n",
      "[Iter.  340]  loss:87.233269  pct:-1.424723345\n",
      "[Iter.  350]  loss:86.005257  pct:-1.407733658\n",
      "[Iter.  360]  loss:84.808685  pct:-1.391276995\n",
      "[Iter.  370]  loss:83.642174  pct:-1.375462350\n",
      "[Iter.  380]  loss:82.504341  pct:-1.360357569\n",
      "[Iter.  390]  loss:81.393921  pct:-1.345893091\n",
      "[Iter.  400]  loss:80.309883  pct:-1.331841210\n",
      "[Iter.  410]  loss:79.251053  pct:-1.318430833\n",
      "[Iter.  420]  loss:78.216484  pct:-1.305432230\n",
      "[Iter.  430]  loss:77.205284  pct:-1.292822048\n",
      "[Iter.  440]  loss:76.216446  pct:-1.280790826\n",
      "[Iter.  450]  loss:75.249161  pct:-1.269129181\n",
      "[Iter.  460]  loss:74.302711  pct:-1.257753934\n",
      "[Iter.  470]  loss:73.376266  pct:-1.246852220\n",
      "[Iter.  480]  loss:72.469101  pct:-1.236320095\n",
      "[Iter.  490]  loss:71.580605  pct:-1.226034803\n",
      "[Iter.  500]  loss:70.710136  pct:-1.216067041\n",
      "[Iter.  510]  loss:69.856987  pct:-1.206544715\n",
      "[Iter.  520]  loss:69.020691  pct:-1.197154526\n",
      "[Iter.  530]  loss:68.200630  pct:-1.188137527\n",
      "[Iter.  540]  loss:67.396286  pct:-1.179379391\n",
      "[Iter.  550]  loss:66.607231  pct:-1.170769069\n",
      "[Iter.  560]  loss:65.832924  pct:-1.162497281\n",
      "[Iter.  570]  loss:65.072807  pct:-1.154614640\n",
      "[Iter.  580]  loss:64.326630  pct:-1.146681239\n",
      "[Iter.  590]  loss:63.593983  pct:-1.138948125\n",
      "[Iter.  600]  loss:62.874298  pct:-1.131686632\n",
      "[Iter.  610]  loss:62.167286  pct:-1.124485200\n",
      "[Iter.  620]  loss:61.472622  pct:-1.117410856\n",
      "[Iter.  630]  loss:60.789925  pct:-1.110571300\n",
      "[Iter.  640]  loss:60.118790  pct:-1.104023328\n",
      "[Iter.  650]  loss:59.459057  pct:-1.097382070\n",
      "[Iter.  660]  loss:58.810333  pct:-1.091042537\n",
      "[Iter.  670]  loss:58.172348  pct:-1.084818253\n",
      "[Iter.  680]  loss:57.544758  pct:-1.078846223\n",
      "[Iter.  690]  loss:56.927334  pct:-1.072945711\n",
      "[Iter.  700]  loss:56.319885  pct:-1.067059595\n",
      "[Iter.  710]  loss:55.722050  pct:-1.061499927\n",
      "[Iter.  720]  loss:55.133545  pct:-1.056143473\n",
      "[Iter.  730]  loss:54.554276  pct:-1.050665997\n",
      "[Iter.  740]  loss:53.983921  pct:-1.045480774\n",
      "[Iter.  750]  loss:53.422256  pct:-1.040429391\n",
      "[Iter.  760]  loss:52.869144  pct:-1.035358793\n",
      "[Iter.  770]  loss:52.324387  pct:-1.030388989\n",
      "[Iter.  780]  loss:51.787796  pct:-1.025507629\n",
      "[Iter.  790]  loss:51.259117  pct:-1.020856137\n",
      "[Iter.  800]  loss:50.738182  pct:-1.016277860\n",
      "[Iter.  810]  loss:50.224842  pct:-1.011742982\n",
      "[Iter.  820]  loss:49.718899  pct:-1.007356673\n",
      "[Iter.  830]  loss:49.220333  pct:-1.002768939\n",
      "[Iter.  840]  loss:48.728790  pct:-0.998658045\n",
      "[Iter.  850]  loss:48.244186  pct:-0.994491919\n",
      "[Iter.  860]  loss:47.766411  pct:-0.990327767\n",
      "[Iter.  870]  loss:47.295322  pct:-0.986233634\n",
      "[Iter.  880]  loss:46.830669  pct:-0.982450254\n",
      "[Iter.  890]  loss:46.372364  pct:-0.978643621\n",
      "[Iter.  900]  loss:45.920422  pct:-0.974594358\n",
      "[Iter.  910]  loss:45.474518  pct:-0.971035898\n",
      "[Iter.  920]  loss:45.034622  pct:-0.967345342\n",
      "[Iter.  930]  loss:44.600643  pct:-0.963656434\n",
      "[Iter.  940]  loss:44.172394  pct:-0.960186510\n",
      "[Iter.  950]  loss:43.749821  pct:-0.956645210\n",
      "[Iter.  960]  loss:43.332764  pct:-0.953277135\n",
      "[Iter.  970]  loss:42.921139  pct:-0.949916123\n",
      "[Iter.  980]  loss:42.514847  pct:-0.946601077\n",
      "[Iter.  990]  loss:42.113708  pct:-0.943525229\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.749865  pct:100.000000000\n",
      "[Iter.    2]  loss:2.749863  pct:-0.000078032\n",
      "[Iter.    4]  loss:2.749858  pct:-0.000182074\n",
      "[Iter.    6]  loss:2.749856  pct:-0.000060691\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.749856\n",
      "Best loss: 2.749856 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1839/10000 [00:29<02:10, 62.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:467.846680  pct:100.000000000\n",
      "[Iter.   10]  loss:78.595139  pct:-83.200660126\n",
      "[Iter.   20]  loss:74.369583  pct:-5.376357238\n",
      "[Iter.   30]  loss:71.190269  pct:-4.275018799\n",
      "[Iter.   40]  loss:68.342812  pct:-3.999785233\n",
      "[Iter.   50]  loss:65.754066  pct:-3.787882086\n",
      "[Iter.   60]  loss:63.382442  pct:-3.606809617\n",
      "[Iter.   70]  loss:61.196266  pct:-3.449182794\n",
      "[Iter.   80]  loss:59.170418  pct:-3.310411754\n",
      "[Iter.   90]  loss:57.284515  pct:-3.187238616\n",
      "[Iter.  100]  loss:55.521751  pct:-3.077208501\n",
      "[Iter.  110]  loss:53.868198  pct:-2.978207580\n",
      "[Iter.  120]  loss:52.312225  pct:-2.888481700\n",
      "[Iter.  130]  loss:50.843861  pct:-2.806924588\n",
      "[Iter.  140]  loss:49.454639  pct:-2.732328297\n",
      "[Iter.  150]  loss:48.137226  pct:-2.663882186\n",
      "[Iter.  160]  loss:46.885315  pct:-2.600713137\n",
      "[Iter.  170]  loss:45.693302  pct:-2.542401151\n",
      "[Iter.  180]  loss:44.556358  pct:-2.488206725\n",
      "[Iter.  190]  loss:43.470150  pct:-2.437830164\n",
      "[Iter.  200]  loss:42.430862  pct:-2.390807410\n",
      "[Iter.  210]  loss:41.435066  pct:-2.346867696\n",
      "[Iter.  220]  loss:40.479755  pct:-2.305561228\n",
      "[Iter.  230]  loss:39.562168  pct:-2.266780694\n",
      "[Iter.  240]  loss:38.679821  pct:-2.230279959\n",
      "[Iter.  250]  loss:37.830498  pct:-2.195778704\n",
      "[Iter.  260]  loss:37.012180  pct:-2.163115640\n",
      "[Iter.  270]  loss:36.222980  pct:-2.132270572\n",
      "[Iter.  280]  loss:35.461250  pct:-2.102892097\n",
      "[Iter.  290]  loss:34.725407  pct:-2.075064055\n",
      "[Iter.  300]  loss:34.014103  pct:-2.048366829\n",
      "[Iter.  310]  loss:33.325970  pct:-2.023082135\n",
      "[Iter.  320]  loss:32.659790  pct:-1.998980564\n",
      "[Iter.  330]  loss:32.014469  pct:-1.975888062\n",
      "[Iter.  340]  loss:31.388979  pct:-1.953773420\n",
      "[Iter.  350]  loss:30.782349  pct:-1.932622039\n",
      "[Iter.  360]  loss:30.193714  pct:-1.912246846\n",
      "[Iter.  370]  loss:29.622242  pct:-1.892685892\n",
      "[Iter.  380]  loss:29.067146  pct:-1.873915125\n",
      "[Iter.  390]  loss:28.527725  pct:-1.855775851\n",
      "[Iter.  400]  loss:28.003288  pct:-1.838341286\n",
      "[Iter.  410]  loss:27.493164  pct:-1.821658234\n",
      "[Iter.  420]  loss:26.996828  pct:-1.805306883\n",
      "[Iter.  430]  loss:26.513685  pct:-1.789628216\n",
      "[Iter.  440]  loss:26.043243  pct:-1.774335835\n",
      "[Iter.  450]  loss:25.584986  pct:-1.759602934\n",
      "[Iter.  460]  loss:25.138432  pct:-1.745375935\n",
      "[Iter.  470]  loss:24.703117  pct:-1.731668014\n",
      "[Iter.  480]  loss:24.278660  pct:-1.718234762\n",
      "[Iter.  490]  loss:23.864689  pct:-1.705081542\n",
      "[Iter.  500]  loss:23.460812  pct:-1.692363393\n",
      "[Iter.  510]  loss:23.066660  pct:-1.680042848\n",
      "[Iter.  520]  loss:22.681890  pct:-1.668076093\n",
      "[Iter.  530]  loss:22.306219  pct:-1.656261355\n",
      "[Iter.  540]  loss:21.939302  pct:-1.644907435\n",
      "[Iter.  550]  loss:21.580843  pct:-1.633869051\n",
      "[Iter.  560]  loss:21.230623  pct:-1.622826907\n",
      "[Iter.  570]  loss:20.888355  pct:-1.612142923\n",
      "[Iter.  580]  loss:20.553774  pct:-1.601760268\n",
      "[Iter.  590]  loss:20.226660  pct:-1.591503863\n",
      "[Iter.  600]  loss:19.906754  pct:-1.581606841\n",
      "[Iter.  610]  loss:19.593843  pct:-1.571879007\n",
      "[Iter.  620]  loss:19.287760  pct:-1.562142108\n",
      "[Iter.  630]  loss:18.988247  pct:-1.552864960\n",
      "[Iter.  640]  loss:18.695162  pct:-1.543507937\n",
      "[Iter.  650]  loss:18.408281  pct:-1.534517304\n",
      "[Iter.  660]  loss:18.127472  pct:-1.525451494\n",
      "[Iter.  670]  loss:17.852539  pct:-1.516664113\n",
      "[Iter.  680]  loss:17.583317  pct:-1.508033443\n",
      "[Iter.  690]  loss:17.319662  pct:-1.499459470\n",
      "[Iter.  700]  loss:17.061420  pct:-1.491031707\n",
      "[Iter.  710]  loss:16.808428  pct:-1.482834509\n",
      "[Iter.  720]  loss:16.560589  pct:-1.474492301\n",
      "[Iter.  730]  loss:16.317720  pct:-1.466544613\n",
      "[Iter.  740]  loss:16.079708  pct:-1.458612526\n",
      "[Iter.  750]  loss:15.846430  pct:-1.450761874\n",
      "[Iter.  760]  loss:15.617740  pct:-1.443165116\n",
      "[Iter.  770]  loss:15.393565  pct:-1.435383763\n",
      "[Iter.  780]  loss:15.173796  pct:-1.427671078\n",
      "[Iter.  790]  loss:14.958307  pct:-1.420135331\n",
      "[Iter.  800]  loss:14.746996  pct:-1.412668804\n",
      "[Iter.  810]  loss:14.539771  pct:-1.405200401\n",
      "[Iter.  820]  loss:14.336512  pct:-1.397955078\n",
      "[Iter.  830]  loss:14.137145  pct:-1.390621198\n",
      "[Iter.  840]  loss:13.941545  pct:-1.383585974\n",
      "[Iter.  850]  loss:13.749664  pct:-1.376326462\n",
      "[Iter.  860]  loss:13.561381  pct:-1.369364098\n",
      "[Iter.  870]  loss:13.376637  pct:-1.362286262\n",
      "[Iter.  880]  loss:13.195367  pct:-1.355121264\n",
      "[Iter.  890]  loss:13.017467  pct:-1.348195640\n",
      "[Iter.  900]  loss:12.842863  pct:-1.341308637\n",
      "[Iter.  910]  loss:12.671483  pct:-1.334437983\n",
      "[Iter.  920]  loss:12.503251  pct:-1.327642262\n",
      "[Iter.  930]  loss:12.338110  pct:-1.320785327\n",
      "[Iter.  940]  loss:12.175995  pct:-1.313937851\n",
      "[Iter.  950]  loss:12.016821  pct:-1.307276876\n",
      "[Iter.  960]  loss:11.860562  pct:-1.300332128\n",
      "[Iter.  970]  loss:11.707112  pct:-1.293783617\n",
      "[Iter.  980]  loss:11.556437  pct:-1.287036597\n",
      "[Iter.  990]  loss:11.408461  pct:-1.280471386\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.745247  pct:100.000000000\n",
      "[Iter.    2]  loss:2.745277  pct:0.001085597\n",
      "[Iter.    4]  loss:2.745269  pct:-0.000277910\n",
      "[Iter.    6]  loss:2.745267  pct:-0.000095532\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.745267\n",
      "Best loss: 2.745267 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  2%|▏         | 162/10000 [00:02<02:40, 61.45it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3942.501709  pct:100.000000000\n",
      "[Iter.   10]  loss:597.725464  pct:-84.838932589\n",
      "[Iter.   20]  loss:589.133118  pct:-1.437507135\n",
      "[Iter.   30]  loss:581.796631  pct:-1.245302054\n",
      "[Iter.   40]  loss:573.364441  pct:-1.449336331\n",
      "[Iter.   50]  loss:564.378052  pct:-1.567308420\n",
      "[Iter.   60]  loss:557.574158  pct:-1.205556102\n",
      "[Iter.   70]  loss:551.460754  pct:-1.096428741\n",
      "[Iter.   80]  loss:545.680298  pct:-1.048208145\n",
      "[Iter.   90]  loss:540.090698  pct:-1.024335977\n",
      "[Iter.  100]  loss:534.657654  pct:-1.005950380\n",
      "[Iter.  110]  loss:529.380066  pct:-0.987096669\n",
      "[Iter.  120]  loss:524.248901  pct:-0.969278007\n",
      "[Iter.  130]  loss:519.253235  pct:-0.952918831\n",
      "[Iter.  140]  loss:514.388733  pct:-0.936826509\n",
      "[Iter.  150]  loss:509.644470  pct:-0.922310772\n",
      "[Iter.  160]  loss:505.013855  pct:-0.908597170\n",
      "[Iter.  170]  loss:500.491852  pct:-0.895421607\n",
      "[Iter.  180]  loss:496.073700  pct:-0.882761995\n",
      "[Iter.  190]  loss:491.755493  pct:-0.870476864\n",
      "[Iter.  200]  loss:487.532959  pct:-0.858665381\n",
      "[Iter.  210]  loss:483.402466  pct:-0.847223370\n",
      "[Iter.  220]  loss:479.360138  pct:-0.836224092\n",
      "[Iter.  230]  loss:475.402863  pct:-0.825532846\n",
      "[Iter.  240]  loss:471.527039  pct:-0.815271484\n",
      "[Iter.  250]  loss:467.729706  pct:-0.805326620\n",
      "[Iter.  260]  loss:464.007935  pct:-0.795709829\n",
      "[Iter.  270]  loss:460.358887  pct:-0.786419279\n",
      "[Iter.  280]  loss:456.780273  pct:-0.777352927\n",
      "[Iter.  290]  loss:453.269348  pct:-0.768624544\n",
      "[Iter.  300]  loss:449.823975  pct:-0.760116154\n",
      "[Iter.  310]  loss:446.441772  pct:-0.751894594\n",
      "[Iter.  320]  loss:443.120636  pct:-0.743912573\n",
      "[Iter.  330]  loss:439.858521  pct:-0.736168712\n",
      "[Iter.  340]  loss:436.653687  pct:-0.728605639\n",
      "[Iter.  350]  loss:433.504089  pct:-0.721303235\n",
      "[Iter.  360]  loss:430.407867  pct:-0.714231307\n",
      "[Iter.  370]  loss:427.363800  pct:-0.707251798\n",
      "[Iter.  380]  loss:424.369843  pct:-0.700564137\n",
      "[Iter.  390]  loss:421.424805  pct:-0.693979059\n",
      "[Iter.  400]  loss:418.527191  pct:-0.687575457\n",
      "[Iter.  410]  loss:415.675446  pct:-0.681376423\n",
      "[Iter.  420]  loss:412.868195  pct:-0.675346838\n",
      "[Iter.  430]  loss:410.104340  pct:-0.669427923\n",
      "[Iter.  440]  loss:407.382507  pct:-0.663692630\n",
      "[Iter.  450]  loss:404.701721  pct:-0.658051361\n",
      "[Iter.  460]  loss:402.060760  pct:-0.652569672\n",
      "[Iter.  470]  loss:399.458282  pct:-0.647284760\n",
      "[Iter.  480]  loss:396.893707  pct:-0.642013273\n",
      "[Iter.  490]  loss:394.365845  pct:-0.636911723\n",
      "[Iter.  500]  loss:391.873810  pct:-0.631909417\n",
      "[Iter.  510]  loss:389.416321  pct:-0.627112339\n",
      "[Iter.  520]  loss:386.992767  pct:-0.622355391\n",
      "[Iter.  530]  loss:384.602386  pct:-0.617681017\n",
      "[Iter.  540]  loss:382.244354  pct:-0.613109099\n",
      "[Iter.  550]  loss:379.917725  pct:-0.608675998\n",
      "[Iter.  560]  loss:377.621887  pct:-0.604298577\n",
      "[Iter.  570]  loss:375.355774  pct:-0.600101148\n",
      "[Iter.  580]  loss:373.119049  pct:-0.595894618\n",
      "[Iter.  590]  loss:370.910889  pct:-0.591811221\n",
      "[Iter.  600]  loss:368.730621  pct:-0.587814324\n",
      "[Iter.  610]  loss:366.577576  pct:-0.583907473\n",
      "[Iter.  620]  loss:364.451294  pct:-0.580035954\n",
      "[Iter.  630]  loss:362.350891  pct:-0.576319214\n",
      "[Iter.  640]  loss:360.275879  pct:-0.572652713\n",
      "[Iter.  650]  loss:358.225891  pct:-0.569005008\n",
      "[Iter.  660]  loss:356.200256  pct:-0.565462971\n",
      "[Iter.  670]  loss:354.198456  pct:-0.561987394\n",
      "[Iter.  680]  loss:352.219818  pct:-0.558624032\n",
      "[Iter.  690]  loss:350.264282  pct:-0.555203253\n",
      "[Iter.  700]  loss:348.330811  pct:-0.552003666\n",
      "[Iter.  710]  loss:346.419250  pct:-0.548777197\n",
      "[Iter.  720]  loss:344.529205  pct:-0.545594728\n",
      "[Iter.  730]  loss:342.660278  pct:-0.542458222\n",
      "[Iter.  740]  loss:340.811829  pct:-0.539440905\n",
      "[Iter.  750]  loss:338.983582  pct:-0.536438855\n",
      "[Iter.  760]  loss:337.174988  pct:-0.533534321\n",
      "[Iter.  770]  loss:335.385834  pct:-0.530630716\n",
      "[Iter.  780]  loss:333.615662  pct:-0.527801696\n",
      "[Iter.  790]  loss:331.864105  pct:-0.525022233\n",
      "[Iter.  800]  loss:330.130859  pct:-0.522275782\n",
      "[Iter.  810]  loss:328.415497  pct:-0.519600789\n",
      "[Iter.  820]  loss:326.717896  pct:-0.516906582\n",
      "[Iter.  830]  loss:325.037598  pct:-0.514296240\n",
      "[Iter.  840]  loss:323.374237  pct:-0.511744059\n",
      "[Iter.  850]  loss:321.727478  pct:-0.509242495\n",
      "[Iter.  860]  loss:320.096954  pct:-0.506802742\n",
      "[Iter.  870]  loss:318.482513  pct:-0.504359975\n",
      "[Iter.  880]  loss:316.884155  pct:-0.501866849\n",
      "[Iter.  890]  loss:315.301208  pct:-0.499534846\n",
      "[Iter.  900]  loss:313.733490  pct:-0.497212971\n",
      "[Iter.  910]  loss:312.180847  pct:-0.494892280\n",
      "[Iter.  920]  loss:310.642975  pct:-0.492622250\n",
      "[Iter.  930]  loss:309.119568  pct:-0.490404453\n",
      "[Iter.  940]  loss:307.610260  pct:-0.488260213\n",
      "[Iter.  950]  loss:306.115112  pct:-0.486052612\n",
      "[Iter.  960]  loss:304.633575  pct:-0.483980309\n",
      "[Iter.  970]  loss:303.165436  pct:-0.481936256\n",
      "[Iter.  980]  loss:301.710938  pct:-0.479770488\n",
      "[Iter.  990]  loss:300.269348  pct:-0.477804805\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.787863  pct:100.000000000\n",
      "[Iter.    2]  loss:2.787422  pct:-0.015829787\n",
      "[Iter.    4]  loss:2.787310  pct:-0.004002979\n",
      "[Iter.    6]  loss:2.787277  pct:-0.001180413\n",
      "[Iter.    8]  loss:2.787270  pct:-0.000265168\n",
      "[Iter.   10]  loss:2.787260  pct:-0.000367815\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.787260\n",
      "Best loss: 2.787260 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:14<00:00, 74.23it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:444.185364  pct:100.000000000\n",
      "[Iter.   10]  loss:72.587067  pct:-83.658383961\n",
      "[Iter.   20]  loss:68.485580  pct:-5.650436635\n",
      "[Iter.   30]  loss:65.571556  pct:-4.254945835\n",
      "[Iter.   40]  loss:62.949562  pct:-3.998675912\n",
      "[Iter.   50]  loss:60.562321  pct:-3.792308135\n",
      "[Iter.   60]  loss:58.373043  pct:-3.614917036\n",
      "[Iter.   70]  loss:56.353249  pct:-3.460149340\n",
      "[Iter.   80]  loss:54.480221  pct:-3.323726401\n",
      "[Iter.   90]  loss:52.735413  pct:-3.202645238\n",
      "[Iter.  100]  loss:51.103714  pct:-3.094123148\n",
      "[Iter.  110]  loss:49.572411  pct:-2.996462069\n",
      "[Iter.  120]  loss:48.130836  pct:-2.908016939\n",
      "[Iter.  130]  loss:46.769955  pct:-2.827463441\n",
      "[Iter.  140]  loss:45.482018  pct:-2.753770392\n",
      "[Iter.  150]  loss:44.260326  pct:-2.686097052\n",
      "[Iter.  160]  loss:43.099079  pct:-2.623675305\n",
      "[Iter.  170]  loss:41.993176  pct:-2.565956507\n",
      "[Iter.  180]  loss:40.938179  pct:-2.512304625\n",
      "[Iter.  190]  loss:39.930138  pct:-2.462350320\n",
      "[Iter.  200]  loss:38.965538  pct:-2.415718218\n",
      "[Iter.  210]  loss:38.041264  pct:-2.372030495\n",
      "[Iter.  220]  loss:37.154449  pct:-2.331189960\n",
      "[Iter.  230]  loss:36.302662  pct:-2.292558710\n",
      "[Iter.  240]  loss:35.483585  pct:-2.256243744\n",
      "[Iter.  250]  loss:34.695187  pct:-2.221868886\n",
      "[Iter.  260]  loss:33.935604  pct:-2.189302303\n",
      "[Iter.  270]  loss:33.203091  pct:-2.158539526\n",
      "[Iter.  280]  loss:32.496105  pct:-2.129276099\n",
      "[Iter.  290]  loss:31.813240  pct:-2.101375346\n",
      "[Iter.  300]  loss:31.153151  pct:-2.074889234\n",
      "[Iter.  310]  loss:30.514645  pct:-2.049570988\n",
      "[Iter.  320]  loss:29.896616  pct:-2.025350937\n",
      "[Iter.  330]  loss:29.298027  pct:-2.002196315\n",
      "[Iter.  340]  loss:28.717892  pct:-1.980117449\n",
      "[Iter.  350]  loss:28.155359  pct:-1.958822155\n",
      "[Iter.  360]  loss:27.609621  pct:-1.938310270\n",
      "[Iter.  370]  loss:27.079859  pct:-1.918759650\n",
      "[Iter.  380]  loss:26.565411  pct:-1.899744641\n",
      "[Iter.  390]  loss:26.065571  pct:-1.881543598\n",
      "[Iter.  400]  loss:25.579739  pct:-1.863884806\n",
      "[Iter.  410]  loss:25.107304  pct:-1.846910966\n",
      "[Iter.  420]  loss:24.647655  pct:-1.830734751\n",
      "[Iter.  430]  loss:24.200354  pct:-1.814784635\n",
      "[Iter.  440]  loss:23.764910  pct:-1.799328576\n",
      "[Iter.  450]  loss:23.340841  pct:-1.784431144\n",
      "[Iter.  460]  loss:22.927723  pct:-1.769937755\n",
      "[Iter.  470]  loss:22.525177  pct:-1.755716999\n",
      "[Iter.  480]  loss:22.132708  pct:-1.742358811\n",
      "[Iter.  490]  loss:21.750032  pct:-1.729002966\n",
      "[Iter.  500]  loss:21.376787  pct:-1.716067507\n",
      "[Iter.  510]  loss:21.012642  pct:-1.703461216\n",
      "[Iter.  520]  loss:20.657299  pct:-1.691090852\n",
      "[Iter.  530]  loss:20.310448  pct:-1.679074056\n",
      "[Iter.  540]  loss:19.971834  pct:-1.667188805\n",
      "[Iter.  550]  loss:19.641132  pct:-1.655841046\n",
      "[Iter.  560]  loss:19.318102  pct:-1.644663179\n",
      "[Iter.  570]  loss:19.002476  pct:-1.633836214\n",
      "[Iter.  580]  loss:18.694067  pct:-1.622992401\n",
      "[Iter.  590]  loss:18.392653  pct:-1.612353747\n",
      "[Iter.  600]  loss:18.097988  pct:-1.602076605\n",
      "[Iter.  610]  loss:17.809900  pct:-1.591822488\n",
      "[Iter.  620]  loss:17.528158  pct:-1.581940895\n",
      "[Iter.  630]  loss:17.252607  pct:-1.572046757\n",
      "[Iter.  640]  loss:16.983013  pct:-1.562628692\n",
      "[Iter.  650]  loss:16.719273  pct:-1.552966704\n",
      "[Iter.  660]  loss:16.461172  pct:-1.543730494\n",
      "[Iter.  670]  loss:16.208574  pct:-1.534506822\n",
      "[Iter.  680]  loss:15.961315  pct:-1.525483584\n",
      "[Iter.  690]  loss:15.719248  pct:-1.516587666\n",
      "[Iter.  700]  loss:15.482262  pct:-1.507617686\n",
      "[Iter.  710]  loss:15.250188  pct:-1.498965649\n",
      "[Iter.  720]  loss:15.022894  pct:-1.490433889\n",
      "[Iter.  730]  loss:14.800265  pct:-1.481928814\n",
      "[Iter.  740]  loss:14.582202  pct:-1.473374631\n",
      "[Iter.  750]  loss:14.368541  pct:-1.465219001\n",
      "[Iter.  760]  loss:14.159228  pct:-1.456741101\n",
      "[Iter.  770]  loss:13.954103  pct:-1.448700807\n",
      "[Iter.  780]  loss:13.753084  pct:-1.440574721\n",
      "[Iter.  790]  loss:13.556074  pct:-1.432478982\n",
      "[Iter.  800]  loss:13.362962  pct:-1.424544978\n",
      "[Iter.  810]  loss:13.173652  pct:-1.416677509\n",
      "[Iter.  820]  loss:12.988050  pct:-1.408882205\n",
      "[Iter.  830]  loss:12.806061  pct:-1.401208521\n",
      "[Iter.  840]  loss:12.627605  pct:-1.393522612\n",
      "[Iter.  850]  loss:12.452628  pct:-1.385672869\n",
      "[Iter.  860]  loss:12.281060  pct:-1.377764718\n",
      "[Iter.  870]  loss:12.112735  pct:-1.370609876\n",
      "[Iter.  880]  loss:11.947661  pct:-1.362808627\n",
      "[Iter.  890]  loss:11.785713  pct:-1.355480362\n",
      "[Iter.  900]  loss:11.626883  pct:-1.347654063\n",
      "[Iter.  910]  loss:11.471056  pct:-1.340226565\n",
      "[Iter.  920]  loss:11.318161  pct:-1.332876188\n",
      "[Iter.  930]  loss:11.168131  pct:-1.325569905\n",
      "[Iter.  940]  loss:11.020929  pct:-1.318049902\n",
      "[Iter.  950]  loss:10.876457  pct:-1.310888744\n",
      "[Iter.  960]  loss:10.734676  pct:-1.303557312\n",
      "[Iter.  970]  loss:10.595549  pct:-1.296058928\n",
      "[Iter.  980]  loss:10.458997  pct:-1.288766271\n",
      "[Iter.  990]  loss:10.324978  pct:-1.281374313\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.752119  pct:100.000000000\n",
      "[Iter.    2]  loss:2.752178  pct:0.002131120\n",
      "[Iter.    4]  loss:2.752178  pct:-0.000008663\n",
      "[Iter.    6]  loss:2.752177  pct:-0.000008663\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.752177\n",
      "Best loss: 2.752177 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 313/10000 [00:04<02:08, 75.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5221.142578  pct:100.000000000\n",
      "[Iter.   10]  loss:843.953857  pct:-83.835834993\n",
      "[Iter.   20]  loss:831.200012  pct:-1.511201721\n",
      "[Iter.   30]  loss:821.600464  pct:-1.154902334\n",
      "[Iter.   40]  loss:812.605591  pct:-1.094798925\n",
      "[Iter.   50]  loss:803.912964  pct:-1.069722760\n",
      "[Iter.   60]  loss:795.488159  pct:-1.047974727\n",
      "[Iter.   70]  loss:787.316040  pct:-1.027308709\n",
      "[Iter.   80]  loss:779.385681  pct:-1.007264997\n",
      "[Iter.   90]  loss:771.686340  pct:-0.987873014\n",
      "[Iter.  100]  loss:764.207947  pct:-0.969097568\n",
      "[Iter.  110]  loss:756.940918  pct:-0.950922957\n",
      "[Iter.  120]  loss:749.876831  pct:-0.933241518\n",
      "[Iter.  130]  loss:743.008301  pct:-0.915954459\n",
      "[Iter.  140]  loss:736.327637  pct:-0.899137204\n",
      "[Iter.  150]  loss:729.826111  pct:-0.882966434\n",
      "[Iter.  160]  loss:723.495422  pct:-0.867424224\n",
      "[Iter.  170]  loss:717.326599  pct:-0.852641641\n",
      "[Iter.  180]  loss:711.309998  pct:-0.838753445\n",
      "[Iter.  190]  loss:705.435913  pct:-0.825812162\n",
      "[Iter.  200]  loss:699.696838  pct:-0.813550119\n",
      "[Iter.  210]  loss:694.086426  pct:-0.801834779\n",
      "[Iter.  220]  loss:688.598999  pct:-0.790597043\n",
      "[Iter.  230]  loss:683.229858  pct:-0.779719493\n",
      "[Iter.  240]  loss:677.973694  pct:-0.769311307\n",
      "[Iter.  250]  loss:672.826233  pct:-0.759241986\n",
      "[Iter.  260]  loss:667.784058  pct:-0.749402304\n",
      "[Iter.  270]  loss:662.843018  pct:-0.739915843\n",
      "[Iter.  280]  loss:657.998840  pct:-0.730818175\n",
      "[Iter.  290]  loss:653.248474  pct:-0.721941426\n",
      "[Iter.  300]  loss:648.588257  pct:-0.713391224\n",
      "[Iter.  310]  loss:644.015381  pct:-0.705050689\n",
      "[Iter.  320]  loss:639.526550  pct:-0.697006733\n",
      "[Iter.  330]  loss:635.119080  pct:-0.689177127\n",
      "[Iter.  340]  loss:630.790344  pct:-0.681562795\n",
      "[Iter.  350]  loss:626.537292  pct:-0.674241734\n",
      "[Iter.  360]  loss:622.357849  pct:-0.667070166\n",
      "[Iter.  370]  loss:618.249634  pct:-0.660105008\n",
      "[Iter.  380]  loss:614.210571  pct:-0.653306089\n",
      "[Iter.  390]  loss:610.238525  pct:-0.646691230\n",
      "[Iter.  400]  loss:606.331360  pct:-0.640268578\n",
      "[Iter.  410]  loss:602.486877  pct:-0.634056339\n",
      "[Iter.  420]  loss:598.703613  pct:-0.627941338\n",
      "[Iter.  430]  loss:594.979431  pct:-0.622041031\n",
      "[Iter.  440]  loss:591.313232  pct:-0.616189155\n",
      "[Iter.  450]  loss:587.703003  pct:-0.610544343\n",
      "[Iter.  460]  loss:584.146606  pct:-0.605134986\n",
      "[Iter.  470]  loss:580.643250  pct:-0.599739328\n",
      "[Iter.  480]  loss:577.191589  pct:-0.594454540\n",
      "[Iter.  490]  loss:573.789551  pct:-0.589412361\n",
      "[Iter.  500]  loss:570.436096  pct:-0.584439815\n",
      "[Iter.  510]  loss:567.129761  pct:-0.579615398\n",
      "[Iter.  520]  loss:563.869385  pct:-0.574890652\n",
      "[Iter.  530]  loss:560.653748  pct:-0.570280511\n",
      "[Iter.  540]  loss:557.481689  pct:-0.565778454\n",
      "[Iter.  550]  loss:554.351929  pct:-0.561410500\n",
      "[Iter.  560]  loss:551.264343  pct:-0.556972077\n",
      "[Iter.  570]  loss:548.217163  pct:-0.552762067\n",
      "[Iter.  580]  loss:545.209290  pct:-0.548664606\n",
      "[Iter.  590]  loss:542.240112  pct:-0.544594031\n",
      "[Iter.  600]  loss:539.308838  pct:-0.540586052\n",
      "[Iter.  610]  loss:536.414185  pct:-0.536733893\n",
      "[Iter.  620]  loss:533.555359  pct:-0.532951172\n",
      "[Iter.  630]  loss:530.731873  pct:-0.529183389\n",
      "[Iter.  640]  loss:527.942322  pct:-0.525604533\n",
      "[Iter.  650]  loss:525.186340  pct:-0.522023208\n",
      "[Iter.  660]  loss:522.463135  pct:-0.518521781\n",
      "[Iter.  670]  loss:519.772217  pct:-0.515044563\n",
      "[Iter.  680]  loss:517.112549  pct:-0.511698756\n",
      "[Iter.  690]  loss:514.483948  pct:-0.508322817\n",
      "[Iter.  700]  loss:511.885376  pct:-0.505083159\n",
      "[Iter.  710]  loss:509.316071  pct:-0.501929834\n",
      "[Iter.  720]  loss:506.776001  pct:-0.498721664\n",
      "[Iter.  730]  loss:504.263977  pct:-0.495687231\n",
      "[Iter.  740]  loss:501.779755  pct:-0.492643243\n",
      "[Iter.  750]  loss:499.322723  pct:-0.489663289\n",
      "[Iter.  760]  loss:496.892120  pct:-0.486779975\n",
      "[Iter.  770]  loss:494.487885  pct:-0.483854692\n",
      "[Iter.  780]  loss:492.108887  pct:-0.481103355\n",
      "[Iter.  790]  loss:489.754883  pct:-0.478350213\n",
      "[Iter.  800]  loss:487.426147  pct:-0.475489971\n",
      "[Iter.  810]  loss:485.121857  pct:-0.472746648\n",
      "[Iter.  820]  loss:482.841461  pct:-0.470066536\n",
      "[Iter.  830]  loss:480.584503  pct:-0.467432520\n",
      "[Iter.  840]  loss:478.349854  pct:-0.464985792\n",
      "[Iter.  850]  loss:476.137695  pct:-0.462456126\n",
      "[Iter.  860]  loss:473.947754  pct:-0.459938675\n",
      "[Iter.  870]  loss:471.779297  pct:-0.457530817\n",
      "[Iter.  880]  loss:469.631989  pct:-0.455151034\n",
      "[Iter.  890]  loss:467.505371  pct:-0.452826358\n",
      "[Iter.  900]  loss:465.399200  pct:-0.450512611\n",
      "[Iter.  910]  loss:463.313202  pct:-0.448217043\n",
      "[Iter.  920]  loss:461.246613  pct:-0.446045860\n",
      "[Iter.  930]  loss:459.199615  pct:-0.443796662\n",
      "[Iter.  940]  loss:457.171509  pct:-0.441661234\n",
      "[Iter.  950]  loss:455.161987  pct:-0.439555275\n",
      "[Iter.  960]  loss:453.170990  pct:-0.437426097\n",
      "[Iter.  970]  loss:451.198242  pct:-0.435320849\n",
      "[Iter.  980]  loss:449.243439  pct:-0.433247137\n",
      "[Iter.  990]  loss:447.305603  pct:-0.431355369\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.824990  pct:100.000000000\n",
      "[Iter.    2]  loss:2.823457  pct:-0.054275227\n",
      "[Iter.    4]  loss:2.822862  pct:-0.021059854\n",
      "[Iter.    6]  loss:2.822592  pct:-0.009594641\n",
      "[Iter.    8]  loss:2.822433  pct:-0.005625567\n",
      "[Iter.   10]  loss:2.822347  pct:-0.003049465\n",
      "[Iter.   12]  loss:2.822320  pct:-0.000937676\n",
      "[Iter.   14]  loss:2.822255  pct:-0.002323092\n",
      "[Iter.   16]  loss:2.822229  pct:-0.000895467\n",
      "[Iter.   18]  loss:2.822214  pct:-0.000540664\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.822214\n",
      "Best loss: 2.822214 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1654/10000 [00:33<02:47, 49.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:326.811188  pct:100.000000000\n",
      "[Iter.   10]  loss:67.408768  pct:-79.373783552\n",
      "[Iter.   20]  loss:61.926617  pct:-8.132697301\n",
      "[Iter.   30]  loss:58.395657  pct:-5.701845625\n",
      "[Iter.   40]  loss:55.342773  pct:-5.227928457\n",
      "[Iter.   50]  loss:52.655178  pct:-4.856271561\n",
      "[Iter.   60]  loss:50.258350  pct:-4.551931615\n",
      "[Iter.   70]  loss:48.098644  pct:-4.297208523\n",
      "[Iter.   80]  loss:46.136063  pct:-4.080326306\n",
      "[Iter.   90]  loss:44.339825  pct:-3.893349028\n",
      "[Iter.  100]  loss:42.685810  pct:-3.730313774\n",
      "[Iter.  110]  loss:41.154762  pct:-3.586784034\n",
      "[Iter.  120]  loss:39.731087  pct:-3.459321494\n",
      "[Iter.  130]  loss:38.401974  pct:-3.345272219\n",
      "[Iter.  140]  loss:37.156754  pct:-3.242594230\n",
      "[Iter.  150]  loss:35.986462  pct:-3.149607512\n",
      "[Iter.  160]  loss:34.883572  pct:-3.064735916\n",
      "[Iter.  170]  loss:33.841526  pct:-2.987210153\n",
      "[Iter.  180]  loss:32.854710  pct:-2.915992634\n",
      "[Iter.  190]  loss:31.918324  pct:-2.850081827\n",
      "[Iter.  200]  loss:31.028091  pct:-2.789094126\n",
      "[Iter.  210]  loss:30.180195  pct:-2.732673964\n",
      "[Iter.  220]  loss:29.371450  pct:-2.679719049\n",
      "[Iter.  230]  loss:28.598879  pct:-2.630348698\n",
      "[Iter.  240]  loss:27.859846  pct:-2.584131878\n",
      "[Iter.  250]  loss:27.152067  pct:-2.540498349\n",
      "[Iter.  260]  loss:26.473400  pct:-2.499504232\n",
      "[Iter.  270]  loss:25.821943  pct:-2.460797744\n",
      "[Iter.  280]  loss:25.195988  pct:-2.424122673\n",
      "[Iter.  290]  loss:24.593985  pct:-2.389281598\n",
      "[Iter.  300]  loss:24.014494  pct:-2.356229261\n",
      "[Iter.  310]  loss:23.456207  pct:-2.324790471\n",
      "[Iter.  320]  loss:22.917967  pct:-2.294660967\n",
      "[Iter.  330]  loss:22.398624  pct:-2.266092913\n",
      "[Iter.  340]  loss:21.897207  pct:-2.238606937\n",
      "[Iter.  350]  loss:21.412788  pct:-2.212240416\n",
      "[Iter.  360]  loss:20.944508  pct:-2.186921122\n",
      "[Iter.  370]  loss:20.491566  pct:-2.162580774\n",
      "[Iter.  380]  loss:20.053209  pct:-2.139204031\n",
      "[Iter.  390]  loss:19.628748  pct:-2.116675482\n",
      "[Iter.  400]  loss:19.217548  pct:-2.094884355\n",
      "[Iter.  410]  loss:18.819025  pct:-2.073746989\n",
      "[Iter.  420]  loss:18.432589  pct:-2.053435083\n",
      "[Iter.  430]  loss:18.057743  pct:-2.033602080\n",
      "[Iter.  440]  loss:17.693975  pct:-2.014468931\n",
      "[Iter.  450]  loss:17.340868  pct:-1.995636613\n",
      "[Iter.  460]  loss:16.997952  pct:-1.977504752\n",
      "[Iter.  470]  loss:16.664806  pct:-1.959913472\n",
      "[Iter.  480]  loss:16.341057  pct:-1.942714095\n",
      "[Iter.  490]  loss:16.026342  pct:-1.925912352\n",
      "[Iter.  500]  loss:15.720329  pct:-1.909438222\n",
      "[Iter.  510]  loss:15.422679  pct:-1.893410321\n",
      "[Iter.  520]  loss:15.133072  pct:-1.877799888\n",
      "[Iter.  530]  loss:14.851220  pct:-1.862488795\n",
      "[Iter.  540]  loss:14.576893  pct:-1.847169968\n",
      "[Iter.  550]  loss:14.309779  pct:-1.832445970\n",
      "[Iter.  560]  loss:14.049646  pct:-1.817867254\n",
      "[Iter.  570]  loss:13.796254  pct:-1.803548735\n",
      "[Iter.  580]  loss:13.549374  pct:-1.789475089\n",
      "[Iter.  590]  loss:13.308782  pct:-1.775668821\n",
      "[Iter.  600]  loss:13.074286  pct:-1.761965319\n",
      "[Iter.  610]  loss:12.845667  pct:-1.748612738\n",
      "[Iter.  620]  loss:12.622773  pct:-1.735166550\n",
      "[Iter.  630]  loss:12.405388  pct:-1.722167460\n",
      "[Iter.  640]  loss:12.193376  pct:-1.709033954\n",
      "[Iter.  650]  loss:11.986528  pct:-1.696389891\n",
      "[Iter.  660]  loss:11.784712  pct:-1.683694829\n",
      "[Iter.  670]  loss:11.587785  pct:-1.671038489\n",
      "[Iter.  680]  loss:11.395601  pct:-1.658500727\n",
      "[Iter.  690]  loss:11.208024  pct:-1.646049586\n",
      "[Iter.  700]  loss:11.024888  pct:-1.633972107\n",
      "[Iter.  710]  loss:10.846095  pct:-1.621721263\n",
      "[Iter.  720]  loss:10.671501  pct:-1.609739949\n",
      "[Iter.  730]  loss:10.501000  pct:-1.597720440\n",
      "[Iter.  740]  loss:10.334496  pct:-1.585609499\n",
      "[Iter.  750]  loss:10.171851  pct:-1.573800923\n",
      "[Iter.  760]  loss:10.012959  pct:-1.562081759\n",
      "[Iter.  770]  loss:9.857732  pct:-1.550258168\n",
      "[Iter.  780]  loss:9.706053  pct:-1.538680924\n",
      "[Iter.  790]  loss:9.557816  pct:-1.527265839\n",
      "[Iter.  800]  loss:9.412962  pct:-1.515551238\n",
      "[Iter.  810]  loss:9.271415  pct:-1.503747743\n",
      "[Iter.  820]  loss:9.133039  pct:-1.492503998\n",
      "[Iter.  830]  loss:8.997787  pct:-1.480909104\n",
      "[Iter.  840]  loss:8.865548  pct:-1.469676878\n",
      "[Iter.  850]  loss:8.736291  pct:-1.457971918\n",
      "[Iter.  860]  loss:8.609894  pct:-1.446805445\n",
      "[Iter.  870]  loss:8.486326  pct:-1.435181247\n",
      "[Iter.  880]  loss:8.365495  pct:-1.423837435\n",
      "[Iter.  890]  loss:8.247327  pct:-1.412562927\n",
      "[Iter.  900]  loss:8.131758  pct:-1.401291798\n",
      "[Iter.  910]  loss:8.018731  pct:-1.389940805\n",
      "[Iter.  920]  loss:7.908172  pct:-1.378759121\n",
      "[Iter.  930]  loss:7.800033  pct:-1.367440075\n",
      "[Iter.  940]  loss:7.694249  pct:-1.356198680\n",
      "[Iter.  950]  loss:7.590755  pct:-1.345084773\n",
      "[Iter.  960]  loss:7.489533  pct:-1.333484851\n",
      "[Iter.  970]  loss:7.390488  pct:-1.322442941\n",
      "[Iter.  980]  loss:7.293592  pct:-1.311092984\n",
      "[Iter.  990]  loss:7.198781  pct:-1.299921397\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.734440  pct:100.000000000\n",
      "[Iter.    2]  loss:2.734499  pct:0.002153618\n",
      "[Iter.    4]  loss:2.734499  pct:-0.000026157\n",
      "[Iter.    6]  loss:2.734499  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.734499\n",
      "Best loss: 2.734499 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 14%|█▍        | 1436/10000 [00:32<03:13, 44.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:287.516083  pct:100.000000000\n",
      "[Iter.   10]  loss:61.042091  pct:-78.769158527\n",
      "[Iter.   20]  loss:55.725677  pct:-8.709422892\n",
      "[Iter.   30]  loss:52.357677  pct:-6.043892479\n",
      "[Iter.   40]  loss:49.468521  pct:-5.518114022\n",
      "[Iter.   50]  loss:46.940239  pct:-5.110890943\n",
      "[Iter.   60]  loss:44.696663  pct:-4.779643436\n",
      "[Iter.   70]  loss:42.683464  pct:-4.504136823\n",
      "[Iter.   80]  loss:40.860439  pct:-4.271032800\n",
      "[Iter.   90]  loss:39.196941  pct:-4.071169946\n",
      "[Iter.  100]  loss:37.669239  pct:-3.897503933\n",
      "[Iter.  110]  loss:36.258469  pct:-3.745152416\n",
      "[Iter.  120]  loss:34.949478  pct:-3.610164820\n",
      "[Iter.  130]  loss:33.729862  pct:-3.489654212\n",
      "[Iter.  140]  loss:32.589333  pct:-3.381364636\n",
      "[Iter.  150]  loss:31.519306  pct:-3.283363951\n",
      "[Iter.  160]  loss:30.512524  pct:-3.194177327\n",
      "[Iter.  170]  loss:29.562780  pct:-3.112634280\n",
      "[Iter.  180]  loss:28.664753  pct:-3.037696078\n",
      "[Iter.  190]  loss:27.813778  pct:-2.968715753\n",
      "[Iter.  200]  loss:27.005873  pct:-2.904694211\n",
      "[Iter.  210]  loss:26.237453  pct:-2.845378387\n",
      "[Iter.  220]  loss:25.505455  pct:-2.789898969\n",
      "[Iter.  230]  loss:24.807116  pct:-2.738000407\n",
      "[Iter.  240]  loss:24.139973  pct:-2.689320597\n",
      "[Iter.  250]  loss:23.501823  pct:-2.643537628\n",
      "[Iter.  260]  loss:22.890673  pct:-2.600439679\n",
      "[Iter.  270]  loss:22.304737  pct:-2.559713298\n",
      "[Iter.  280]  loss:21.742449  pct:-2.520936615\n",
      "[Iter.  290]  loss:21.202280  pct:-2.484397075\n",
      "[Iter.  300]  loss:20.682940  pct:-2.449455974\n",
      "[Iter.  310]  loss:20.183216  pct:-2.416114178\n",
      "[Iter.  320]  loss:19.701971  pct:-2.384382343\n",
      "[Iter.  330]  loss:19.238216  pct:-2.353849027\n",
      "[Iter.  340]  loss:18.790991  pct:-2.324672731\n",
      "[Iter.  350]  loss:18.359432  pct:-2.296625084\n",
      "[Iter.  360]  loss:17.942738  pct:-2.269648844\n",
      "[Iter.  370]  loss:17.540157  pct:-2.243694751\n",
      "[Iter.  380]  loss:17.151007  pct:-2.218626734\n",
      "[Iter.  390]  loss:16.774647  pct:-2.194389788\n",
      "[Iter.  400]  loss:16.410480  pct:-2.170932509\n",
      "[Iter.  410]  loss:16.057961  pct:-2.148139349\n",
      "[Iter.  420]  loss:15.716537  pct:-2.126197708\n",
      "[Iter.  430]  loss:15.385753  pct:-2.104686637\n",
      "[Iter.  440]  loss:15.065168  pct:-2.083643900\n",
      "[Iter.  450]  loss:14.754313  pct:-2.063407840\n",
      "[Iter.  460]  loss:14.452795  pct:-2.043588858\n",
      "[Iter.  470]  loss:14.160253  pct:-2.024123756\n",
      "[Iter.  480]  loss:13.876297  pct:-2.005300206\n",
      "[Iter.  490]  loss:13.600616  pct:-1.986701078\n",
      "[Iter.  500]  loss:13.332889  pct:-1.968497919\n",
      "[Iter.  510]  loss:13.072788  pct:-1.950817804\n",
      "[Iter.  520]  loss:12.820045  pct:-1.933357417\n",
      "[Iter.  530]  loss:12.574408  pct:-1.916038120\n",
      "[Iter.  540]  loss:12.335596  pct:-1.899186832\n",
      "[Iter.  550]  loss:12.103371  pct:-1.882563408\n",
      "[Iter.  560]  loss:11.877520  pct:-1.866017865\n",
      "[Iter.  570]  loss:11.657776  pct:-1.850080959\n",
      "[Iter.  580]  loss:11.443970  pct:-1.834021811\n",
      "[Iter.  590]  loss:11.235895  pct:-1.818202727\n",
      "[Iter.  600]  loss:11.033357  pct:-1.802602173\n",
      "[Iter.  610]  loss:10.836176  pct:-1.787132909\n",
      "[Iter.  620]  loss:10.644153  pct:-1.772057585\n",
      "[Iter.  630]  loss:10.457159  pct:-1.756772993\n",
      "[Iter.  640]  loss:10.275023  pct:-1.741740131\n",
      "[Iter.  650]  loss:10.097574  pct:-1.726986706\n",
      "[Iter.  660]  loss:9.924684  pct:-1.712199971\n",
      "[Iter.  670]  loss:9.756208  pct:-1.697536751\n",
      "[Iter.  680]  loss:9.592005  pct:-1.683068224\n",
      "[Iter.  690]  loss:9.431955  pct:-1.668571297\n",
      "[Iter.  700]  loss:9.275936  pct:-1.654155530\n",
      "[Iter.  710]  loss:9.123837  pct:-1.639712193\n",
      "[Iter.  720]  loss:8.975528  pct:-1.625518956\n",
      "[Iter.  730]  loss:8.830890  pct:-1.611471385\n",
      "[Iter.  740]  loss:8.689819  pct:-1.597464929\n",
      "[Iter.  750]  loss:8.552221  pct:-1.583439568\n",
      "[Iter.  760]  loss:8.418022  pct:-1.569172941\n",
      "[Iter.  770]  loss:8.287088  pct:-1.555398159\n",
      "[Iter.  780]  loss:8.159350  pct:-1.541409876\n",
      "[Iter.  790]  loss:8.034711  pct:-1.527566596\n",
      "[Iter.  800]  loss:7.913088  pct:-1.513714235\n",
      "[Iter.  810]  loss:7.794376  pct:-1.500197440\n",
      "[Iter.  820]  loss:7.678525  pct:-1.486346011\n",
      "[Iter.  830]  loss:7.565456  pct:-1.472536169\n",
      "[Iter.  840]  loss:7.455080  pct:-1.458945534\n",
      "[Iter.  850]  loss:7.347357  pct:-1.444957784\n",
      "[Iter.  860]  loss:7.242180  pct:-1.431493281\n",
      "[Iter.  870]  loss:7.139505  pct:-1.417735491\n",
      "[Iter.  880]  loss:7.039262  pct:-1.404062133\n",
      "[Iter.  890]  loss:6.941386  pct:-1.390430813\n",
      "[Iter.  900]  loss:6.845815  pct:-1.376836195\n",
      "[Iter.  910]  loss:6.752482  pct:-1.363355152\n",
      "[Iter.  920]  loss:6.661347  pct:-1.349645198\n",
      "[Iter.  930]  loss:6.572336  pct:-1.336234055\n",
      "[Iter.  940]  loss:6.485399  pct:-1.322770900\n",
      "[Iter.  950]  loss:6.400487  pct:-1.309284084\n",
      "[Iter.  960]  loss:6.317549  pct:-1.295803243\n",
      "[Iter.  970]  loss:6.236545  pct:-1.282208335\n",
      "[Iter.  980]  loss:6.157428  pct:-1.268600166\n",
      "[Iter.  990]  loss:6.080144  pct:-1.255132109\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.725228  pct:100.000000000\n",
      "[Iter.    2]  loss:2.725272  pct:0.001627235\n",
      "[Iter.    4]  loss:2.725268  pct:-0.000157472\n",
      "[Iter.    6]  loss:2.725267  pct:-0.000034994\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.725267\n",
      "Best loss: 2.725267 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  5%|▍         | 454/10000 [00:10<03:45, 42.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:621.930115  pct:100.000000000\n",
      "[Iter.   10]  loss:96.056084  pct:-84.555163188\n",
      "[Iter.   20]  loss:92.245384  pct:-3.967160972\n",
      "[Iter.   30]  loss:88.988098  pct:-3.531110092\n",
      "[Iter.   40]  loss:85.991409  pct:-3.367516449\n",
      "[Iter.   50]  loss:83.145363  pct:-3.309686945\n",
      "[Iter.   60]  loss:80.322937  pct:-3.394567954\n",
      "[Iter.   70]  loss:77.722870  pct:-3.237017016\n",
      "[Iter.   80]  loss:75.399033  pct:-2.989901536\n",
      "[Iter.   90]  loss:73.273987  pct:-2.818399260\n",
      "[Iter.  100]  loss:71.292213  pct:-2.704607000\n",
      "[Iter.  110]  loss:69.426674  pct:-2.616750779\n",
      "[Iter.  120]  loss:67.663956  pct:-2.538963920\n",
      "[Iter.  130]  loss:65.991150  pct:-2.472225824\n",
      "[Iter.  140]  loss:64.399734  pct:-2.411558834\n",
      "[Iter.  150]  loss:62.883583  pct:-2.354282110\n",
      "[Iter.  160]  loss:61.436329  pct:-2.301481739\n",
      "[Iter.  170]  loss:60.052330  pct:-2.252736933\n",
      "[Iter.  180]  loss:58.726749  pct:-2.207375795\n",
      "[Iter.  190]  loss:57.455284  pct:-2.165053088\n",
      "[Iter.  200]  loss:56.234276  pct:-2.125145353\n",
      "[Iter.  210]  loss:55.060352  pct:-2.087558656\n",
      "[Iter.  220]  loss:53.930336  pct:-2.052323095\n",
      "[Iter.  230]  loss:52.841377  pct:-2.019195171\n",
      "[Iter.  240]  loss:51.791073  pct:-1.987655257\n",
      "[Iter.  250]  loss:50.777065  pct:-1.957880987\n",
      "[Iter.  260]  loss:49.797222  pct:-1.929696280\n",
      "[Iter.  270]  loss:48.849621  pct:-1.902920038\n",
      "[Iter.  280]  loss:47.932549  pct:-1.877337594\n",
      "[Iter.  290]  loss:47.044380  pct:-1.852954542\n",
      "[Iter.  300]  loss:46.183548  pct:-1.829830069\n",
      "[Iter.  310]  loss:45.348721  pct:-1.807629469\n",
      "[Iter.  320]  loss:44.538559  pct:-1.786514770\n",
      "[Iter.  330]  loss:43.751900  pct:-1.766243136\n",
      "[Iter.  340]  loss:42.987541  pct:-1.747029330\n",
      "[Iter.  350]  loss:42.244526  pct:-1.728443332\n",
      "[Iter.  360]  loss:41.521873  pct:-1.710641603\n",
      "[Iter.  370]  loss:40.818630  pct:-1.693669377\n",
      "[Iter.  380]  loss:40.134048  pct:-1.677130646\n",
      "[Iter.  390]  loss:39.467304  pct:-1.661293235\n",
      "[Iter.  400]  loss:38.817680  pct:-1.645979840\n",
      "[Iter.  410]  loss:38.184498  pct:-1.631170435\n",
      "[Iter.  420]  loss:37.567059  pct:-1.616989368\n",
      "[Iter.  430]  loss:36.964806  pct:-1.603141111\n",
      "[Iter.  440]  loss:36.377090  pct:-1.589931664\n",
      "[Iter.  450]  loss:35.803398  pct:-1.577070389\n",
      "[Iter.  460]  loss:35.243221  pct:-1.564591292\n",
      "[Iter.  470]  loss:34.695961  pct:-1.552810057\n",
      "[Iter.  480]  loss:34.161308  pct:-1.540965273\n",
      "[Iter.  490]  loss:33.638695  pct:-1.529840488\n",
      "[Iter.  500]  loss:33.127827  pct:-1.518691721\n",
      "[Iter.  510]  loss:32.628216  pct:-1.508130629\n",
      "[Iter.  520]  loss:32.139534  pct:-1.497727600\n",
      "[Iter.  530]  loss:31.661352  pct:-1.487830654\n",
      "[Iter.  540]  loss:31.193430  pct:-1.477897117\n",
      "[Iter.  550]  loss:30.735388  pct:-1.468393010\n",
      "[Iter.  560]  loss:30.286924  pct:-1.459111051\n",
      "[Iter.  570]  loss:29.847769  pct:-1.449984070\n",
      "[Iter.  580]  loss:29.417555  pct:-1.441360429\n",
      "[Iter.  590]  loss:28.996058  pct:-1.432808903\n",
      "[Iter.  600]  loss:28.583080  pct:-1.424252999\n",
      "[Iter.  610]  loss:28.178335  pct:-1.416030385\n",
      "[Iter.  620]  loss:27.781567  pct:-1.408062497\n",
      "[Iter.  630]  loss:27.392601  pct:-1.400085215\n",
      "[Iter.  640]  loss:27.011147  pct:-1.392545628\n",
      "[Iter.  650]  loss:26.637074  pct:-1.384883933\n",
      "[Iter.  660]  loss:26.270151  pct:-1.377487577\n",
      "[Iter.  670]  loss:25.910187  pct:-1.370240958\n",
      "[Iter.  680]  loss:25.556955  pct:-1.363291717\n",
      "[Iter.  690]  loss:25.210304  pct:-1.356386442\n",
      "[Iter.  700]  loss:24.870111  pct:-1.349419631\n",
      "[Iter.  710]  loss:24.536135  pct:-1.342883992\n",
      "[Iter.  720]  loss:24.208239  pct:-1.336380493\n",
      "[Iter.  730]  loss:23.886305  pct:-1.329852005\n",
      "[Iter.  740]  loss:23.570118  pct:-1.323716275\n",
      "[Iter.  750]  loss:23.259583  pct:-1.317496296\n",
      "[Iter.  760]  loss:22.954578  pct:-1.311305221\n",
      "[Iter.  770]  loss:22.654909  pct:-1.305487997\n",
      "[Iter.  780]  loss:22.360554  pct:-1.299300698\n",
      "[Iter.  790]  loss:22.071285  pct:-1.293655323\n",
      "[Iter.  800]  loss:21.787010  pct:-1.287985959\n",
      "[Iter.  810]  loss:21.507563  pct:-1.282633794\n",
      "[Iter.  820]  loss:21.232981  pct:-1.276676087\n",
      "[Iter.  830]  loss:20.963070  pct:-1.271186631\n",
      "[Iter.  840]  loss:20.697742  pct:-1.265689876\n",
      "[Iter.  850]  loss:20.436831  pct:-1.260581641\n",
      "[Iter.  860]  loss:20.180325  pct:-1.255116178\n",
      "[Iter.  870]  loss:19.928085  pct:-1.249926514\n",
      "[Iter.  880]  loss:19.680004  pct:-1.244882302\n",
      "[Iter.  890]  loss:19.435978  pct:-1.239970188\n",
      "[Iter.  900]  loss:19.195940  pct:-1.235018474\n",
      "[Iter.  910]  loss:18.959784  pct:-1.230241725\n",
      "[Iter.  920]  loss:18.727459  pct:-1.225354707\n",
      "[Iter.  930]  loss:18.498901  pct:-1.220440996\n",
      "[Iter.  940]  loss:18.274012  pct:-1.215692493\n",
      "[Iter.  950]  loss:18.052753  pct:-1.210780469\n",
      "[Iter.  960]  loss:17.835033  pct:-1.206021189\n",
      "[Iter.  970]  loss:17.620775  pct:-1.201333291\n",
      "[Iter.  980]  loss:17.409933  pct:-1.196554237\n",
      "[Iter.  990]  loss:17.202394  pct:-1.192070088\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.724986  pct:100.000000000\n",
      "[Iter.    2]  loss:2.724919  pct:-0.002476066\n",
      "[Iter.    4]  loss:2.724894  pct:-0.000918705\n",
      "[Iter.    6]  loss:2.724888  pct:-0.000209992\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.724888\n",
      "Best loss: 2.724888 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  8%|▊         | 764/10000 [00:10<02:02, 75.29it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:397.841797  pct:100.000000000\n",
      "[Iter.   10]  loss:87.146408  pct:-78.095206682\n",
      "[Iter.   20]  loss:80.667480  pct:-7.434532019\n",
      "[Iter.   30]  loss:75.624352  pct:-6.251749699\n",
      "[Iter.   40]  loss:71.635498  pct:-5.274562195\n",
      "[Iter.   50]  loss:68.164482  pct:-4.845385353\n",
      "[Iter.   60]  loss:65.095009  pct:-4.503039077\n",
      "[Iter.   70]  loss:62.347115  pct:-4.221359419\n",
      "[Iter.   80]  loss:59.862728  pct:-3.984765713\n",
      "[Iter.   90]  loss:57.598255  pct:-3.782776082\n",
      "[Iter.  100]  loss:55.520042  pct:-3.608117524\n",
      "[Iter.  110]  loss:53.601543  pct:-3.455507073\n",
      "[Iter.  120]  loss:51.821507  pct:-3.320868789\n",
      "[Iter.  130]  loss:50.162640  pct:-3.201116668\n",
      "[Iter.  140]  loss:48.610703  pct:-3.093810683\n",
      "[Iter.  150]  loss:47.153790  pct:-2.997103352\n",
      "[Iter.  160]  loss:45.781914  pct:-2.909364819\n",
      "[Iter.  170]  loss:44.486492  pct:-2.829548820\n",
      "[Iter.  180]  loss:43.260292  pct:-2.756342531\n",
      "[Iter.  190]  loss:42.097031  pct:-2.688981878\n",
      "[Iter.  200]  loss:40.991142  pct:-2.626998508\n",
      "[Iter.  210]  loss:39.937874  pct:-2.569502517\n",
      "[Iter.  220]  loss:38.932949  pct:-2.516220013\n",
      "[Iter.  230]  loss:37.972744  pct:-2.466304509\n",
      "[Iter.  240]  loss:37.053848  pct:-2.419882329\n",
      "[Iter.  250]  loss:36.173340  pct:-2.376294134\n",
      "[Iter.  260]  loss:35.328526  pct:-2.335461155\n",
      "[Iter.  270]  loss:34.517078  pct:-2.296861052\n",
      "[Iter.  280]  loss:33.736782  pct:-2.260609420\n",
      "[Iter.  290]  loss:32.985718  pct:-2.226247598\n",
      "[Iter.  300]  loss:32.262051  pct:-2.193880242\n",
      "[Iter.  310]  loss:31.564247  pct:-2.162923570\n",
      "[Iter.  320]  loss:30.890791  pct:-2.133604484\n",
      "[Iter.  330]  loss:30.240295  pct:-2.105791109\n",
      "[Iter.  340]  loss:29.611511  pct:-2.079292451\n",
      "[Iter.  350]  loss:29.003347  pct:-2.053808834\n",
      "[Iter.  360]  loss:28.414677  pct:-2.029664792\n",
      "[Iter.  370]  loss:27.844563  pct:-2.006407261\n",
      "[Iter.  380]  loss:27.292065  pct:-1.984221742\n",
      "[Iter.  390]  loss:26.756395  pct:-1.962729216\n",
      "[Iter.  400]  loss:26.236729  pct:-1.942214806\n",
      "[Iter.  410]  loss:25.732332  pct:-1.922482200\n",
      "[Iter.  420]  loss:25.242529  pct:-1.903454805\n",
      "[Iter.  430]  loss:24.766665  pct:-1.885169319\n",
      "[Iter.  440]  loss:24.304144  pct:-1.867512677\n",
      "[Iter.  450]  loss:23.854406  pct:-1.850456246\n",
      "[Iter.  460]  loss:23.416929  pct:-1.833946757\n",
      "[Iter.  470]  loss:22.991238  pct:-1.817879706\n",
      "[Iter.  480]  loss:22.576847  pct:-1.802384763\n",
      "[Iter.  490]  loss:22.173328  pct:-1.787311910\n",
      "[Iter.  500]  loss:21.780239  pct:-1.772802384\n",
      "[Iter.  510]  loss:21.397200  pct:-1.758655966\n",
      "[Iter.  520]  loss:21.023886  pct:-1.744685801\n",
      "[Iter.  530]  loss:20.659874  pct:-1.731420011\n",
      "[Iter.  540]  loss:20.304895  pct:-1.718202938\n",
      "[Iter.  550]  loss:19.958620  pct:-1.705378544\n",
      "[Iter.  560]  loss:19.620750  pct:-1.692850723\n",
      "[Iter.  570]  loss:19.290997  pct:-1.680638449\n",
      "[Iter.  580]  loss:18.969046  pct:-1.668917993\n",
      "[Iter.  590]  loss:18.654676  pct:-1.657274739\n",
      "[Iter.  600]  loss:18.347605  pct:-1.646084224\n",
      "[Iter.  610]  loss:18.047649  pct:-1.634847557\n",
      "[Iter.  620]  loss:17.754557  pct:-1.623993914\n",
      "[Iter.  630]  loss:17.468172  pct:-1.613020184\n",
      "[Iter.  640]  loss:17.188217  pct:-1.602657159\n",
      "[Iter.  650]  loss:16.914526  pct:-1.592318591\n",
      "[Iter.  660]  loss:16.646906  pct:-1.582190875\n",
      "[Iter.  670]  loss:16.385191  pct:-1.572153630\n",
      "[Iter.  680]  loss:16.129166  pct:-1.562540924\n",
      "[Iter.  690]  loss:15.878720  pct:-1.552748427\n",
      "[Iter.  700]  loss:15.633695  pct:-1.543106941\n",
      "[Iter.  710]  loss:15.393903  pct:-1.533814466\n",
      "[Iter.  720]  loss:15.159196  pct:-1.524674295\n",
      "[Iter.  730]  loss:14.929496  pct:-1.515252458\n",
      "[Iter.  740]  loss:14.704620  pct:-1.506249461\n",
      "[Iter.  750]  loss:14.484446  pct:-1.497317061\n",
      "[Iter.  760]  loss:14.268851  pct:-1.488453877\n",
      "[Iter.  770]  loss:14.057709  pct:-1.479744486\n",
      "[Iter.  780]  loss:13.850901  pct:-1.471136542\n",
      "[Iter.  790]  loss:13.648318  pct:-1.462593404\n",
      "[Iter.  800]  loss:13.449869  pct:-1.454018954\n",
      "[Iter.  810]  loss:13.255423  pct:-1.445713423\n",
      "[Iter.  820]  loss:13.064880  pct:-1.437466212\n",
      "[Iter.  830]  loss:12.878153  pct:-1.429232557\n",
      "[Iter.  840]  loss:12.695159  pct:-1.420963791\n",
      "[Iter.  850]  loss:12.515815  pct:-1.412697374\n",
      "[Iter.  860]  loss:12.340005  pct:-1.404701678\n",
      "[Iter.  870]  loss:12.167652  pct:-1.396699531\n",
      "[Iter.  880]  loss:11.998686  pct:-1.388651578\n",
      "[Iter.  890]  loss:11.833019  pct:-1.380706041\n",
      "[Iter.  900]  loss:11.670577  pct:-1.372787484\n",
      "[Iter.  910]  loss:11.511278  pct:-1.364961613\n",
      "[Iter.  920]  loss:11.355047  pct:-1.357198779\n",
      "[Iter.  930]  loss:11.201814  pct:-1.349475040\n",
      "[Iter.  940]  loss:11.051523  pct:-1.341662103\n",
      "[Iter.  950]  loss:10.904115  pct:-1.333829578\n",
      "[Iter.  960]  loss:10.759501  pct:-1.326235310\n",
      "[Iter.  970]  loss:10.617622  pct:-1.318631176\n",
      "[Iter.  980]  loss:10.478424  pct:-1.311011998\n",
      "[Iter.  990]  loss:10.341846  pct:-1.303417434\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.749731  pct:100.000000000\n",
      "[Iter.    2]  loss:2.749753  pct:0.000806367\n",
      "[Iter.    4]  loss:2.749752  pct:-0.000060694\n",
      "[Iter.    6]  loss:2.749752  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.749752\n",
      "Best loss: 2.749752 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1837/10000 [00:24<01:47, 76.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:617.110352  pct:100.000000000\n",
      "[Iter.   10]  loss:105.194870  pct:-82.953634632\n",
      "[Iter.   20]  loss:99.179482  pct:-5.718328745\n",
      "[Iter.   30]  loss:95.355515  pct:-3.855602915\n",
      "[Iter.   40]  loss:91.908058  pct:-3.615371777\n",
      "[Iter.   50]  loss:88.763077  pct:-3.421877741\n",
      "[Iter.   60]  loss:85.875603  pct:-3.253012587\n",
      "[Iter.   70]  loss:83.205742  pct:-3.108986435\n",
      "[Iter.   80]  loss:80.723351  pct:-2.983437562\n",
      "[Iter.   90]  loss:78.404945  pct:-2.872037813\n",
      "[Iter.  100]  loss:76.231247  pct:-2.772399643\n",
      "[Iter.  110]  loss:74.186440  pct:-2.682374375\n",
      "[Iter.  120]  loss:72.257072  pct:-2.600700449\n",
      "[Iter.  130]  loss:70.431183  pct:-2.526935462\n",
      "[Iter.  140]  loss:68.697968  pct:-2.460863586\n",
      "[Iter.  150]  loss:67.052200  pct:-2.395656336\n",
      "[Iter.  160]  loss:65.484879  pct:-2.337465094\n",
      "[Iter.  170]  loss:63.989407  pct:-2.283690506\n",
      "[Iter.  180]  loss:62.559933  pct:-2.233922696\n",
      "[Iter.  190]  loss:61.191410  pct:-2.187538549\n",
      "[Iter.  200]  loss:59.879444  pct:-2.144036133\n",
      "[Iter.  210]  loss:58.619884  pct:-2.103492525\n",
      "[Iter.  220]  loss:57.409233  pct:-2.065257222\n",
      "[Iter.  230]  loss:56.244125  pct:-2.029477950\n",
      "[Iter.  240]  loss:55.121647  pct:-1.995725736\n",
      "[Iter.  250]  loss:54.039154  pct:-1.963825266\n",
      "[Iter.  260]  loss:52.994240  pct:-1.933624358\n",
      "[Iter.  270]  loss:51.984707  pct:-1.904986150\n",
      "[Iter.  280]  loss:51.008492  pct:-1.877889520\n",
      "[Iter.  290]  loss:50.063725  pct:-1.852175922\n",
      "[Iter.  300]  loss:49.148762  pct:-1.827596283\n",
      "[Iter.  310]  loss:48.262012  pct:-1.804214869\n",
      "[Iter.  320]  loss:47.402012  pct:-1.781941047\n",
      "[Iter.  330]  loss:46.567444  pct:-1.760617304\n",
      "[Iter.  340]  loss:45.757069  pct:-1.740218373\n",
      "[Iter.  350]  loss:44.969685  pct:-1.720792124\n",
      "[Iter.  360]  loss:44.204296  pct:-1.702009911\n",
      "[Iter.  370]  loss:43.459854  pct:-1.684094198\n",
      "[Iter.  380]  loss:42.735458  pct:-1.666815885\n",
      "[Iter.  390]  loss:42.030220  pct:-1.650241671\n",
      "[Iter.  400]  loss:41.343319  pct:-1.634302871\n",
      "[Iter.  410]  loss:40.673973  pct:-1.618994006\n",
      "[Iter.  420]  loss:40.021519  pct:-1.604107803\n",
      "[Iter.  430]  loss:39.385235  pct:-1.589854396\n",
      "[Iter.  440]  loss:38.764469  pct:-1.576138085\n",
      "[Iter.  450]  loss:38.158657  pct:-1.562802448\n",
      "[Iter.  460]  loss:37.567226  pct:-1.549925258\n",
      "[Iter.  470]  loss:36.989628  pct:-1.537506564\n",
      "[Iter.  480]  loss:36.425358  pct:-1.525481743\n",
      "[Iter.  490]  loss:35.873947  pct:-1.513809906\n",
      "[Iter.  500]  loss:35.334949  pct:-1.502476569\n",
      "[Iter.  510]  loss:34.807941  pct:-1.491464016\n",
      "[Iter.  520]  loss:34.292496  pct:-1.480827903\n",
      "[Iter.  530]  loss:33.788242  pct:-1.470448204\n",
      "[Iter.  540]  loss:33.294823  pct:-1.460329431\n",
      "[Iter.  550]  loss:32.811821  pct:-1.450681127\n",
      "[Iter.  560]  loss:32.338970  pct:-1.441098925\n",
      "[Iter.  570]  loss:31.875900  pct:-1.431925362\n",
      "[Iter.  580]  loss:31.422308  pct:-1.422994477\n",
      "[Iter.  590]  loss:30.977942  pct:-1.414175100\n",
      "[Iter.  600]  loss:30.542551  pct:-1.405485488\n",
      "[Iter.  610]  loss:30.115839  pct:-1.397106730\n",
      "[Iter.  620]  loss:29.697550  pct:-1.388934190\n",
      "[Iter.  630]  loss:29.287422  pct:-1.381015074\n",
      "[Iter.  640]  loss:28.885284  pct:-1.373073239\n",
      "[Iter.  650]  loss:28.490793  pct:-1.365716847\n",
      "[Iter.  660]  loss:28.103786  pct:-1.358357265\n",
      "[Iter.  670]  loss:27.724148  pct:-1.350845276\n",
      "[Iter.  680]  loss:27.351601  pct:-1.343764116\n",
      "[Iter.  690]  loss:26.985996  pct:-1.336683748\n",
      "[Iter.  700]  loss:26.627106  pct:-1.329913968\n",
      "[Iter.  710]  loss:26.274746  pct:-1.323312325\n",
      "[Iter.  720]  loss:25.928757  pct:-1.316812836\n",
      "[Iter.  730]  loss:25.589031  pct:-1.310226704\n",
      "[Iter.  740]  loss:25.255342  pct:-1.304030360\n",
      "[Iter.  750]  loss:24.927616  pct:-1.297651633\n",
      "[Iter.  760]  loss:24.605606  pct:-1.291780324\n",
      "[Iter.  770]  loss:24.289246  pct:-1.285725182\n",
      "[Iter.  780]  loss:23.978382  pct:-1.279840057\n",
      "[Iter.  790]  loss:23.672884  pct:-1.274056447\n",
      "[Iter.  800]  loss:23.372618  pct:-1.268397488\n",
      "[Iter.  810]  loss:23.077511  pct:-1.262618040\n",
      "[Iter.  820]  loss:22.787378  pct:-1.257208911\n",
      "[Iter.  830]  loss:22.502161  pct:-1.251645895\n",
      "[Iter.  840]  loss:22.221727  pct:-1.246252102\n",
      "[Iter.  850]  loss:21.945953  pct:-1.241010645\n",
      "[Iter.  860]  loss:21.674707  pct:-1.235972536\n",
      "[Iter.  870]  loss:21.407953  pct:-1.230716269\n",
      "[Iter.  880]  loss:21.145567  pct:-1.225648799\n",
      "[Iter.  890]  loss:20.887426  pct:-1.220778637\n",
      "[Iter.  900]  loss:20.633530  pct:-1.215548094\n",
      "[Iter.  910]  loss:20.383684  pct:-1.210871377\n",
      "[Iter.  920]  loss:20.137911  pct:-1.205735497\n",
      "[Iter.  930]  loss:19.896021  pct:-1.201167070\n",
      "[Iter.  940]  loss:19.657974  pct:-1.196453539\n",
      "[Iter.  950]  loss:19.423714  pct:-1.191682094\n",
      "[Iter.  960]  loss:19.193140  pct:-1.187072966\n",
      "[Iter.  970]  loss:18.966177  pct:-1.182521687\n",
      "[Iter.  980]  loss:18.742773  pct:-1.177907023\n",
      "[Iter.  990]  loss:18.522837  pct:-1.173446267\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.768663  pct:100.000000000\n",
      "[Iter.    2]  loss:2.768671  pct:0.000284174\n",
      "[Iter.    4]  loss:2.768666  pct:-0.000189449\n",
      "[Iter.    6]  loss:2.768664  pct:-0.000060279\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.768664\n",
      "Best loss: 2.768664 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 25%|██▍       | 2467/10000 [00:47<02:25, 51.80it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:996.585876  pct:100.000000000\n",
      "[Iter.   10]  loss:163.335266  pct:-83.610517671\n",
      "[Iter.   20]  loss:157.547577  pct:-3.543441258\n",
      "[Iter.   30]  loss:152.967133  pct:-2.907340389\n",
      "[Iter.   40]  loss:148.750961  pct:-2.756259592\n",
      "[Iter.   50]  loss:144.831116  pct:-2.635173277\n",
      "[Iter.   60]  loss:141.167374  pct:-2.529665015\n",
      "[Iter.   70]  loss:137.729095  pct:-2.435603999\n",
      "[Iter.   80]  loss:134.490891  pct:-2.351140799\n",
      "[Iter.   90]  loss:131.433380  pct:-2.273395889\n",
      "[Iter.  100]  loss:128.539215  pct:-2.202001528\n",
      "[Iter.  110]  loss:125.792351  pct:-2.136985446\n",
      "[Iter.  120]  loss:123.179146  pct:-2.077395756\n",
      "[Iter.  130]  loss:120.687950  pct:-2.022416751\n",
      "[Iter.  140]  loss:118.308464  pct:-1.971602038\n",
      "[Iter.  150]  loss:116.031654  pct:-1.924468981\n",
      "[Iter.  160]  loss:113.849594  pct:-1.880573240\n",
      "[Iter.  170]  loss:111.755348  pct:-1.839484740\n",
      "[Iter.  180]  loss:109.742508  pct:-1.801113149\n",
      "[Iter.  190]  loss:107.805511  pct:-1.765037538\n",
      "[Iter.  200]  loss:105.939178  pct:-1.731203704\n",
      "[Iter.  210]  loss:104.138977  pct:-1.699278248\n",
      "[Iter.  220]  loss:102.400848  pct:-1.669047182\n",
      "[Iter.  230]  loss:100.720955  pct:-1.640507398\n",
      "[Iter.  240]  loss:99.095787  pct:-1.613534987\n",
      "[Iter.  250]  loss:97.522133  pct:-1.588013196\n",
      "[Iter.  260]  loss:95.997337  pct:-1.563537924\n",
      "[Iter.  270]  loss:94.518578  pct:-1.540417481\n",
      "[Iter.  280]  loss:93.083389  pct:-1.518419268\n",
      "[Iter.  290]  loss:91.689636  pct:-1.497316613\n",
      "[Iter.  300]  loss:90.335083  pct:-1.477324241\n",
      "[Iter.  310]  loss:89.018051  pct:-1.457940610\n",
      "[Iter.  320]  loss:87.736412  pct:-1.439751918\n",
      "[Iter.  330]  loss:86.488754  pct:-1.422052426\n",
      "[Iter.  340]  loss:85.273384  pct:-1.405234922\n",
      "[Iter.  350]  loss:84.088936  pct:-1.389001099\n",
      "[Iter.  360]  loss:82.934006  pct:-1.373462636\n",
      "[Iter.  370]  loss:81.807297  pct:-1.358560912\n",
      "[Iter.  380]  loss:80.707672  pct:-1.344164491\n",
      "[Iter.  390]  loss:79.634079  pct:-1.330224391\n",
      "[Iter.  400]  loss:78.585342  pct:-1.316944436\n",
      "[Iter.  410]  loss:77.560532  pct:-1.304073711\n",
      "[Iter.  420]  loss:76.558693  pct:-1.291686201\n",
      "[Iter.  430]  loss:75.578918  pct:-1.279769073\n",
      "[Iter.  440]  loss:74.620453  pct:-1.268165245\n",
      "[Iter.  450]  loss:73.682449  pct:-1.257032762\n",
      "[Iter.  460]  loss:72.764259  pct:-1.246144788\n",
      "[Iter.  470]  loss:71.865181  pct:-1.235604371\n",
      "[Iter.  480]  loss:70.984444  pct:-1.225541066\n",
      "[Iter.  490]  loss:70.121437  pct:-1.215768621\n",
      "[Iter.  500]  loss:69.275604  pct:-1.206240003\n",
      "[Iter.  510]  loss:68.446388  pct:-1.196981264\n",
      "[Iter.  520]  loss:67.633202  pct:-1.188063631\n",
      "[Iter.  530]  loss:66.835617  pct:-1.179279577\n",
      "[Iter.  540]  loss:66.053131  pct:-1.170761932\n",
      "[Iter.  550]  loss:65.285133  pct:-1.162696951\n",
      "[Iter.  560]  loss:64.531372  pct:-1.154568050\n",
      "[Iter.  570]  loss:63.791290  pct:-1.146855806\n",
      "[Iter.  580]  loss:63.064587  pct:-1.139189442\n",
      "[Iter.  590]  loss:62.350685  pct:-1.132016489\n",
      "[Iter.  600]  loss:61.649349  pct:-1.124824700\n",
      "[Iter.  610]  loss:60.960239  pct:-1.117789257\n",
      "[Iter.  620]  loss:60.282944  pct:-1.111044988\n",
      "[Iter.  630]  loss:59.617165  pct:-1.104423694\n",
      "[Iter.  640]  loss:58.962589  pct:-1.097964575\n",
      "[Iter.  650]  loss:58.318871  pct:-1.091740928\n",
      "[Iter.  660]  loss:57.685818  pct:-1.085502548\n",
      "[Iter.  670]  loss:57.063084  pct:-1.079527160\n",
      "[Iter.  680]  loss:56.450432  pct:-1.073639533\n",
      "[Iter.  690]  loss:55.847542  pct:-1.067998942\n",
      "[Iter.  700]  loss:55.254211  pct:-1.062410921\n",
      "[Iter.  710]  loss:54.670227  pct:-1.056904732\n",
      "[Iter.  720]  loss:54.095303  pct:-1.051622611\n",
      "[Iter.  730]  loss:53.529274  pct:-1.046354430\n",
      "[Iter.  740]  loss:52.971809  pct:-1.041420064\n",
      "[Iter.  750]  loss:52.422871  pct:-1.036284691\n",
      "[Iter.  760]  loss:51.882179  pct:-1.031403601\n",
      "[Iter.  770]  loss:51.349518  pct:-1.026675143\n",
      "[Iter.  780]  loss:50.824738  pct:-1.021977023\n",
      "[Iter.  790]  loss:50.307682  pct:-1.017330411\n",
      "[Iter.  800]  loss:49.798153  pct:-1.012825662\n",
      "[Iter.  810]  loss:49.295971  pct:-1.008435007\n",
      "[Iter.  820]  loss:48.800957  pct:-1.004167646\n",
      "[Iter.  830]  loss:48.313004  pct:-0.999884467\n",
      "[Iter.  840]  loss:47.831917  pct:-0.995770695\n",
      "[Iter.  850]  loss:47.357647  pct:-0.991534311\n",
      "[Iter.  860]  loss:46.889950  pct:-0.987585266\n",
      "[Iter.  870]  loss:46.428707  pct:-0.983670654\n",
      "[Iter.  880]  loss:45.973724  pct:-0.979959998\n",
      "[Iter.  890]  loss:45.524925  pct:-0.976207909\n",
      "[Iter.  900]  loss:45.082226  pct:-0.972433080\n",
      "[Iter.  910]  loss:44.645527  pct:-0.968672034\n",
      "[Iter.  920]  loss:44.214622  pct:-0.965168111\n",
      "[Iter.  930]  loss:43.789413  pct:-0.961693262\n",
      "[Iter.  940]  loss:43.369816  pct:-0.958217050\n",
      "[Iter.  950]  loss:42.955761  pct:-0.954707468\n",
      "[Iter.  960]  loss:42.547100  pct:-0.951352926\n",
      "[Iter.  970]  loss:42.143715  pct:-0.948090849\n",
      "[Iter.  980]  loss:41.745552  pct:-0.944773954\n",
      "[Iter.  990]  loss:41.352448  pct:-0.941668115\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.744665  pct:100.000000000\n",
      "[Iter.    2]  loss:2.744668  pct:0.000095553\n",
      "[Iter.    4]  loss:2.744660  pct:-0.000260598\n",
      "[Iter.    6]  loss:2.744661  pct:0.000034747\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.744661\n",
      "Best loss: 2.744661 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 23%|██▎       | 2312/10000 [00:33<01:50, 69.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:466.103638  pct:100.000000000\n",
      "[Iter.   10]  loss:80.402718  pct:-82.750035945\n",
      "[Iter.   20]  loss:75.959023  pct:-5.526797105\n",
      "[Iter.   30]  loss:72.462555  pct:-4.603097136\n",
      "[Iter.   40]  loss:69.480896  pct:-4.114758220\n",
      "[Iter.   50]  loss:66.783875  pct:-3.881673438\n",
      "[Iter.   60]  loss:64.320763  pct:-3.688183555\n",
      "[Iter.   70]  loss:62.056049  pct:-3.520967716\n",
      "[Iter.   80]  loss:59.962048  pct:-3.374371704\n",
      "[Iter.   90]  loss:58.016487  pct:-3.244653133\n",
      "[Iter.  100]  loss:56.201187  pct:-3.128938131\n",
      "[Iter.  110]  loss:54.501034  pct:-3.025119998\n",
      "[Iter.  120]  loss:52.903400  pct:-2.931381757\n",
      "[Iter.  130]  loss:51.397564  pct:-2.846388842\n",
      "[Iter.  140]  loss:49.974430  pct:-2.768874128\n",
      "[Iter.  150]  loss:48.626221  pct:-2.697798412\n",
      "[Iter.  160]  loss:47.346165  pct:-2.632439826\n",
      "[Iter.  170]  loss:46.128418  pct:-2.572007136\n",
      "[Iter.  180]  loss:44.967758  pct:-2.516149136\n",
      "[Iter.  190]  loss:43.859695  pct:-2.464127164\n",
      "[Iter.  200]  loss:42.800186  pct:-2.415678602\n",
      "[Iter.  210]  loss:41.785717  pct:-2.370244706\n",
      "[Iter.  220]  loss:40.812939  pct:-2.328016341\n",
      "[Iter.  230]  loss:39.879135  pct:-2.288008627\n",
      "[Iter.  240]  loss:38.981663  pct:-2.250481056\n",
      "[Iter.  250]  loss:38.118176  pct:-2.215111370\n",
      "[Iter.  260]  loss:37.286575  pct:-2.181636918\n",
      "[Iter.  270]  loss:36.484943  pct:-2.149921039\n",
      "[Iter.  280]  loss:35.711456  pct:-2.120017243\n",
      "[Iter.  290]  loss:34.964592  pct:-2.091385780\n",
      "[Iter.  300]  loss:34.242828  pct:-2.064270080\n",
      "[Iter.  310]  loss:33.544807  pct:-2.038444160\n",
      "[Iter.  320]  loss:32.869293  pct:-2.013766877\n",
      "[Iter.  330]  loss:32.215157  pct:-1.990114766\n",
      "[Iter.  340]  loss:31.581305  pct:-1.967558357\n",
      "[Iter.  350]  loss:30.966719  pct:-1.946043348\n",
      "[Iter.  360]  loss:30.370533  pct:-1.925246554\n",
      "[Iter.  370]  loss:29.791883  pct:-1.905299196\n",
      "[Iter.  380]  loss:29.229980  pct:-1.886094246\n",
      "[Iter.  390]  loss:28.684034  pct:-1.867760814\n",
      "[Iter.  400]  loss:28.153381  pct:-1.849994298\n",
      "[Iter.  410]  loss:27.637367  pct:-1.832867224\n",
      "[Iter.  420]  loss:27.135380  pct:-1.816336023\n",
      "[Iter.  430]  loss:26.646811  pct:-1.800488010\n",
      "[Iter.  440]  loss:26.171135  pct:-1.785112640\n",
      "[Iter.  450]  loss:25.707886  pct:-1.770076871\n",
      "[Iter.  460]  loss:25.256527  pct:-1.755721181\n",
      "[Iter.  470]  loss:24.816687  pct:-1.741491685\n",
      "[Iter.  480]  loss:24.387882  pct:-1.727887385\n",
      "[Iter.  490]  loss:23.969707  pct:-1.714682479\n",
      "[Iter.  500]  loss:23.561800  pct:-1.701762469\n",
      "[Iter.  510]  loss:23.163750  pct:-1.689388367\n",
      "[Iter.  520]  loss:22.775249  pct:-1.677190519\n",
      "[Iter.  530]  loss:22.395979  pct:-1.665275078\n",
      "[Iter.  540]  loss:22.025660  pct:-1.653508282\n",
      "[Iter.  550]  loss:21.663946  pct:-1.642236449\n",
      "[Iter.  560]  loss:21.310562  pct:-1.631207978\n",
      "[Iter.  570]  loss:20.965221  pct:-1.620514403\n",
      "[Iter.  580]  loss:20.627686  pct:-1.609979936\n",
      "[Iter.  590]  loss:20.297714  pct:-1.599652626\n",
      "[Iter.  600]  loss:19.975060  pct:-1.589611128\n",
      "[Iter.  610]  loss:19.659536  pct:-1.579585520\n",
      "[Iter.  620]  loss:19.350903  pct:-1.569893606\n",
      "[Iter.  630]  loss:19.048956  pct:-1.560374970\n",
      "[Iter.  640]  loss:18.753468  pct:-1.551205005\n",
      "[Iter.  650]  loss:18.464308  pct:-1.541900312\n",
      "[Iter.  660]  loss:18.181301  pct:-1.532722869\n",
      "[Iter.  670]  loss:17.904230  pct:-1.523933834\n",
      "[Iter.  680]  loss:17.632938  pct:-1.515238192\n",
      "[Iter.  690]  loss:17.367256  pct:-1.506738212\n",
      "[Iter.  700]  loss:17.107073  pct:-1.498125737\n",
      "[Iter.  710]  loss:16.852217  pct:-1.489770413\n",
      "[Iter.  720]  loss:16.602564  pct:-1.481424472\n",
      "[Iter.  730]  loss:16.357941  pct:-1.473406073\n",
      "[Iter.  740]  loss:16.118254  pct:-1.465263695\n",
      "[Iter.  750]  loss:15.883354  pct:-1.457350933\n",
      "[Iter.  760]  loss:15.653113  pct:-1.449573051\n",
      "[Iter.  770]  loss:15.427423  pct:-1.441821079\n",
      "[Iter.  780]  loss:15.206183  pct:-1.434069947\n",
      "[Iter.  790]  loss:14.989265  pct:-1.426511738\n",
      "[Iter.  800]  loss:14.776593  pct:-1.418830258\n",
      "[Iter.  810]  loss:14.567997  pct:-1.411666591\n",
      "[Iter.  820]  loss:14.363431  pct:-1.404215021\n",
      "[Iter.  830]  loss:14.162792  pct:-1.396872178\n",
      "[Iter.  840]  loss:13.965990  pct:-1.389571607\n",
      "[Iter.  850]  loss:13.772919  pct:-1.382439515\n",
      "[Iter.  860]  loss:13.583499  pct:-1.375305776\n",
      "[Iter.  870]  loss:13.397655  pct:-1.368156087\n",
      "[Iter.  880]  loss:13.215307  pct:-1.361045979\n",
      "[Iter.  890]  loss:13.036365  pct:-1.354056150\n",
      "[Iter.  900]  loss:12.860765  pct:-1.346994382\n",
      "[Iter.  910]  loss:12.688415  pct:-1.340129280\n",
      "[Iter.  920]  loss:12.519255  pct:-1.333183813\n",
      "[Iter.  930]  loss:12.353217  pct:-1.326257542\n",
      "[Iter.  940]  loss:12.190213  pct:-1.319526079\n",
      "[Iter.  950]  loss:12.030190  pct:-1.312714822\n",
      "[Iter.  960]  loss:11.873104  pct:-1.305767958\n",
      "[Iter.  970]  loss:11.718861  pct:-1.299099780\n",
      "[Iter.  980]  loss:11.567407  pct:-1.292395026\n",
      "[Iter.  990]  loss:11.418705  pct:-1.285522955\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.743438  pct:100.000000000\n",
      "[Iter.    2]  loss:2.743476  pct:0.001416552\n",
      "[Iter.    4]  loss:2.743471  pct:-0.000208569\n",
      "[Iter.    6]  loss:2.743468  pct:-0.000112975\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.743468\n",
      "Best loss: 2.743468 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1692/10000 [00:26<02:12, 62.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3936.020264  pct:100.000000000\n",
      "[Iter.   10]  loss:631.205933  pct:-83.963349570\n",
      "[Iter.   20]  loss:618.082458  pct:-2.079111340\n",
      "[Iter.   30]  loss:608.912109  pct:-1.483677298\n",
      "[Iter.   40]  loss:601.257996  pct:-1.257014543\n",
      "[Iter.   50]  loss:593.978394  pct:-1.210728523\n",
      "[Iter.   60]  loss:586.954834  pct:-1.182460447\n",
      "[Iter.   70]  loss:580.164978  pct:-1.156793600\n",
      "[Iter.   80]  loss:573.595337  pct:-1.132374646\n",
      "[Iter.   90]  loss:567.230957  pct:-1.109559209\n",
      "[Iter.  100]  loss:561.061951  pct:-1.087565175\n",
      "[Iter.  110]  loss:555.080811  pct:-1.066039165\n",
      "[Iter.  120]  loss:549.274292  pct:-1.046067247\n",
      "[Iter.  130]  loss:543.632263  pct:-1.027178750\n",
      "[Iter.  140]  loss:538.146362  pct:-1.009119813\n",
      "[Iter.  150]  loss:532.808838  pct:-0.991835082\n",
      "[Iter.  160]  loss:527.612061  pct:-0.975354944\n",
      "[Iter.  170]  loss:522.549744  pct:-0.959477099\n",
      "[Iter.  180]  loss:517.615601  pct:-0.944243706\n",
      "[Iter.  190]  loss:512.803589  pct:-0.929649669\n",
      "[Iter.  200]  loss:508.108337  pct:-0.915604252\n",
      "[Iter.  210]  loss:503.524872  pct:-0.902064627\n",
      "[Iter.  220]  loss:499.048157  pct:-0.889075265\n",
      "[Iter.  230]  loss:494.673584  pct:-0.876583291\n",
      "[Iter.  240]  loss:490.397278  pct:-0.864470287\n",
      "[Iter.  250]  loss:486.214813  pct:-0.852872719\n",
      "[Iter.  260]  loss:482.122803  pct:-0.841605477\n",
      "[Iter.  270]  loss:478.117462  pct:-0.830771860\n",
      "[Iter.  280]  loss:474.195282  pct:-0.820338198\n",
      "[Iter.  290]  loss:470.353271  pct:-0.810216939\n",
      "[Iter.  300]  loss:466.588684  pct:-0.800374448\n",
      "[Iter.  310]  loss:462.898407  pct:-0.790905829\n",
      "[Iter.  320]  loss:459.279816  pct:-0.781724727\n",
      "[Iter.  330]  loss:455.730591  pct:-0.772780499\n",
      "[Iter.  340]  loss:452.247711  pct:-0.764240915\n",
      "[Iter.  350]  loss:448.829437  pct:-0.755841067\n",
      "[Iter.  360]  loss:445.473450  pct:-0.747720018\n",
      "[Iter.  370]  loss:442.177521  pct:-0.739871020\n",
      "[Iter.  380]  loss:438.939819  pct:-0.732217552\n",
      "[Iter.  390]  loss:435.758514  pct:-0.724770183\n",
      "[Iter.  400]  loss:432.631561  pct:-0.717588532\n",
      "[Iter.  410]  loss:429.557495  pct:-0.710550602\n",
      "[Iter.  420]  loss:426.534485  pct:-0.703749856\n",
      "[Iter.  430]  loss:423.561127  pct:-0.697096779\n",
      "[Iter.  440]  loss:420.635986  pct:-0.690606431\n",
      "[Iter.  450]  loss:417.757355  pct:-0.684352192\n",
      "[Iter.  460]  loss:414.924286  pct:-0.678161333\n",
      "[Iter.  470]  loss:412.135315  pct:-0.672163824\n",
      "[Iter.  480]  loss:409.388977  pct:-0.666368009\n",
      "[Iter.  490]  loss:406.684326  pct:-0.660655521\n",
      "[Iter.  500]  loss:404.019806  pct:-0.655181450\n",
      "[Iter.  510]  loss:401.394745  pct:-0.649735730\n",
      "[Iter.  520]  loss:398.807861  pct:-0.644473695\n",
      "[Iter.  530]  loss:396.258362  pct:-0.639280154\n",
      "[Iter.  540]  loss:393.745056  pct:-0.634259338\n",
      "[Iter.  550]  loss:391.267029  pct:-0.629348180\n",
      "[Iter.  560]  loss:388.823517  pct:-0.624512617\n",
      "[Iter.  570]  loss:386.413269  pct:-0.619882208\n",
      "[Iter.  580]  loss:384.036041  pct:-0.615203455\n",
      "[Iter.  590]  loss:381.690491  pct:-0.610763128\n",
      "[Iter.  600]  loss:379.376190  pct:-0.606329105\n",
      "[Iter.  610]  loss:377.092072  pct:-0.602072221\n",
      "[Iter.  620]  loss:374.837799  pct:-0.597804258\n",
      "[Iter.  630]  loss:372.612366  pct:-0.593705692\n",
      "[Iter.  640]  loss:370.415222  pct:-0.589659323\n",
      "[Iter.  650]  loss:368.245453  pct:-0.585766771\n",
      "[Iter.  660]  loss:366.102936  pct:-0.581817663\n",
      "[Iter.  670]  loss:363.986725  pct:-0.578037139\n",
      "[Iter.  680]  loss:361.896423  pct:-0.574279602\n",
      "[Iter.  690]  loss:359.831085  pct:-0.570698687\n",
      "[Iter.  700]  loss:357.790344  pct:-0.567138597\n",
      "[Iter.  710]  loss:355.773865  pct:-0.563592485\n",
      "[Iter.  720]  loss:353.780914  pct:-0.560173368\n",
      "[Iter.  730]  loss:351.811127  pct:-0.556781759\n",
      "[Iter.  740]  loss:349.863892  pct:-0.553488778\n",
      "[Iter.  750]  loss:347.938873  pct:-0.550219201\n",
      "[Iter.  760]  loss:346.035309  pct:-0.547097378\n",
      "[Iter.  770]  loss:344.153168  pct:-0.543915914\n",
      "[Iter.  780]  loss:342.291931  pct:-0.540816342\n",
      "[Iter.  790]  loss:340.450989  pct:-0.537828156\n",
      "[Iter.  800]  loss:338.630005  pct:-0.534874019\n",
      "[Iter.  810]  loss:336.828583  pct:-0.531973568\n",
      "[Iter.  820]  loss:335.046570  pct:-0.529056330\n",
      "[Iter.  830]  loss:333.283020  pct:-0.526359606\n",
      "[Iter.  840]  loss:331.538086  pct:-0.523559251\n",
      "[Iter.  850]  loss:329.811462  pct:-0.520791912\n",
      "[Iter.  860]  loss:328.102783  pct:-0.518077567\n",
      "[Iter.  870]  loss:326.411591  pct:-0.515445986\n",
      "[Iter.  880]  loss:324.737427  pct:-0.512899623\n",
      "[Iter.  890]  loss:323.080017  pct:-0.510384554\n",
      "[Iter.  900]  loss:321.439056  pct:-0.507911541\n",
      "[Iter.  910]  loss:319.814423  pct:-0.505425136\n",
      "[Iter.  920]  loss:318.205505  pct:-0.503078386\n",
      "[Iter.  930]  loss:316.612366  pct:-0.500663760\n",
      "[Iter.  940]  loss:315.034637  pct:-0.498315430\n",
      "[Iter.  950]  loss:313.472260  pct:-0.495938460\n",
      "[Iter.  960]  loss:311.924530  pct:-0.493737307\n",
      "[Iter.  970]  loss:310.391479  pct:-0.491481236\n",
      "[Iter.  980]  loss:308.872833  pct:-0.489268018\n",
      "[Iter.  990]  loss:307.368408  pct:-0.487069398\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.788718  pct:100.000000000\n",
      "[Iter.    2]  loss:2.788279  pct:-0.015756538\n",
      "[Iter.    4]  loss:2.788169  pct:-0.003933342\n",
      "[Iter.    6]  loss:2.788124  pct:-0.001607603\n",
      "[Iter.    8]  loss:2.788108  pct:-0.000598585\n",
      "[Iter.   10]  loss:2.788092  pct:-0.000564384\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.788092\n",
      "Best loss: 2.788092 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:39<00:00, 62.83it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:451.455414  pct:100.000000000\n",
      "[Iter.   10]  loss:74.224220  pct:-83.558905182\n",
      "[Iter.   20]  loss:70.471664  pct:-5.055702617\n",
      "[Iter.   30]  loss:67.128281  pct:-4.744295195\n",
      "[Iter.   40]  loss:64.427795  pct:-4.022872631\n",
      "[Iter.   50]  loss:61.981934  pct:-3.796283577\n",
      "[Iter.   60]  loss:59.741470  pct:-3.614703716\n",
      "[Iter.   70]  loss:57.675579  pct:-3.458052261\n",
      "[Iter.   80]  loss:55.760487  pct:-3.320456420\n",
      "[Iter.   90]  loss:53.976925  pct:-3.198612163\n",
      "[Iter.  100]  loss:52.309315  pct:-3.089487168\n",
      "[Iter.  110]  loss:50.744568  pct:-2.991335032\n",
      "[Iter.  120]  loss:49.271751  pct:-2.902412079\n",
      "[Iter.  130]  loss:47.881557  pct:-2.821482695\n",
      "[Iter.  140]  loss:46.566032  pct:-2.747456692\n",
      "[Iter.  150]  loss:45.318222  pct:-2.679657895\n",
      "[Iter.  160]  loss:44.132267  pct:-2.616949638\n",
      "[Iter.  170]  loss:43.002914  pct:-2.559017804\n",
      "[Iter.  180]  loss:41.925591  pct:-2.505234652\n",
      "[Iter.  190]  loss:40.896248  pct:-2.455165541\n",
      "[Iter.  200]  loss:39.911293  pct:-2.408423475\n",
      "[Iter.  210]  loss:38.967556  pct:-2.364586458\n",
      "[Iter.  220]  loss:38.062138  pct:-2.323518560\n",
      "[Iter.  230]  loss:37.192478  pct:-2.284841258\n",
      "[Iter.  240]  loss:36.356197  pct:-2.248521378\n",
      "[Iter.  250]  loss:35.551247  pct:-2.214067402\n",
      "[Iter.  260]  loss:34.775684  pct:-2.181533306\n",
      "[Iter.  270]  loss:34.027786  pct:-2.150635180\n",
      "[Iter.  280]  loss:33.305935  pct:-2.121358538\n",
      "[Iter.  290]  loss:32.608665  pct:-2.093529101\n",
      "[Iter.  300]  loss:31.934647  pct:-2.066993084\n",
      "[Iter.  310]  loss:31.282616  pct:-2.041766589\n",
      "[Iter.  320]  loss:30.651495  pct:-2.017480535\n",
      "[Iter.  330]  loss:30.040215  pct:-1.994292421\n",
      "[Iter.  340]  loss:29.447805  pct:-1.972053606\n",
      "[Iter.  350]  loss:28.873323  pct:-1.950848140\n",
      "[Iter.  360]  loss:28.315908  pct:-1.930553681\n",
      "[Iter.  370]  loss:27.774797  pct:-1.910978748\n",
      "[Iter.  380]  loss:27.249260  pct:-1.892137979\n",
      "[Iter.  390]  loss:26.738642  pct:-1.873879184\n",
      "[Iter.  400]  loss:26.242340  pct:-1.856121399\n",
      "[Iter.  410]  loss:25.759649  pct:-1.839358874\n",
      "[Iter.  420]  loss:25.290085  pct:-1.822868133\n",
      "[Iter.  430]  loss:24.833061  pct:-1.807125692\n",
      "[Iter.  440]  loss:24.388145  pct:-1.791626766\n",
      "[Iter.  450]  loss:23.954819  pct:-1.776792426\n",
      "[Iter.  460]  loss:23.532633  pct:-1.762425768\n",
      "[Iter.  470]  loss:23.121172  pct:-1.748469368\n",
      "[Iter.  480]  loss:22.720062  pct:-1.734815589\n",
      "[Iter.  490]  loss:22.328913  pct:-1.721604089\n",
      "[Iter.  500]  loss:21.947390  pct:-1.708650738\n",
      "[Iter.  510]  loss:21.575138  pct:-1.696108363\n",
      "[Iter.  520]  loss:21.211840  pct:-1.683875276\n",
      "[Iter.  530]  loss:20.857201  pct:-1.671892013\n",
      "[Iter.  540]  loss:20.510933  pct:-1.660183006\n",
      "[Iter.  550]  loss:20.172750  pct:-1.648791162\n",
      "[Iter.  560]  loss:19.842348  pct:-1.637864776\n",
      "[Iter.  570]  loss:19.519529  pct:-1.626918117\n",
      "[Iter.  580]  loss:19.204065  pct:-1.616145626\n",
      "[Iter.  590]  loss:18.895676  pct:-1.605856148\n",
      "[Iter.  600]  loss:18.594183  pct:-1.595564490\n",
      "[Iter.  610]  loss:18.299376  pct:-1.585482054\n",
      "[Iter.  620]  loss:18.011051  pct:-1.575596695\n",
      "[Iter.  630]  loss:17.729038  pct:-1.565777237\n",
      "[Iter.  640]  loss:17.453089  pct:-1.556483067\n",
      "[Iter.  650]  loss:17.183064  pct:-1.547148800\n",
      "[Iter.  660]  loss:16.918835  pct:-1.537728244\n",
      "[Iter.  670]  loss:16.660210  pct:-1.528622008\n",
      "[Iter.  680]  loss:16.407015  pct:-1.519757639\n",
      "[Iter.  690]  loss:16.159111  pct:-1.510962391\n",
      "[Iter.  700]  loss:15.916392  pct:-1.502054762\n",
      "[Iter.  710]  loss:15.678673  pct:-1.493551622\n",
      "[Iter.  720]  loss:15.445849  pct:-1.484968626\n",
      "[Iter.  730]  loss:15.217759  pct:-1.476709244\n",
      "[Iter.  740]  loss:14.994331  pct:-1.468204159\n",
      "[Iter.  750]  loss:14.775415  pct:-1.459991340\n",
      "[Iter.  760]  loss:14.560878  pct:-1.451990448\n",
      "[Iter.  770]  loss:14.350636  pct:-1.443884595\n",
      "[Iter.  780]  loss:14.144595  pct:-1.435757894\n",
      "[Iter.  790]  loss:13.942601  pct:-1.428064502\n",
      "[Iter.  800]  loss:13.744578  pct:-1.420271867\n",
      "[Iter.  810]  loss:13.550462  pct:-1.412313912\n",
      "[Iter.  820]  loss:13.360118  pct:-1.404703840\n",
      "[Iter.  830]  loss:13.173461  pct:-1.397120543\n",
      "[Iter.  840]  loss:12.990383  pct:-1.389747256\n",
      "[Iter.  850]  loss:12.810867  pct:-1.381913348\n",
      "[Iter.  860]  loss:12.634777  pct:-1.374538009\n",
      "[Iter.  870]  loss:12.462043  pct:-1.367133426\n",
      "[Iter.  880]  loss:12.292613  pct:-1.359566659\n",
      "[Iter.  890]  loss:12.126405  pct:-1.352098751\n",
      "[Iter.  900]  loss:11.963324  pct:-1.344843524\n",
      "[Iter.  910]  loss:11.803288  pct:-1.337714659\n",
      "[Iter.  920]  loss:11.646283  pct:-1.330182776\n",
      "[Iter.  930]  loss:11.492188  pct:-1.323131575\n",
      "[Iter.  940]  loss:11.340981  pct:-1.315728764\n",
      "[Iter.  950]  loss:11.192578  pct:-1.308556653\n",
      "[Iter.  960]  loss:11.046935  pct:-1.301248293\n",
      "[Iter.  970]  loss:10.903961  pct:-1.294240428\n",
      "[Iter.  980]  loss:10.763611  pct:-1.287150050\n",
      "[Iter.  990]  loss:10.625863  pct:-1.279754226\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.767709  pct:100.000000000\n",
      "[Iter.    2]  loss:2.767773  pct:0.002334474\n",
      "[Iter.    4]  loss:2.767774  pct:0.000008614\n",
      "[Iter.    6]  loss:2.767773  pct:-0.000025842\n",
      "[Iter.    8]  loss:2.767773  pct:0.000008614\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.767773\n",
      "Best loss: 2.767773 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 10%|█         | 1046/10000 [00:25<03:35, 41.50it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5387.118164  pct:100.000000000\n",
      "[Iter.   10]  loss:764.223816  pct:-85.813869131\n",
      "[Iter.   20]  loss:754.595215  pct:-1.259919002\n",
      "[Iter.   30]  loss:747.276672  pct:-0.969863357\n",
      "[Iter.   40]  loss:740.792664  pct:-0.867685160\n",
      "[Iter.   50]  loss:734.522339  pct:-0.846434504\n",
      "[Iter.   60]  loss:728.397827  pct:-0.833808775\n",
      "[Iter.   70]  loss:722.407166  pct:-0.822443642\n",
      "[Iter.   80]  loss:716.543335  pct:-0.811707143\n",
      "[Iter.   90]  loss:710.800598  pct:-0.801450036\n",
      "[Iter.  100]  loss:705.174133  pct:-0.791567264\n",
      "[Iter.  110]  loss:699.660278  pct:-0.781913959\n",
      "[Iter.  120]  loss:694.256592  pct:-0.772330042\n",
      "[Iter.  130]  loss:688.964355  pct:-0.762288236\n",
      "[Iter.  140]  loss:683.784729  pct:-0.751798903\n",
      "[Iter.  150]  loss:678.716553  pct:-0.741194714\n",
      "[Iter.  160]  loss:673.756470  pct:-0.730803306\n",
      "[Iter.  170]  loss:668.898926  pct:-0.720964349\n",
      "[Iter.  180]  loss:664.139221  pct:-0.711573065\n",
      "[Iter.  190]  loss:659.471680  pct:-0.702795642\n",
      "[Iter.  200]  loss:654.892212  pct:-0.694414622\n",
      "[Iter.  210]  loss:650.398865  pct:-0.686120110\n",
      "[Iter.  220]  loss:645.989319  pct:-0.677975645\n",
      "[Iter.  230]  loss:641.660156  pct:-0.670160090\n",
      "[Iter.  240]  loss:637.407471  pct:-0.662762914\n",
      "[Iter.  250]  loss:633.228394  pct:-0.655636675\n",
      "[Iter.  260]  loss:629.120544  pct:-0.648715244\n",
      "[Iter.  270]  loss:625.081726  pct:-0.641978456\n",
      "[Iter.  280]  loss:621.109375  pct:-0.635493074\n",
      "[Iter.  290]  loss:617.200500  pct:-0.629337548\n",
      "[Iter.  300]  loss:613.354492  pct:-0.623137586\n",
      "[Iter.  310]  loss:609.568787  pct:-0.617213311\n",
      "[Iter.  320]  loss:605.841370  pct:-0.611484229\n",
      "[Iter.  330]  loss:602.170593  pct:-0.605897278\n",
      "[Iter.  340]  loss:598.554626  pct:-0.600488771\n",
      "[Iter.  350]  loss:594.992249  pct:-0.595163377\n",
      "[Iter.  360]  loss:591.481628  pct:-0.590027874\n",
      "[Iter.  370]  loss:588.021667  pct:-0.584965073\n",
      "[Iter.  380]  loss:584.610901  pct:-0.580040973\n",
      "[Iter.  390]  loss:581.248352  pct:-0.575177237\n",
      "[Iter.  400]  loss:577.932129  pct:-0.570534632\n",
      "[Iter.  410]  loss:574.661560  pct:-0.565908812\n",
      "[Iter.  420]  loss:571.435120  pct:-0.561450540\n",
      "[Iter.  430]  loss:568.252319  pct:-0.556983669\n",
      "[Iter.  440]  loss:565.111572  pct:-0.552702904\n",
      "[Iter.  450]  loss:562.012024  pct:-0.548484316\n",
      "[Iter.  460]  loss:558.952820  pct:-0.544330721\n",
      "[Iter.  470]  loss:555.932800  pct:-0.540299543\n",
      "[Iter.  480]  loss:552.950989  pct:-0.536361863\n",
      "[Iter.  490]  loss:550.006714  pct:-0.532465799\n",
      "[Iter.  500]  loss:547.099060  pct:-0.528657875\n",
      "[Iter.  510]  loss:544.227173  pct:-0.524930020\n",
      "[Iter.  520]  loss:541.390198  pct:-0.521285088\n",
      "[Iter.  530]  loss:538.587219  pct:-0.517737212\n",
      "[Iter.  540]  loss:535.817627  pct:-0.514232827\n",
      "[Iter.  550]  loss:533.080627  pct:-0.510808039\n",
      "[Iter.  560]  loss:530.375671  pct:-0.507419688\n",
      "[Iter.  570]  loss:527.701904  pct:-0.504127024\n",
      "[Iter.  580]  loss:525.058899  pct:-0.500851968\n",
      "[Iter.  590]  loss:522.445435  pct:-0.497746893\n",
      "[Iter.  600]  loss:519.861389  pct:-0.494605798\n",
      "[Iter.  610]  loss:517.305908  pct:-0.491569678\n",
      "[Iter.  620]  loss:514.778809  pct:-0.488511646\n",
      "[Iter.  630]  loss:512.279846  pct:-0.485443915\n",
      "[Iter.  640]  loss:509.808350  pct:-0.482450481\n",
      "[Iter.  650]  loss:507.363129  pct:-0.479635327\n",
      "[Iter.  660]  loss:504.944031  pct:-0.476798128\n",
      "[Iter.  670]  loss:502.550293  pct:-0.474060024\n",
      "[Iter.  680]  loss:500.181366  pct:-0.471381081\n",
      "[Iter.  690]  loss:497.836823  pct:-0.468738665\n",
      "[Iter.  700]  loss:495.516144  pct:-0.466152483\n",
      "[Iter.  710]  loss:493.218811  pct:-0.463624201\n",
      "[Iter.  720]  loss:490.944214  pct:-0.461174050\n",
      "[Iter.  730]  loss:488.692657  pct:-0.458617564\n",
      "[Iter.  740]  loss:486.463379  pct:-0.456171897\n",
      "[Iter.  750]  loss:484.256165  pct:-0.453726725\n",
      "[Iter.  760]  loss:482.070129  pct:-0.451421235\n",
      "[Iter.  770]  loss:479.905212  pct:-0.449087562\n",
      "[Iter.  780]  loss:477.760956  pct:-0.446808356\n",
      "[Iter.  790]  loss:475.637024  pct:-0.444559535\n",
      "[Iter.  800]  loss:473.532898  pct:-0.442380612\n",
      "[Iter.  810]  loss:471.448822  pct:-0.440112173\n",
      "[Iter.  820]  loss:469.384003  pct:-0.437973167\n",
      "[Iter.  830]  loss:467.338013  pct:-0.435888309\n",
      "[Iter.  840]  loss:465.310944  pct:-0.433747959\n",
      "[Iter.  850]  loss:463.301910  pct:-0.431761434\n",
      "[Iter.  860]  loss:461.311249  pct:-0.429668339\n",
      "[Iter.  870]  loss:459.338104  pct:-0.427725215\n",
      "[Iter.  880]  loss:457.382721  pct:-0.425695862\n",
      "[Iter.  890]  loss:455.444305  pct:-0.423806025\n",
      "[Iter.  900]  loss:453.522614  pct:-0.421937847\n",
      "[Iter.  910]  loss:451.618103  pct:-0.419937274\n",
      "[Iter.  920]  loss:449.729797  pct:-0.418120011\n",
      "[Iter.  930]  loss:447.857635  pct:-0.416285929\n",
      "[Iter.  940]  loss:446.001465  pct:-0.414455512\n",
      "[Iter.  950]  loss:444.160767  pct:-0.412711255\n",
      "[Iter.  960]  loss:442.335449  pct:-0.410958716\n",
      "[Iter.  970]  loss:440.525116  pct:-0.409267052\n",
      "[Iter.  980]  loss:438.729980  pct:-0.407499013\n",
      "[Iter.  990]  loss:436.949646  pct:-0.405792755\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.842166  pct:100.000000000\n",
      "[Iter.    2]  loss:2.841106  pct:-0.037287428\n",
      "[Iter.    4]  loss:2.840555  pct:-0.019384947\n",
      "[Iter.    6]  loss:2.840227  pct:-0.011566076\n",
      "[Iter.    8]  loss:2.840057  pct:-0.005993566\n",
      "[Iter.   10]  loss:2.839942  pct:-0.004054714\n",
      "[Iter.   12]  loss:2.839888  pct:-0.001888918\n",
      "[Iter.   14]  loss:2.839823  pct:-0.002283536\n",
      "[Iter.   16]  loss:2.839791  pct:-0.001125003\n",
      "[Iter.   18]  loss:2.839740  pct:-0.001788271\n",
      "[Iter.   20]  loss:2.839718  pct:-0.000772413\n",
      "[Iter.   22]  loss:2.839686  pct:-0.001125044\n",
      "[Iter.   24]  loss:2.839682  pct:-0.000159523\n",
      "[Iter.   26]  loss:2.839664  pct:-0.000646489\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.839664\n",
      "Best loss: 2.839664 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 324/10000 [00:06<03:11, 50.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:328.046021  pct:100.000000000\n",
      "[Iter.   10]  loss:68.268723  pct:-79.189280307\n",
      "[Iter.   20]  loss:61.776627  pct:-9.509619788\n",
      "[Iter.   30]  loss:57.905392  pct:-6.266504190\n",
      "[Iter.   40]  loss:54.861473  pct:-5.256710162\n",
      "[Iter.   50]  loss:52.209885  pct:-4.833243241\n",
      "[Iter.   60]  loss:49.846298  pct:-4.527086091\n",
      "[Iter.   70]  loss:47.715191  pct:-4.275357261\n",
      "[Iter.   80]  loss:45.777233  pct:-4.061511078\n",
      "[Iter.   90]  loss:44.002228  pct:-3.877484984\n",
      "[Iter.  100]  loss:42.366695  pct:-3.716930850\n",
      "[Iter.  110]  loss:40.851845  pct:-3.575569447\n",
      "[Iter.  120]  loss:39.442497  pct:-3.449899366\n",
      "[Iter.  130]  loss:38.126217  pct:-3.337213556\n",
      "[Iter.  140]  loss:36.892521  pct:-3.235820610\n",
      "[Iter.  150]  loss:35.732735  pct:-3.143689279\n",
      "[Iter.  160]  loss:34.639385  pct:-3.059797876\n",
      "[Iter.  170]  loss:33.606098  pct:-2.982983219\n",
      "[Iter.  180]  loss:32.627346  pct:-2.912424201\n",
      "[Iter.  190]  loss:31.698385  pct:-2.847184687\n",
      "[Iter.  200]  loss:30.815042  pct:-2.786712119\n",
      "[Iter.  210]  loss:29.973637  pct:-2.730503677\n",
      "[Iter.  220]  loss:29.170897  pct:-2.678153829\n",
      "[Iter.  230]  loss:28.403976  pct:-2.629059031\n",
      "[Iter.  240]  loss:27.670280  pct:-2.583074892\n",
      "[Iter.  250]  loss:26.967516  pct:-2.539780947\n",
      "[Iter.  260]  loss:26.293589  pct:-2.499033684\n",
      "[Iter.  270]  loss:25.646601  pct:-2.460629943\n",
      "[Iter.  280]  loss:25.024899  pct:-2.424111487\n",
      "[Iter.  290]  loss:24.426920  pct:-2.389534532\n",
      "[Iter.  300]  loss:23.851274  pct:-2.356602667\n",
      "[Iter.  310]  loss:23.296667  pct:-2.325273610\n",
      "[Iter.  320]  loss:22.761892  pct:-2.295499086\n",
      "[Iter.  330]  loss:22.245922  pct:-2.266816058\n",
      "[Iter.  340]  loss:21.747742  pct:-2.239423421\n",
      "[Iter.  350]  loss:21.266418  pct:-2.213210221\n",
      "[Iter.  360]  loss:20.801128  pct:-2.187909875\n",
      "[Iter.  370]  loss:20.351048  pct:-2.163732963\n",
      "[Iter.  380]  loss:19.915466  pct:-2.140338019\n",
      "[Iter.  390]  loss:19.493675  pct:-2.117907109\n",
      "[Iter.  400]  loss:19.085041  pct:-2.096239836\n",
      "[Iter.  410]  loss:18.688990  pct:-2.075192848\n",
      "[Iter.  420]  loss:18.304996  pct:-2.054649054\n",
      "[Iter.  430]  loss:17.932480  pct:-2.035054376\n",
      "[Iter.  440]  loss:17.570959  pct:-2.016011004\n",
      "[Iter.  450]  loss:17.219995  pct:-1.997407146\n",
      "[Iter.  460]  loss:16.879171  pct:-1.979234706\n",
      "[Iter.  470]  loss:16.548084  pct:-1.961512832\n",
      "[Iter.  480]  loss:16.226345  pct:-1.944268543\n",
      "[Iter.  490]  loss:15.913560  pct:-1.927637724\n",
      "[Iter.  500]  loss:15.609447  pct:-1.911033042\n",
      "[Iter.  510]  loss:15.313643  pct:-1.895032109\n",
      "[Iter.  520]  loss:15.025844  pct:-1.879362676\n",
      "[Iter.  530]  loss:14.745757  pct:-1.864031893\n",
      "[Iter.  540]  loss:14.473127  pct:-1.848869040\n",
      "[Iter.  550]  loss:14.207675  pct:-1.834105223\n",
      "[Iter.  560]  loss:13.949164  pct:-1.819513678\n",
      "[Iter.  570]  loss:13.697353  pct:-1.805205104\n",
      "[Iter.  580]  loss:13.452036  pct:-1.790984379\n",
      "[Iter.  590]  loss:13.212969  pct:-1.777181382\n",
      "[Iter.  600]  loss:12.979961  pct:-1.763475220\n",
      "[Iter.  610]  loss:12.752814  pct:-1.749982881\n",
      "[Iter.  620]  loss:12.531323  pct:-1.736799854\n",
      "[Iter.  630]  loss:12.315337  pct:-1.723570962\n",
      "[Iter.  640]  loss:12.104672  pct:-1.710588562\n",
      "[Iter.  650]  loss:11.899171  pct:-1.697704399\n",
      "[Iter.  660]  loss:11.698682  pct:-1.684899278\n",
      "[Iter.  670]  loss:11.503040  pct:-1.672338136\n",
      "[Iter.  680]  loss:11.312104  pct:-1.659874984\n",
      "[Iter.  690]  loss:11.125737  pct:-1.647501041\n",
      "[Iter.  700]  loss:10.943826  pct:-1.635050922\n",
      "[Iter.  710]  loss:10.766212  pct:-1.622962724\n",
      "[Iter.  720]  loss:10.592812  pct:-1.610593709\n",
      "[Iter.  730]  loss:10.423466  pct:-1.598686566\n",
      "[Iter.  740]  loss:10.258084  pct:-1.586626136\n",
      "[Iter.  750]  loss:10.096549  pct:-1.574711792\n",
      "[Iter.  760]  loss:9.938750  pct:-1.562898041\n",
      "[Iter.  770]  loss:9.784593  pct:-1.551076689\n",
      "[Iter.  780]  loss:9.633972  pct:-1.539363632\n",
      "[Iter.  790]  loss:9.486788  pct:-1.527764139\n",
      "[Iter.  800]  loss:9.342967  pct:-1.516011170\n",
      "[Iter.  810]  loss:9.202389  pct:-1.504642684\n",
      "[Iter.  820]  loss:9.065008  pct:-1.492879767\n",
      "[Iter.  830]  loss:8.930722  pct:-1.481365757\n",
      "[Iter.  840]  loss:8.799451  pct:-1.469885176\n",
      "[Iter.  850]  loss:8.671125  pct:-1.458334891\n",
      "[Iter.  860]  loss:8.545664  pct:-1.446889215\n",
      "[Iter.  870]  loss:8.422996  pct:-1.435444556\n",
      "[Iter.  880]  loss:8.303071  pct:-1.423775477\n",
      "[Iter.  890]  loss:8.185771  pct:-1.412730702\n",
      "[Iter.  900]  loss:8.071065  pct:-1.401285714\n",
      "[Iter.  910]  loss:7.958890  pct:-1.389841223\n",
      "[Iter.  920]  loss:7.849174  pct:-1.378527185\n",
      "[Iter.  930]  loss:7.741855  pct:-1.367275398\n",
      "[Iter.  940]  loss:7.636899  pct:-1.355697757\n",
      "[Iter.  950]  loss:7.534225  pct:-1.344440159\n",
      "[Iter.  960]  loss:7.433774  pct:-1.333262449\n",
      "[Iter.  970]  loss:7.335502  pct:-1.321964413\n",
      "[Iter.  980]  loss:7.239366  pct:-1.310565631\n",
      "[Iter.  990]  loss:7.145291  pct:-1.299488541\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.729938  pct:100.000000000\n",
      "[Iter.    2]  loss:2.729998  pct:0.002218304\n",
      "[Iter.    4]  loss:2.729997  pct:-0.000034933\n",
      "[Iter.    6]  loss:2.729997  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.729997\n",
      "Best loss: 2.729997 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [02:14<00:00, 74.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:285.238281  pct:100.000000000\n",
      "[Iter.   10]  loss:61.866894  pct:-78.310450718\n",
      "[Iter.   20]  loss:55.066326  pct:-10.992256460\n",
      "[Iter.   30]  loss:51.687778  pct:-6.135415063\n",
      "[Iter.   40]  loss:48.827995  pct:-5.532803415\n",
      "[Iter.   50]  loss:46.326950  pct:-5.122154231\n",
      "[Iter.   60]  loss:44.107121  pct:-4.791659187\n",
      "[Iter.   70]  loss:42.114929  pct:-4.516711342\n",
      "[Iter.   80]  loss:40.310802  pct:-4.283817577\n",
      "[Iter.   90]  loss:38.664528  pct:-4.083953844\n",
      "[Iter.  100]  loss:37.152592  pct:-3.910396092\n",
      "[Iter.  110]  loss:35.756363  pct:-3.758092575\n",
      "[Iter.  120]  loss:34.460808  pct:-3.623285505\n",
      "[Iter.  130]  loss:33.253712  pct:-3.502808486\n",
      "[Iter.  140]  loss:32.124886  pct:-3.394586901\n",
      "[Iter.  150]  loss:31.065886  pct:-3.296509845\n",
      "[Iter.  160]  loss:30.069500  pct:-3.207330346\n",
      "[Iter.  170]  loss:29.129610  pct:-3.125725099\n",
      "[Iter.  180]  loss:28.240921  pct:-3.050809947\n",
      "[Iter.  190]  loss:27.398897  pct:-2.981573614\n",
      "[Iter.  200]  loss:26.599508  pct:-2.917595115\n",
      "[Iter.  210]  loss:25.839291  pct:-2.858013985\n",
      "[Iter.  220]  loss:25.115150  pct:-2.802476964\n",
      "[Iter.  230]  loss:24.424324  pct:-2.750636184\n",
      "[Iter.  240]  loss:23.764399  pct:-2.701919037\n",
      "[Iter.  250]  loss:23.133200  pct:-2.656069250\n",
      "[Iter.  260]  loss:22.528769  pct:-2.612829874\n",
      "[Iter.  270]  loss:21.949362  pct:-2.571852684\n",
      "[Iter.  280]  loss:21.393358  pct:-2.533119530\n",
      "[Iter.  290]  loss:20.859312  pct:-2.496317630\n",
      "[Iter.  300]  loss:20.345900  pct:-2.461310681\n",
      "[Iter.  310]  loss:19.851904  pct:-2.427986359\n",
      "[Iter.  320]  loss:19.376236  pct:-2.396082288\n",
      "[Iter.  330]  loss:18.917910  pct:-2.365404409\n",
      "[Iter.  340]  loss:18.475988  pct:-2.335994002\n",
      "[Iter.  350]  loss:18.049580  pct:-2.307907749\n",
      "[Iter.  360]  loss:17.637907  pct:-2.280787702\n",
      "[Iter.  370]  loss:17.240252  pct:-2.254550307\n",
      "[Iter.  380]  loss:16.855907  pct:-2.229341608\n",
      "[Iter.  390]  loss:16.484241  pct:-2.204959632\n",
      "[Iter.  400]  loss:16.124662  pct:-2.181350514\n",
      "[Iter.  410]  loss:15.776605  pct:-2.158542847\n",
      "[Iter.  420]  loss:15.439582  pct:-2.136218716\n",
      "[Iter.  430]  loss:15.113102  pct:-2.114564465\n",
      "[Iter.  440]  loss:14.796713  pct:-2.093475481\n",
      "[Iter.  450]  loss:14.489977  pct:-2.073000909\n",
      "[Iter.  460]  loss:14.192474  pct:-2.053160748\n",
      "[Iter.  470]  loss:13.903905  pct:-2.033256802\n",
      "[Iter.  480]  loss:13.623833  pct:-2.014342114\n",
      "[Iter.  490]  loss:13.351963  pct:-1.995544612\n",
      "[Iter.  500]  loss:13.087982  pct:-1.977094039\n",
      "[Iter.  510]  loss:12.831563  pct:-1.959195683\n",
      "[Iter.  520]  loss:12.582445  pct:-1.941445881\n",
      "[Iter.  530]  loss:12.340363  pct:-1.923971001\n",
      "[Iter.  540]  loss:12.105044  pct:-1.906898464\n",
      "[Iter.  550]  loss:11.876250  pct:-1.890072362\n",
      "[Iter.  560]  loss:11.653767  pct:-1.873349163\n",
      "[Iter.  570]  loss:11.437350  pct:-1.857050735\n",
      "[Iter.  580]  loss:11.226808  pct:-1.840834405\n",
      "[Iter.  590]  loss:11.021947  pct:-1.824745686\n",
      "[Iter.  600]  loss:10.822566  pct:-1.808944248\n",
      "[Iter.  610]  loss:10.628485  pct:-1.793302123\n",
      "[Iter.  620]  loss:10.439531  pct:-1.777801865\n",
      "[Iter.  630]  loss:10.255538  pct:-1.762467431\n",
      "[Iter.  640]  loss:10.076366  pct:-1.747071313\n",
      "[Iter.  650]  loss:9.901841  pct:-1.732025748\n",
      "[Iter.  660]  loss:9.731820  pct:-1.717065082\n",
      "[Iter.  670]  loss:9.566176  pct:-1.702083374\n",
      "[Iter.  680]  loss:9.404785  pct:-1.687103094\n",
      "[Iter.  690]  loss:9.247481  pct:-1.672593340\n",
      "[Iter.  700]  loss:9.094159  pct:-1.657988961\n",
      "[Iter.  710]  loss:8.944720  pct:-1.643239974\n",
      "[Iter.  720]  loss:8.799033  pct:-1.628749686\n",
      "[Iter.  730]  loss:8.656975  pct:-1.614477066\n",
      "[Iter.  740]  loss:8.518473  pct:-1.599890543\n",
      "[Iter.  750]  loss:8.383402  pct:-1.585622282\n",
      "[Iter.  760]  loss:8.251659  pct:-1.571467997\n",
      "[Iter.  770]  loss:8.123155  pct:-1.557320134\n",
      "[Iter.  780]  loss:7.997834  pct:-1.542755741\n",
      "[Iter.  790]  loss:7.875557  pct:-1.528879653\n",
      "[Iter.  800]  loss:7.756258  pct:-1.514793924\n",
      "[Iter.  810]  loss:7.639859  pct:-1.500714402\n",
      "[Iter.  820]  loss:7.526274  pct:-1.486748239\n",
      "[Iter.  830]  loss:7.415459  pct:-1.472369667\n",
      "[Iter.  840]  loss:7.307287  pct:-1.458735563\n",
      "[Iter.  850]  loss:7.201723  pct:-1.444648214\n",
      "[Iter.  860]  loss:7.098690  pct:-1.430672510\n",
      "[Iter.  870]  loss:6.998108  pct:-1.416897701\n",
      "[Iter.  880]  loss:6.899937  pct:-1.402832102\n",
      "[Iter.  890]  loss:6.804107  pct:-1.388845889\n",
      "[Iter.  900]  loss:6.710548  pct:-1.375034015\n",
      "[Iter.  910]  loss:6.619215  pct:-1.361042106\n",
      "[Iter.  920]  loss:6.530039  pct:-1.347231927\n",
      "[Iter.  930]  loss:6.442971  pct:-1.333347061\n",
      "[Iter.  940]  loss:6.357950  pct:-1.319593434\n",
      "[Iter.  950]  loss:6.274916  pct:-1.305980154\n",
      "[Iter.  960]  loss:6.193856  pct:-1.291809014\n",
      "[Iter.  970]  loss:6.114675  pct:-1.278382824\n",
      "[Iter.  980]  loss:6.037371  pct:-1.264243197\n",
      "[Iter.  990]  loss:5.961867  pct:-1.250607758\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.717154  pct:100.000000000\n",
      "[Iter.    2]  loss:2.717200  pct:0.001719816\n",
      "[Iter.    4]  loss:2.717197  pct:-0.000122842\n",
      "[Iter.    6]  loss:2.717196  pct:-0.000043872\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.717196\n",
      "Best loss: 2.717196 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1678/10000 [00:25<02:06, 65.81it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:621.128906  pct:100.000000000\n",
      "[Iter.   10]  loss:103.227020  pct:-83.380736311\n",
      "[Iter.   20]  loss:98.861771  pct:-4.228785857\n",
      "[Iter.   30]  loss:94.881378  pct:-4.026220075\n",
      "[Iter.   40]  loss:91.202415  pct:-3.877433885\n",
      "[Iter.   50]  loss:87.941063  pct:-3.575949740\n",
      "[Iter.   60]  loss:84.994247  pct:-3.350898195\n",
      "[Iter.   70]  loss:82.281639  pct:-3.191519920\n",
      "[Iter.   80]  loss:79.764275  pct:-3.059448656\n",
      "[Iter.   90]  loss:77.417107  pct:-2.942630621\n",
      "[Iter.  100]  loss:75.219482  pct:-2.838680367\n",
      "[Iter.  110]  loss:73.154663  pct:-2.745059218\n",
      "[Iter.  120]  loss:71.208336  pct:-2.660564792\n",
      "[Iter.  130]  loss:69.368500  pct:-2.583736999\n",
      "[Iter.  140]  loss:67.624855  pct:-2.513597267\n",
      "[Iter.  150]  loss:65.968773  pct:-2.448925254\n",
      "[Iter.  160]  loss:64.392555  pct:-2.389339050\n",
      "[Iter.  170]  loss:62.889542  pct:-2.334141898\n",
      "[Iter.  180]  loss:61.453728  pct:-2.283072617\n",
      "[Iter.  190]  loss:60.079983  pct:-2.235413563\n",
      "[Iter.  200]  loss:58.763760  pct:-2.190784824\n",
      "[Iter.  210]  loss:57.500820  pct:-2.149180824\n",
      "[Iter.  220]  loss:56.287479  pct:-2.110127744\n",
      "[Iter.  230]  loss:55.120411  pct:-2.073406899\n",
      "[Iter.  240]  loss:53.996662  pct:-2.038716259\n",
      "[Iter.  250]  loss:52.913364  pct:-2.006230916\n",
      "[Iter.  260]  loss:51.868168  pct:-1.975297819\n",
      "[Iter.  270]  loss:50.858856  pct:-1.945917346\n",
      "[Iter.  280]  loss:49.883236  pct:-1.918289837\n",
      "[Iter.  290]  loss:48.939522  pct:-1.891846277\n",
      "[Iter.  300]  loss:48.025852  pct:-1.866936073\n",
      "[Iter.  310]  loss:47.140678  pct:-1.843119397\n",
      "[Iter.  320]  loss:46.282524  pct:-1.820411428\n",
      "[Iter.  330]  loss:45.450073  pct:-1.798628927\n",
      "[Iter.  340]  loss:44.642048  pct:-1.777830711\n",
      "[Iter.  350]  loss:43.857292  pct:-1.757884649\n",
      "[Iter.  360]  loss:43.094734  pct:-1.738725638\n",
      "[Iter.  370]  loss:42.353325  pct:-1.720417391\n",
      "[Iter.  380]  loss:41.632122  pct:-1.702824636\n",
      "[Iter.  390]  loss:40.930244  pct:-1.685903960\n",
      "[Iter.  400]  loss:40.246773  pct:-1.669845096\n",
      "[Iter.  410]  loss:39.581078  pct:-1.654033714\n",
      "[Iter.  420]  loss:38.932323  pct:-1.639051182\n",
      "[Iter.  430]  loss:38.299847  pct:-1.624554485\n",
      "[Iter.  440]  loss:37.683029  pct:-1.610495938\n",
      "[Iter.  450]  loss:37.081261  pct:-1.596921762\n",
      "[Iter.  460]  loss:36.493954  pct:-1.583837673\n",
      "[Iter.  470]  loss:35.920536  pct:-1.571267581\n",
      "[Iter.  480]  loss:35.360653  pct:-1.558671388\n",
      "[Iter.  490]  loss:34.813770  pct:-1.546585213\n",
      "[Iter.  500]  loss:34.279312  pct:-1.535191839\n",
      "[Iter.  510]  loss:33.756840  pct:-1.524162386\n",
      "[Iter.  520]  loss:33.246025  pct:-1.513218271\n",
      "[Iter.  530]  loss:32.746422  pct:-1.502745878\n",
      "[Iter.  540]  loss:32.257679  pct:-1.492507582\n",
      "[Iter.  550]  loss:31.779377  pct:-1.482753927\n",
      "[Iter.  560]  loss:31.311275  pct:-1.472972556\n",
      "[Iter.  570]  loss:30.853024  pct:-1.463536525\n",
      "[Iter.  580]  loss:30.404325  pct:-1.454308176\n",
      "[Iter.  590]  loss:29.964867  pct:-1.445382655\n",
      "[Iter.  600]  loss:29.534338  pct:-1.436778097\n",
      "[Iter.  610]  loss:29.112574  pct:-1.428047494\n",
      "[Iter.  620]  loss:28.699230  pct:-1.419810680\n",
      "[Iter.  630]  loss:28.294073  pct:-1.411735041\n",
      "[Iter.  640]  loss:27.896910  pct:-1.403698187\n",
      "[Iter.  650]  loss:27.507498  pct:-1.395896285\n",
      "[Iter.  660]  loss:27.125629  pct:-1.388233729\n",
      "[Iter.  670]  loss:26.751076  pct:-1.380811020\n",
      "[Iter.  680]  loss:26.383621  pct:-1.373606551\n",
      "[Iter.  690]  loss:26.023121  pct:-1.366379288\n",
      "[Iter.  700]  loss:25.669394  pct:-1.359281011\n",
      "[Iter.  710]  loss:25.322199  pct:-1.352562814\n",
      "[Iter.  720]  loss:24.981388  pct:-1.345897240\n",
      "[Iter.  730]  loss:24.646870  pct:-1.339070637\n",
      "[Iter.  740]  loss:24.318369  pct:-1.332829492\n",
      "[Iter.  750]  loss:23.995853  pct:-1.326221709\n",
      "[Iter.  760]  loss:23.679045  pct:-1.320264360\n",
      "[Iter.  770]  loss:23.367933  pct:-1.313868249\n",
      "[Iter.  780]  loss:23.062319  pct:-1.307836974\n",
      "[Iter.  790]  loss:22.762049  pct:-1.301994319\n",
      "[Iter.  800]  loss:22.467043  pct:-1.296042382\n",
      "[Iter.  810]  loss:22.177141  pct:-1.290342189\n",
      "[Iter.  820]  loss:21.892225  pct:-1.284727917\n",
      "[Iter.  830]  loss:21.612167  pct:-1.279257379\n",
      "[Iter.  840]  loss:21.336885  pct:-1.273735769\n",
      "[Iter.  850]  loss:21.066309  pct:-1.268116088\n",
      "[Iter.  860]  loss:20.800289  pct:-1.262773756\n",
      "[Iter.  870]  loss:20.538692  pct:-1.257658861\n",
      "[Iter.  880]  loss:20.281443  pct:-1.252513189\n",
      "[Iter.  890]  loss:20.028494  pct:-1.247193138\n",
      "[Iter.  900]  loss:19.779741  pct:-1.241993509\n",
      "[Iter.  910]  loss:19.535032  pct:-1.237169947\n",
      "[Iter.  920]  loss:19.294336  pct:-1.232124677\n",
      "[Iter.  930]  loss:19.057558  pct:-1.227190484\n",
      "[Iter.  940]  loss:18.824593  pct:-1.222430852\n",
      "[Iter.  950]  loss:18.595314  pct:-1.217973581\n",
      "[Iter.  960]  loss:18.369799  pct:-1.212753736\n",
      "[Iter.  970]  loss:18.147871  pct:-1.208111460\n",
      "[Iter.  980]  loss:17.929474  pct:-1.203431192\n",
      "[Iter.  990]  loss:17.714510  pct:-1.198941555\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.731333  pct:100.000000000\n",
      "[Iter.    2]  loss:2.731268  pct:-0.002409209\n",
      "[Iter.    4]  loss:2.731241  pct:-0.000977674\n",
      "[Iter.    6]  loss:2.731233  pct:-0.000279338\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.731233\n",
      "Best loss: 2.731233 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  9%|▉         | 925/10000 [00:15<02:27, 61.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:402.504456  pct:100.000000000\n",
      "[Iter.   10]  loss:83.282639  pct:-79.308886484\n",
      "[Iter.   20]  loss:77.555214  pct:-6.877093139\n",
      "[Iter.   30]  loss:73.299614  pct:-5.487187463\n",
      "[Iter.   40]  loss:69.636307  pct:-4.997716894\n",
      "[Iter.   50]  loss:66.415749  pct:-4.624826210\n",
      "[Iter.   60]  loss:63.545452  pct:-4.321710647\n",
      "[Iter.   70]  loss:60.959602  pct:-4.069291626\n",
      "[Iter.   80]  loss:58.609596  pct:-3.855022035\n",
      "[Iter.   90]  loss:56.458214  pct:-3.670699994\n",
      "[Iter.  100]  loss:54.476398  pct:-3.510233861\n",
      "[Iter.  110]  loss:52.641018  pct:-3.369129762\n",
      "[Iter.  120]  loss:50.933315  pct:-3.244053220\n",
      "[Iter.  130]  loss:49.337967  pct:-3.132229562\n",
      "[Iter.  140]  loss:47.842152  pct:-3.031773238\n",
      "[Iter.  150]  loss:46.435215  pct:-2.940788817\n",
      "[Iter.  160]  loss:45.108074  pct:-2.858048161\n",
      "[Iter.  170]  loss:43.852993  pct:-2.782386966\n",
      "[Iter.  180]  loss:42.663216  pct:-2.713104152\n",
      "[Iter.  190]  loss:41.533039  pct:-2.649065541\n",
      "[Iter.  200]  loss:40.457348  pct:-2.589965114\n",
      "[Iter.  210]  loss:39.431721  pct:-2.535082476\n",
      "[Iter.  220]  loss:38.452194  pct:-2.484107976\n",
      "[Iter.  230]  loss:37.515358  pct:-2.436366147\n",
      "[Iter.  240]  loss:36.618088  pct:-2.391741013\n",
      "[Iter.  250]  loss:35.757618  pct:-2.349849133\n",
      "[Iter.  260]  loss:34.931438  pct:-2.310499277\n",
      "[Iter.  270]  loss:34.137280  pct:-2.273479052\n",
      "[Iter.  280]  loss:33.373154  pct:-2.238391093\n",
      "[Iter.  290]  loss:32.637192  pct:-2.205251326\n",
      "[Iter.  300]  loss:31.927715  pct:-2.173828177\n",
      "[Iter.  310]  loss:31.243183  pct:-2.144006106\n",
      "[Iter.  320]  loss:30.582178  pct:-2.115677578\n",
      "[Iter.  330]  loss:29.943466  pct:-2.088510265\n",
      "[Iter.  340]  loss:29.325787  pct:-2.062819288\n",
      "[Iter.  350]  loss:28.728102  pct:-2.038086373\n",
      "[Iter.  360]  loss:28.149347  pct:-2.014593343\n",
      "[Iter.  370]  loss:27.588591  pct:-1.992077036\n",
      "[Iter.  380]  loss:27.045012  pct:-1.970303989\n",
      "[Iter.  390]  loss:26.517763  pct:-1.949521753\n",
      "[Iter.  400]  loss:26.006111  pct:-1.929468900\n",
      "[Iter.  410]  loss:25.509310  pct:-1.910325514\n",
      "[Iter.  420]  loss:25.026741  pct:-1.891735783\n",
      "[Iter.  430]  loss:24.557787  pct:-1.873812039\n",
      "[Iter.  440]  loss:24.101870  pct:-1.856508323\n",
      "[Iter.  450]  loss:23.658428  pct:-1.839863042\n",
      "[Iter.  460]  loss:23.227032  pct:-1.823436793\n",
      "[Iter.  470]  loss:22.807142  pct:-1.807761988\n",
      "[Iter.  480]  loss:22.398291  pct:-1.792647316\n",
      "[Iter.  490]  loss:22.000063  pct:-1.777937871\n",
      "[Iter.  500]  loss:21.612085  pct:-1.763529500\n",
      "[Iter.  510]  loss:21.233950  pct:-1.749649213\n",
      "[Iter.  520]  loss:20.865335  pct:-1.735966236\n",
      "[Iter.  530]  loss:20.505854  pct:-1.722866197\n",
      "[Iter.  540]  loss:20.155170  pct:-1.710161490\n",
      "[Iter.  550]  loss:19.813034  pct:-1.697511733\n",
      "[Iter.  560]  loss:19.479115  pct:-1.685352804\n",
      "[Iter.  570]  loss:19.153168  pct:-1.673314294\n",
      "[Iter.  580]  loss:18.834927  pct:-1.661558672\n",
      "[Iter.  590]  loss:18.524134  pct:-1.650088315\n",
      "[Iter.  600]  loss:18.220541  pct:-1.638903536\n",
      "[Iter.  610]  loss:17.923908  pct:-1.628012948\n",
      "[Iter.  620]  loss:17.634068  pct:-1.617061940\n",
      "[Iter.  630]  loss:17.350763  pct:-1.606573265\n",
      "[Iter.  640]  loss:17.073801  pct:-1.596254154\n",
      "[Iter.  650]  loss:16.803001  pct:-1.586053605\n",
      "[Iter.  660]  loss:16.538218  pct:-1.575812873\n",
      "[Iter.  670]  loss:16.279198  pct:-1.566189651\n",
      "[Iter.  680]  loss:16.025816  pct:-1.556475533\n",
      "[Iter.  690]  loss:15.777934  pct:-1.546766105\n",
      "[Iter.  700]  loss:15.535385  pct:-1.537266802\n",
      "[Iter.  710]  loss:15.297999  pct:-1.528032603\n",
      "[Iter.  720]  loss:15.065632  pct:-1.518940547\n",
      "[Iter.  730]  loss:14.838151  pct:-1.509932609\n",
      "[Iter.  740]  loss:14.615456  pct:-1.500829524\n",
      "[Iter.  750]  loss:14.397370  pct:-1.492155254\n",
      "[Iter.  760]  loss:14.183811  pct:-1.483320535\n",
      "[Iter.  770]  loss:13.974631  pct:-1.474779067\n",
      "[Iter.  780]  loss:13.769739  pct:-1.466172194\n",
      "[Iter.  790]  loss:13.569018  pct:-1.457694912\n",
      "[Iter.  800]  loss:13.372382  pct:-1.449155677\n",
      "[Iter.  810]  loss:13.179670  pct:-1.441118178\n",
      "[Iter.  820]  loss:12.990861  pct:-1.432580558\n",
      "[Iter.  830]  loss:12.805834  pct:-1.424286838\n",
      "[Iter.  840]  loss:12.624457  pct:-1.416358043\n",
      "[Iter.  850]  loss:12.446670  pct:-1.408280576\n",
      "[Iter.  860]  loss:12.272397  pct:-1.400153962\n",
      "[Iter.  870]  loss:12.101546  pct:-1.392154713\n",
      "[Iter.  880]  loss:11.934034  pct:-1.384219306\n",
      "[Iter.  890]  loss:11.769770  pct:-1.376438798\n",
      "[Iter.  900]  loss:11.608704  pct:-1.368472450\n",
      "[Iter.  910]  loss:11.450713  pct:-1.360965538\n",
      "[Iter.  920]  loss:11.295777  pct:-1.353067138\n",
      "[Iter.  930]  loss:11.143807  pct:-1.345369206\n",
      "[Iter.  940]  loss:10.994754  pct:-1.337546209\n",
      "[Iter.  950]  loss:10.848523  pct:-1.330004289\n",
      "[Iter.  960]  loss:10.705075  pct:-1.322280223\n",
      "[Iter.  970]  loss:10.564348  pct:-1.314582473\n",
      "[Iter.  980]  loss:10.426259  pct:-1.307124463\n",
      "[Iter.  990]  loss:10.290801  pct:-1.299200337\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.745679  pct:100.000000000\n",
      "[Iter.    2]  loss:2.745702  pct:0.000824924\n",
      "[Iter.    4]  loss:2.745699  pct:-0.000086833\n",
      "[Iter.    6]  loss:2.745699  pct:-0.000017367\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.745699\n",
      "Best loss: 2.745699 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 21%|██        | 2063/10000 [00:37<02:25, 54.63it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:638.273987  pct:100.000000000\n",
      "[Iter.   10]  loss:103.758293  pct:-83.743924565\n",
      "[Iter.   20]  loss:98.840317  pct:-4.739839323\n",
      "[Iter.   30]  loss:93.107559  pct:-5.800019421\n",
      "[Iter.   40]  loss:89.770073  pct:-3.584549198\n",
      "[Iter.   50]  loss:86.891594  pct:-3.206501799\n",
      "[Iter.   60]  loss:84.238350  pct:-3.053510585\n",
      "[Iter.   70]  loss:81.769554  pct:-2.930726657\n",
      "[Iter.   80]  loss:79.461876  pct:-2.822172931\n",
      "[Iter.   90]  loss:77.296036  pct:-2.725634304\n",
      "[Iter.  100]  loss:75.256096  pct:-2.639126134\n",
      "[Iter.  110]  loss:73.329727  pct:-2.559751062\n",
      "[Iter.  120]  loss:71.505859  pct:-2.487214760\n",
      "[Iter.  130]  loss:69.774544  pct:-2.421222020\n",
      "[Iter.  140]  loss:68.127525  pct:-2.360486137\n",
      "[Iter.  150]  loss:66.557571  pct:-2.304434090\n",
      "[Iter.  160]  loss:65.058441  pct:-2.252381235\n",
      "[Iter.  170]  loss:63.624577  pct:-2.203963956\n",
      "[Iter.  180]  loss:62.251064  pct:-2.158776281\n",
      "[Iter.  190]  loss:60.933514  pct:-2.116511057\n",
      "[Iter.  200]  loss:59.667973  pct:-2.076921223\n",
      "[Iter.  210]  loss:58.450874  pct:-2.039784802\n",
      "[Iter.  220]  loss:57.279007  pct:-2.004875691\n",
      "[Iter.  230]  loss:56.149567  pct:-1.971822431\n",
      "[Iter.  240]  loss:55.059925  pct:-1.940605486\n",
      "[Iter.  250]  loss:54.007679  pct:-1.911092491\n",
      "[Iter.  260]  loss:52.990631  pct:-1.883154213\n",
      "[Iter.  270]  loss:52.006802  pct:-1.856610268\n",
      "[Iter.  280]  loss:51.054386  pct:-1.831328666\n",
      "[Iter.  290]  loss:50.131683  pct:-1.807293867\n",
      "[Iter.  300]  loss:49.237160  pct:-1.784347863\n",
      "[Iter.  310]  loss:48.369335  pct:-1.762539836\n",
      "[Iter.  320]  loss:47.526924  pct:-1.741622121\n",
      "[Iter.  330]  loss:46.708702  pct:-1.721596886\n",
      "[Iter.  340]  loss:45.913475  pct:-1.702524402\n",
      "[Iter.  350]  loss:45.140285  pct:-1.684014429\n",
      "[Iter.  360]  loss:44.388058  pct:-1.666422299\n",
      "[Iter.  370]  loss:43.655895  pct:-1.649458240\n",
      "[Iter.  380]  loss:42.942924  pct:-1.633160264\n",
      "[Iter.  390]  loss:42.248268  pct:-1.617627072\n",
      "[Iter.  400]  loss:41.571239  pct:-1.602500377\n",
      "[Iter.  410]  loss:40.911152  pct:-1.587846775\n",
      "[Iter.  420]  loss:40.267269  pct:-1.573856325\n",
      "[Iter.  430]  loss:39.639011  pct:-1.560219416\n",
      "[Iter.  440]  loss:39.025703  pct:-1.547233222\n",
      "[Iter.  450]  loss:38.426807  pct:-1.534619428\n",
      "[Iter.  460]  loss:37.841835  pct:-1.522302843\n",
      "[Iter.  470]  loss:37.270264  pct:-1.510421864\n",
      "[Iter.  480]  loss:36.711639  pct:-1.498847104\n",
      "[Iter.  490]  loss:36.165474  pct:-1.487717452\n",
      "[Iter.  500]  loss:35.631306  pct:-1.477011595\n",
      "[Iter.  510]  loss:35.108753  pct:-1.466554425\n",
      "[Iter.  520]  loss:34.597473  pct:-1.456275183\n",
      "[Iter.  530]  loss:34.097054  pct:-1.446405102\n",
      "[Iter.  540]  loss:33.607124  pct:-1.436866675\n",
      "[Iter.  550]  loss:33.127403  pct:-1.427438613\n",
      "[Iter.  560]  loss:32.657536  pct:-1.418365643\n",
      "[Iter.  570]  loss:32.197254  pct:-1.409418575\n",
      "[Iter.  580]  loss:31.746216  pct:-1.400859707\n",
      "[Iter.  590]  loss:31.304218  pct:-1.392284141\n",
      "[Iter.  600]  loss:30.870937  pct:-1.384097634\n",
      "[Iter.  610]  loss:30.446135  pct:-1.376060517\n",
      "[Iter.  620]  loss:30.029577  pct:-1.368177990\n",
      "[Iter.  630]  loss:29.621040  pct:-1.360448426\n",
      "[Iter.  640]  loss:29.220299  pct:-1.352895011\n",
      "[Iter.  650]  loss:28.827089  pct:-1.345672269\n",
      "[Iter.  660]  loss:28.441236  pct:-1.338507713\n",
      "[Iter.  670]  loss:28.062551  pct:-1.331464606\n",
      "[Iter.  680]  loss:27.690842  pct:-1.324576005\n",
      "[Iter.  690]  loss:27.325869  pct:-1.318028078\n",
      "[Iter.  700]  loss:26.967491  pct:-1.311495206\n",
      "[Iter.  710]  loss:26.615576  pct:-1.304961435\n",
      "[Iter.  720]  loss:26.269913  pct:-1.298724752\n",
      "[Iter.  730]  loss:25.930376  pct:-1.292492558\n",
      "[Iter.  740]  loss:25.596827  pct:-1.286327274\n",
      "[Iter.  750]  loss:25.269102  pct:-1.280332373\n",
      "[Iter.  760]  loss:24.947056  pct:-1.274466654\n",
      "[Iter.  770]  loss:24.630577  pct:-1.268601520\n",
      "[Iter.  780]  loss:24.319458  pct:-1.263141657\n",
      "[Iter.  790]  loss:24.013670  pct:-1.257380161\n",
      "[Iter.  800]  loss:23.713041  pct:-1.251906362\n",
      "[Iter.  810]  loss:23.417477  pct:-1.246422370\n",
      "[Iter.  820]  loss:23.126818  pct:-1.241205255\n",
      "[Iter.  830]  loss:22.841017  pct:-1.235798792\n",
      "[Iter.  840]  loss:22.559959  pct:-1.230494074\n",
      "[Iter.  850]  loss:22.283512  pct:-1.225389156\n",
      "[Iter.  860]  loss:22.011597  pct:-1.220253946\n",
      "[Iter.  870]  loss:21.744081  pct:-1.215337471\n",
      "[Iter.  880]  loss:21.480860  pct:-1.210544307\n",
      "[Iter.  890]  loss:21.221878  pct:-1.205639382\n",
      "[Iter.  900]  loss:20.967033  pct:-1.200858213\n",
      "[Iter.  910]  loss:20.716227  pct:-1.196195970\n",
      "[Iter.  920]  loss:20.469414  pct:-1.191398537\n",
      "[Iter.  930]  loss:20.226482  pct:-1.186801776\n",
      "[Iter.  940]  loss:19.987329  pct:-1.182375184\n",
      "[Iter.  950]  loss:19.751905  pct:-1.177866418\n",
      "[Iter.  960]  loss:19.520134  pct:-1.173413217\n",
      "[Iter.  970]  loss:19.291941  pct:-1.169014943\n",
      "[Iter.  980]  loss:19.067265  pct:-1.164611357\n",
      "[Iter.  990]  loss:18.846043  pct:-1.160218463\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.778517  pct:100.000000000\n",
      "[Iter.    2]  loss:2.778533  pct:0.000600655\n",
      "[Iter.    4]  loss:2.778525  pct:-0.000291745\n",
      "[Iter.    6]  loss:2.778523  pct:-0.000077227\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.778523\n",
      "Best loss: 2.778523 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  6%|▋         | 642/10000 [00:09<02:21, 66.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:996.061768  pct:100.000000000\n",
      "[Iter.   10]  loss:166.425308  pct:-83.291669414\n",
      "[Iter.   20]  loss:158.490158  pct:-4.767994863\n",
      "[Iter.   30]  loss:153.396713  pct:-3.213729411\n",
      "[Iter.   40]  loss:148.976425  pct:-2.881605474\n",
      "[Iter.   50]  loss:144.959732  pct:-2.696193784\n",
      "[Iter.   60]  loss:141.249710  pct:-2.559346599\n",
      "[Iter.   70]  loss:137.782013  pct:-2.455011866\n",
      "[Iter.   80]  loss:134.509338  pct:-2.375255297\n",
      "[Iter.   90]  loss:131.437485  pct:-2.283747489\n",
      "[Iter.  100]  loss:128.528229  pct:-2.213414223\n",
      "[Iter.  110]  loss:125.766403  pct:-2.148808544\n",
      "[Iter.  120]  loss:123.138039  pct:-2.089878136\n",
      "[Iter.  130]  loss:120.637032  pct:-2.031059702\n",
      "[Iter.  140]  loss:118.248589  pct:-1.979858889\n",
      "[Iter.  150]  loss:115.965813  pct:-1.930488902\n",
      "[Iter.  160]  loss:113.778946  pct:-1.885785741\n",
      "[Iter.  170]  loss:111.676331  pct:-1.847982805\n",
      "[Iter.  180]  loss:109.657089  pct:-1.808119342\n",
      "[Iter.  190]  loss:107.715370  pct:-1.770719129\n",
      "[Iter.  200]  loss:105.845161  pct:-1.736250581\n",
      "[Iter.  210]  loss:104.041771  pct:-1.703800607\n",
      "[Iter.  220]  loss:102.300842  pct:-1.673297786\n",
      "[Iter.  230]  loss:100.618546  pct:-1.644460315\n",
      "[Iter.  240]  loss:98.991280  pct:-1.617262426\n",
      "[Iter.  250]  loss:97.415833  pct:-1.591500876\n",
      "[Iter.  260]  loss:95.889343  pct:-1.566982716\n",
      "[Iter.  270]  loss:94.408989  pct:-1.543815255\n",
      "[Iter.  280]  loss:92.972458  pct:-1.521604121\n",
      "[Iter.  290]  loss:91.577423  pct:-1.500481779\n",
      "[Iter.  300]  loss:90.221733  pct:-1.480375792\n",
      "[Iter.  310]  loss:88.903542  pct:-1.461057644\n",
      "[Iter.  320]  loss:87.621033  pct:-1.442584657\n",
      "[Iter.  330]  loss:86.372383  pct:-1.425056928\n",
      "[Iter.  340]  loss:85.156174  pct:-1.408099867\n",
      "[Iter.  350]  loss:83.970840  pct:-1.391952222\n",
      "[Iter.  360]  loss:82.815094  pct:-1.376366431\n",
      "[Iter.  370]  loss:81.687653  pct:-1.361396035\n",
      "[Iter.  380]  loss:80.587387  pct:-1.346917763\n",
      "[Iter.  390]  loss:79.513153  pct:-1.333005136\n",
      "[Iter.  400]  loss:78.463837  pct:-1.319676513\n",
      "[Iter.  410]  loss:77.438454  pct:-1.306822403\n",
      "[Iter.  420]  loss:76.436119  pct:-1.294362874\n",
      "[Iter.  430]  loss:75.455956  pct:-1.282330377\n",
      "[Iter.  440]  loss:74.497101  pct:-1.270747509\n",
      "[Iter.  450]  loss:73.558800  pct:-1.259513560\n",
      "[Iter.  460]  loss:72.640289  pct:-1.248675128\n",
      "[Iter.  470]  loss:71.740891  pct:-1.238154215\n",
      "[Iter.  480]  loss:70.859917  pct:-1.227993979\n",
      "[Iter.  490]  loss:69.996689  pct:-1.218217413\n",
      "[Iter.  500]  loss:69.150620  pct:-1.208727655\n",
      "[Iter.  510]  loss:68.321274  pct:-1.199332282\n",
      "[Iter.  520]  loss:67.507942  pct:-1.190451464\n",
      "[Iter.  530]  loss:66.710152  pct:-1.181772842\n",
      "[Iter.  540]  loss:65.927505  pct:-1.173204017\n",
      "[Iter.  550]  loss:65.159500  pct:-1.164924056\n",
      "[Iter.  560]  loss:64.405609  pct:-1.156993209\n",
      "[Iter.  570]  loss:63.665436  pct:-1.149237388\n",
      "[Iter.  580]  loss:62.938583  pct:-1.141675083\n",
      "[Iter.  590]  loss:62.224728  pct:-1.134210059\n",
      "[Iter.  600]  loss:61.523457  pct:-1.126997391\n",
      "[Iter.  610]  loss:60.834366  pct:-1.120045536\n",
      "[Iter.  620]  loss:60.157177  pct:-1.113168295\n",
      "[Iter.  630]  loss:59.491478  pct:-1.106599476\n",
      "[Iter.  640]  loss:58.836975  pct:-1.100162395\n",
      "[Iter.  650]  loss:58.193409  pct:-1.093812404\n",
      "[Iter.  660]  loss:57.560486  pct:-1.087619951\n",
      "[Iter.  670]  loss:56.937855  pct:-1.081698780\n",
      "[Iter.  680]  loss:56.325264  pct:-1.075893696\n",
      "[Iter.  690]  loss:55.722519  pct:-1.070114925\n",
      "[Iter.  700]  loss:55.129375  pct:-1.064459171\n",
      "[Iter.  710]  loss:54.545563  pct:-1.058986627\n",
      "[Iter.  720]  loss:53.970871  pct:-1.053599493\n",
      "[Iter.  730]  loss:53.405056  pct:-1.048371023\n",
      "[Iter.  740]  loss:52.847919  pct:-1.043228071\n",
      "[Iter.  750]  loss:52.299191  pct:-1.038317021\n",
      "[Iter.  760]  loss:51.758747  pct:-1.033368614\n",
      "[Iter.  770]  loss:51.226387  pct:-1.028541274\n",
      "[Iter.  780]  loss:50.701942  pct:-1.023778194\n",
      "[Iter.  790]  loss:50.185062  pct:-1.019448192\n",
      "[Iter.  800]  loss:49.675850  pct:-1.014669444\n",
      "[Iter.  810]  loss:49.174019  pct:-1.010211311\n",
      "[Iter.  820]  loss:48.679375  pct:-1.005905510\n",
      "[Iter.  830]  loss:48.191700  pct:-1.001809732\n",
      "[Iter.  840]  loss:47.710957  pct:-0.997564743\n",
      "[Iter.  850]  loss:47.236950  pct:-0.993496435\n",
      "[Iter.  860]  loss:46.769550  pct:-0.989478783\n",
      "[Iter.  870]  loss:46.308571  pct:-0.985640141\n",
      "[Iter.  880]  loss:45.853981  pct:-0.981653796\n",
      "[Iter.  890]  loss:45.405590  pct:-0.977867026\n",
      "[Iter.  900]  loss:44.963257  pct:-0.974182300\n",
      "[Iter.  910]  loss:44.526844  pct:-0.970598755\n",
      "[Iter.  920]  loss:44.096294  pct:-0.966943944\n",
      "[Iter.  930]  loss:43.671478  pct:-0.963382836\n",
      "[Iter.  940]  loss:43.252235  pct:-0.959992369\n",
      "[Iter.  950]  loss:42.838486  pct:-0.956597251\n",
      "[Iter.  960]  loss:42.430134  pct:-0.953236071\n",
      "[Iter.  970]  loss:42.027081  pct:-0.949922254\n",
      "[Iter.  980]  loss:41.629196  pct:-0.946733306\n",
      "[Iter.  990]  loss:41.236492  pct:-0.943337960\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.758664  pct:100.000000000\n",
      "[Iter.    2]  loss:2.758670  pct:0.000233349\n",
      "[Iter.    4]  loss:2.758665  pct:-0.000190135\n",
      "[Iter.    6]  loss:2.758663  pct:-0.000069140\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.758663\n",
      "Best loss: 2.758663 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 23%|██▎       | 2300/10000 [00:35<01:57, 65.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:465.897156  pct:100.000000000\n",
      "[Iter.   10]  loss:77.999336  pct:-83.258251816\n",
      "[Iter.   20]  loss:74.208252  pct:-4.860405834\n",
      "[Iter.   30]  loss:70.965576  pct:-4.369697030\n",
      "[Iter.   40]  loss:68.032890  pct:-4.132547089\n",
      "[Iter.   50]  loss:65.414314  pct:-3.848985450\n",
      "[Iter.   60]  loss:63.033287  pct:-3.639917728\n",
      "[Iter.   70]  loss:60.845383  pct:-3.471030086\n",
      "[Iter.   80]  loss:58.821041  pct:-3.327025805\n",
      "[Iter.   90]  loss:56.937977  pct:-3.201344680\n",
      "[Iter.  100]  loss:55.178486  pct:-3.090188771\n",
      "[Iter.  110]  loss:53.528255  pct:-2.990713467\n",
      "[Iter.  120]  loss:51.975582  pct:-2.900661205\n",
      "[Iter.  130]  loss:50.510494  pct:-2.818800350\n",
      "[Iter.  140]  loss:49.124454  pct:-2.744062902\n",
      "[Iter.  150]  loss:47.810150  pct:-2.675458415\n",
      "[Iter.  160]  loss:46.561249  pct:-2.612209674\n",
      "[Iter.  170]  loss:45.372192  pct:-2.553746791\n",
      "[Iter.  180]  loss:44.238159  pct:-2.499401381\n",
      "[Iter.  190]  loss:43.154804  pct:-2.448915077\n",
      "[Iter.  200]  loss:42.118343  pct:-2.401727675\n",
      "[Iter.  210]  loss:41.125359  pct:-2.357606431\n",
      "[Iter.  220]  loss:40.172783  pct:-2.316273259\n",
      "[Iter.  230]  loss:39.257912  pct:-2.277340901\n",
      "[Iter.  240]  loss:38.378277  pct:-2.240656264\n",
      "[Iter.  250]  loss:37.531647  pct:-2.206013835\n",
      "[Iter.  260]  loss:36.715935  pct:-2.173397775\n",
      "[Iter.  270]  loss:35.929371  pct:-2.142295651\n",
      "[Iter.  280]  loss:35.170235  pct:-2.112856923\n",
      "[Iter.  290]  loss:34.436996  pct:-2.084826066\n",
      "[Iter.  300]  loss:33.728199  pct:-2.058244120\n",
      "[Iter.  310]  loss:33.042576  pct:-2.032789147\n",
      "[Iter.  320]  loss:32.378925  pct:-2.008470877\n",
      "[Iter.  330]  loss:31.736082  pct:-1.985375487\n",
      "[Iter.  340]  loss:31.113039  pct:-1.963200936\n",
      "[Iter.  350]  loss:30.508831  pct:-1.941976778\n",
      "[Iter.  360]  loss:29.922585  pct:-1.921563268\n",
      "[Iter.  370]  loss:29.353514  pct:-1.901810371\n",
      "[Iter.  380]  loss:28.800779  pct:-1.883026272\n",
      "[Iter.  390]  loss:28.263700  pct:-1.864806681\n",
      "[Iter.  400]  loss:27.741608  pct:-1.847220322\n",
      "[Iter.  410]  loss:27.233833  pct:-1.830371041\n",
      "[Iter.  420]  loss:26.739822  pct:-1.813960303\n",
      "[Iter.  430]  loss:26.258945  pct:-1.798354962\n",
      "[Iter.  440]  loss:25.790731  pct:-1.783064882\n",
      "[Iter.  450]  loss:25.334671  pct:-1.768311266\n",
      "[Iter.  460]  loss:24.890341  pct:-1.753842452\n",
      "[Iter.  470]  loss:24.457283  pct:-1.739862818\n",
      "[Iter.  480]  loss:24.035028  pct:-1.726498246\n",
      "[Iter.  490]  loss:23.623222  pct:-1.713358099\n",
      "[Iter.  500]  loss:23.221472  pct:-1.700659455\n",
      "[Iter.  510]  loss:22.829414  pct:-1.688340095\n",
      "[Iter.  520]  loss:22.446737  pct:-1.676245707\n",
      "[Iter.  530]  loss:22.073111  pct:-1.664503416\n",
      "[Iter.  540]  loss:21.708256  pct:-1.652937910\n",
      "[Iter.  550]  loss:21.351866  pct:-1.641725633\n",
      "[Iter.  560]  loss:21.003666  pct:-1.630770108\n",
      "[Iter.  570]  loss:20.663397  pct:-1.620046186\n",
      "[Iter.  580]  loss:20.330818  pct:-1.609506228\n",
      "[Iter.  590]  loss:20.005671  pct:-1.599284524\n",
      "[Iter.  600]  loss:19.687727  pct:-1.589267264\n",
      "[Iter.  610]  loss:19.376818  pct:-1.579203489\n",
      "[Iter.  620]  loss:19.072653  pct:-1.569736017\n",
      "[Iter.  630]  loss:18.775097  pct:-1.560118177\n",
      "[Iter.  640]  loss:18.483923  pct:-1.550851836\n",
      "[Iter.  650]  loss:18.198969  pct:-1.541631999\n",
      "[Iter.  660]  loss:17.920008  pct:-1.532840588\n",
      "[Iter.  670]  loss:17.646927  pct:-1.523887882\n",
      "[Iter.  680]  loss:17.379524  pct:-1.515293007\n",
      "[Iter.  690]  loss:17.117685  pct:-1.506594251\n",
      "[Iter.  700]  loss:16.861238  pct:-1.498139694\n",
      "[Iter.  710]  loss:16.610031  pct:-1.489851128\n",
      "[Iter.  720]  loss:16.363943  pct:-1.481562714\n",
      "[Iter.  730]  loss:16.122839  pct:-1.473386485\n",
      "[Iter.  740]  loss:15.886580  pct:-1.465365418\n",
      "[Iter.  750]  loss:15.655042  pct:-1.457448776\n",
      "[Iter.  760]  loss:15.428117  pct:-1.449532366\n",
      "[Iter.  770]  loss:15.205671  pct:-1.441818797\n",
      "[Iter.  780]  loss:14.987570  pct:-1.434343128\n",
      "[Iter.  790]  loss:14.773777  pct:-1.426467424\n",
      "[Iter.  800]  loss:14.564139  pct:-1.418984744\n",
      "[Iter.  810]  loss:14.358547  pct:-1.411632712\n",
      "[Iter.  820]  loss:14.156937  pct:-1.404115348\n",
      "[Iter.  830]  loss:13.959176  pct:-1.396916486\n",
      "[Iter.  840]  loss:13.765203  pct:-1.389577296\n",
      "[Iter.  850]  loss:13.574932  pct:-1.382256626\n",
      "[Iter.  860]  loss:13.388275  pct:-1.375012048\n",
      "[Iter.  870]  loss:13.205118  pct:-1.368040059\n",
      "[Iter.  880]  loss:13.025414  pct:-1.360871320\n",
      "[Iter.  890]  loss:12.849058  pct:-1.353932923\n",
      "[Iter.  900]  loss:12.676019  pct:-1.346709107\n",
      "[Iter.  910]  loss:12.506205  pct:-1.339648620\n",
      "[Iter.  920]  loss:12.339534  pct:-1.332704881\n",
      "[Iter.  930]  loss:12.175925  pct:-1.325889240\n",
      "[Iter.  940]  loss:12.015312  pct:-1.319103531\n",
      "[Iter.  950]  loss:11.857638  pct:-1.312274148\n",
      "[Iter.  960]  loss:11.702815  pct:-1.305684138\n",
      "[Iter.  970]  loss:11.550837  pct:-1.298649017\n",
      "[Iter.  980]  loss:11.401608  pct:-1.291924574\n",
      "[Iter.  990]  loss:11.255063  pct:-1.285304706\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.736076  pct:100.000000000\n",
      "[Iter.    2]  loss:2.736118  pct:0.001551072\n",
      "[Iter.    4]  loss:2.736110  pct:-0.000296268\n",
      "[Iter.    6]  loss:2.736105  pct:-0.000174276\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.736105\n",
      "Best loss: 2.736105 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 260/10000 [00:03<02:05, 77.77it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3981.084473  pct:100.000000000\n",
      "[Iter.   10]  loss:620.956726  pct:-84.402320740\n",
      "[Iter.   20]  loss:608.441467  pct:-2.015480027\n",
      "[Iter.   30]  loss:600.297180  pct:-1.338548989\n",
      "[Iter.   40]  loss:592.528931  pct:-1.294067300\n",
      "[Iter.   50]  loss:585.227661  pct:-1.232221610\n",
      "[Iter.   60]  loss:578.380371  pct:-1.170021599\n",
      "[Iter.   70]  loss:571.876831  pct:-1.124439964\n",
      "[Iter.   80]  loss:565.614441  pct:-1.095059250\n",
      "[Iter.   90]  loss:559.555176  pct:-1.071271293\n",
      "[Iter.  100]  loss:553.676208  pct:-1.050650148\n",
      "[Iter.  110]  loss:547.965210  pct:-1.031469015\n",
      "[Iter.  120]  loss:542.413452  pct:-1.013158812\n",
      "[Iter.  130]  loss:537.013000  pct:-0.995633799\n",
      "[Iter.  140]  loss:531.756836  pct:-0.978777897\n",
      "[Iter.  150]  loss:526.637939  pct:-0.962638586\n",
      "[Iter.  160]  loss:521.650146  pct:-0.947100958\n",
      "[Iter.  170]  loss:516.786926  pct:-0.932276210\n",
      "[Iter.  180]  loss:512.042664  pct:-0.918030711\n",
      "[Iter.  190]  loss:507.412415  pct:-0.904270162\n",
      "[Iter.  200]  loss:502.890503  pct:-0.891170868\n",
      "[Iter.  210]  loss:498.472687  pct:-0.878484707\n",
      "[Iter.  220]  loss:494.154480  pct:-0.866287542\n",
      "[Iter.  230]  loss:489.931885  pct:-0.854509144\n",
      "[Iter.  240]  loss:485.800873  pct:-0.843180877\n",
      "[Iter.  250]  loss:481.757812  pct:-0.832246406\n",
      "[Iter.  260]  loss:477.799225  pct:-0.821696617\n",
      "[Iter.  270]  loss:473.921906  pct:-0.811495527\n",
      "[Iter.  280]  loss:470.122742  pct:-0.801643430\n",
      "[Iter.  290]  loss:466.399109  pct:-0.792055453\n",
      "[Iter.  300]  loss:462.748108  pct:-0.782806165\n",
      "[Iter.  310]  loss:459.166809  pct:-0.773919713\n",
      "[Iter.  320]  loss:455.653229  pct:-0.765207818\n",
      "[Iter.  330]  loss:452.204803  pct:-0.756809142\n",
      "[Iter.  340]  loss:448.819550  pct:-0.748610780\n",
      "[Iter.  350]  loss:445.494995  pct:-0.740732984\n",
      "[Iter.  360]  loss:442.229431  pct:-0.733019226\n",
      "[Iter.  370]  loss:439.020844  pct:-0.725548193\n",
      "[Iter.  380]  loss:435.867462  pct:-0.718275999\n",
      "[Iter.  390]  loss:432.767212  pct:-0.711282790\n",
      "[Iter.  400]  loss:429.718903  pct:-0.704376219\n",
      "[Iter.  410]  loss:426.720642  pct:-0.697725997\n",
      "[Iter.  420]  loss:423.771210  pct:-0.691185774\n",
      "[Iter.  430]  loss:420.869080  pct:-0.684834189\n",
      "[Iter.  440]  loss:418.012634  pct:-0.678701632\n",
      "[Iter.  450]  loss:415.200653  pct:-0.672702443\n",
      "[Iter.  460]  loss:412.431885  pct:-0.666850664\n",
      "[Iter.  470]  loss:409.705078  pct:-0.661153209\n",
      "[Iter.  480]  loss:407.019226  pct:-0.655557423\n",
      "[Iter.  490]  loss:404.373138  pct:-0.650113674\n",
      "[Iter.  500]  loss:401.765503  pct:-0.644858733\n",
      "[Iter.  510]  loss:399.195709  pct:-0.639625275\n",
      "[Iter.  520]  loss:396.662354  pct:-0.634614966\n",
      "[Iter.  530]  loss:394.164856  pct:-0.629628080\n",
      "[Iter.  540]  loss:391.702026  pct:-0.624822217\n",
      "[Iter.  550]  loss:389.272949  pct:-0.620133924\n",
      "[Iter.  560]  loss:386.877045  pct:-0.615481899\n",
      "[Iter.  570]  loss:384.513275  pct:-0.610987280\n",
      "[Iter.  580]  loss:382.180878  pct:-0.606584379\n",
      "[Iter.  590]  loss:379.879181  pct:-0.602253255\n",
      "[Iter.  600]  loss:377.607300  pct:-0.598053596\n",
      "[Iter.  610]  loss:375.364563  pct:-0.593933649\n",
      "[Iter.  620]  loss:373.150513  pct:-0.589839988\n",
      "[Iter.  630]  loss:370.964264  pct:-0.585889260\n",
      "[Iter.  640]  loss:368.805115  pct:-0.582036972\n",
      "[Iter.  650]  loss:366.672607  pct:-0.578220648\n",
      "[Iter.  660]  loss:364.566254  pct:-0.574450809\n",
      "[Iter.  670]  loss:362.485504  pct:-0.570746604\n",
      "[Iter.  680]  loss:360.429413  pct:-0.567220285\n",
      "[Iter.  690]  loss:358.397552  pct:-0.563733225\n",
      "[Iter.  700]  loss:356.389526  pct:-0.560278972\n",
      "[Iter.  710]  loss:354.404999  pct:-0.556842287\n",
      "[Iter.  720]  loss:352.443176  pct:-0.553553848\n",
      "[Iter.  730]  loss:350.503723  pct:-0.550288176\n",
      "[Iter.  740]  loss:348.585999  pct:-0.547133877\n",
      "[Iter.  750]  loss:346.689575  pct:-0.544033136\n",
      "[Iter.  760]  loss:344.814087  pct:-0.540970486\n",
      "[Iter.  770]  loss:342.959320  pct:-0.537903443\n",
      "[Iter.  780]  loss:341.124603  pct:-0.534966303\n",
      "[Iter.  790]  loss:339.309662  pct:-0.532046469\n",
      "[Iter.  800]  loss:337.513916  pct:-0.529235106\n",
      "[Iter.  810]  loss:335.737213  pct:-0.526408778\n",
      "[Iter.  820]  loss:333.979187  pct:-0.523631595\n",
      "[Iter.  830]  loss:332.239563  pct:-0.520877974\n",
      "[Iter.  840]  loss:330.517731  pct:-0.518250223\n",
      "[Iter.  850]  loss:328.813568  pct:-0.515603987\n",
      "[Iter.  860]  loss:327.126678  pct:-0.513023127\n",
      "[Iter.  870]  loss:325.456940  pct:-0.510425740\n",
      "[Iter.  880]  loss:323.803772  pct:-0.507952827\n",
      "[Iter.  890]  loss:322.167114  pct:-0.505447390\n",
      "[Iter.  900]  loss:320.546417  pct:-0.503060974\n",
      "[Iter.  910]  loss:318.941467  pct:-0.500691901\n",
      "[Iter.  920]  loss:317.351959  pct:-0.498369833\n",
      "[Iter.  930]  loss:315.777802  pct:-0.496028989\n",
      "[Iter.  940]  loss:314.218781  pct:-0.493708230\n",
      "[Iter.  950]  loss:312.674561  pct:-0.491447382\n",
      "[Iter.  960]  loss:311.144714  pct:-0.489277474\n",
      "[Iter.  970]  loss:309.629028  pct:-0.487132182\n",
      "[Iter.  980]  loss:308.127411  pct:-0.484973079\n",
      "[Iter.  990]  loss:306.639587  pct:-0.482859828\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.802560  pct:100.000000000\n",
      "[Iter.    2]  loss:2.802117  pct:-0.015780805\n",
      "[Iter.    4]  loss:2.802005  pct:-0.004016019\n",
      "[Iter.    6]  loss:2.801967  pct:-0.001344399\n",
      "[Iter.    8]  loss:2.801944  pct:-0.000816861\n",
      "[Iter.   10]  loss:2.801922  pct:-0.000782832\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.801922\n",
      "Best loss: 2.801922 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 24%|██▍       | 2415/10000 [00:37<01:57, 64.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:457.632050  pct:100.000000000\n",
      "[Iter.   10]  loss:76.510643  pct:-83.281186117\n",
      "[Iter.   20]  loss:71.749794  pct:-6.222466329\n",
      "[Iter.   30]  loss:68.513634  pct:-4.510340863\n",
      "[Iter.   40]  loss:65.751411  pct:-4.031638872\n",
      "[Iter.   50]  loss:63.251255  pct:-3.802437618\n",
      "[Iter.   60]  loss:60.961910  pct:-3.619445632\n",
      "[Iter.   70]  loss:58.851814  pct:-3.461335068\n",
      "[Iter.   80]  loss:56.896492  pct:-3.322450276\n",
      "[Iter.   90]  loss:55.076214  pct:-3.199280138\n",
      "[Iter.  100]  loss:53.374752  pct:-3.089286052\n",
      "[Iter.  110]  loss:51.778713  pct:-2.990250553\n",
      "[Iter.  120]  loss:50.276768  pct:-2.900700697\n",
      "[Iter.  130]  loss:48.859364  pct:-2.819203061\n",
      "[Iter.  140]  loss:47.518288  pct:-2.744767430\n",
      "[Iter.  150]  loss:46.246502  pct:-2.676413227\n",
      "[Iter.  160]  loss:45.037914  pct:-2.613360138\n",
      "[Iter.  170]  loss:43.887135  pct:-2.555135473\n",
      "[Iter.  180]  loss:42.789505  pct:-2.501028054\n",
      "[Iter.  190]  loss:41.740841  pct:-2.450750699\n",
      "[Iter.  200]  loss:40.737511  pct:-2.403713506\n",
      "[Iter.  210]  loss:39.776230  pct:-2.359694558\n",
      "[Iter.  220]  loss:38.854012  pct:-2.318516174\n",
      "[Iter.  230]  loss:37.968193  pct:-2.279863639\n",
      "[Iter.  240]  loss:37.116497  pct:-2.243182901\n",
      "[Iter.  250]  loss:36.296669  pct:-2.208796893\n",
      "[Iter.  260]  loss:35.506828  pct:-2.176069375\n",
      "[Iter.  270]  loss:34.745159  pct:-2.145134317\n",
      "[Iter.  280]  loss:34.010082  pct:-2.115623938\n",
      "[Iter.  290]  loss:33.300014  pct:-2.087815442\n",
      "[Iter.  300]  loss:32.613667  pct:-2.061104092\n",
      "[Iter.  310]  loss:31.949644  pct:-2.036025128\n",
      "[Iter.  320]  loss:31.306969  pct:-2.011526006\n",
      "[Iter.  330]  loss:30.684452  pct:-1.988428322\n",
      "[Iter.  340]  loss:30.081141  pct:-1.966179932\n",
      "[Iter.  350]  loss:29.496069  pct:-1.944977995\n",
      "[Iter.  360]  loss:28.928419  pct:-1.924493200\n",
      "[Iter.  370]  loss:28.377316  pct:-1.905059484\n",
      "[Iter.  380]  loss:27.842098  pct:-1.886074406\n",
      "[Iter.  390]  loss:27.321991  pct:-1.868060607\n",
      "[Iter.  400]  loss:26.816425  pct:-1.850398252\n",
      "[Iter.  410]  loss:26.324839  pct:-1.833155162\n",
      "[Iter.  420]  loss:25.846483  pct:-1.817125697\n",
      "[Iter.  430]  loss:25.380882  pct:-1.801409357\n",
      "[Iter.  440]  loss:24.927612  pct:-1.785871562\n",
      "[Iter.  450]  loss:24.486109  pct:-1.771142456\n",
      "[Iter.  460]  loss:24.055975  pct:-1.756644240\n",
      "[Iter.  470]  loss:23.636715  pct:-1.742851935\n",
      "[Iter.  480]  loss:23.227999  pct:-1.729158231\n",
      "[Iter.  490]  loss:22.829405  pct:-1.716006218\n",
      "[Iter.  500]  loss:22.440592  pct:-1.703123764\n",
      "[Iter.  510]  loss:22.061184  pct:-1.690721376\n",
      "[Iter.  520]  loss:21.690884  pct:-1.678515052\n",
      "[Iter.  530]  loss:21.329376  pct:-1.666632959\n",
      "[Iter.  540]  loss:20.976368  pct:-1.655033258\n",
      "[Iter.  550]  loss:20.631603  pct:-1.643586298\n",
      "[Iter.  560]  loss:20.294769  pct:-1.632611629\n",
      "[Iter.  570]  loss:19.965622  pct:-1.621833361\n",
      "[Iter.  580]  loss:19.643909  pct:-1.611332193\n",
      "[Iter.  590]  loss:19.329456  pct:-1.600766516\n",
      "[Iter.  600]  loss:19.021986  pct:-1.590682720\n",
      "[Iter.  610]  loss:18.721312  pct:-1.580667962\n",
      "[Iter.  620]  loss:18.427221  pct:-1.570884977\n",
      "[Iter.  630]  loss:18.139509  pct:-1.561342823\n",
      "[Iter.  640]  loss:17.858015  pct:-1.551828870\n",
      "[Iter.  650]  loss:17.582561  pct:-1.542464639\n",
      "[Iter.  660]  loss:17.312946  pct:-1.533423747\n",
      "[Iter.  670]  loss:17.049042  pct:-1.524319239\n",
      "[Iter.  680]  loss:16.790649  pct:-1.515582740\n",
      "[Iter.  690]  loss:16.537659  pct:-1.506735781\n",
      "[Iter.  700]  loss:16.289917  pct:-1.498045787\n",
      "[Iter.  710]  loss:16.047249  pct:-1.489683170\n",
      "[Iter.  720]  loss:15.809557  pct:-1.481200184\n",
      "[Iter.  730]  loss:15.576684  pct:-1.472988544\n",
      "[Iter.  740]  loss:15.348487  pct:-1.464991508\n",
      "[Iter.  750]  loss:15.124888  pct:-1.456811226\n",
      "[Iter.  760]  loss:14.905765  pct:-1.448763351\n",
      "[Iter.  770]  loss:14.691002  pct:-1.440802896\n",
      "[Iter.  780]  loss:14.480507  pct:-1.432815792\n",
      "[Iter.  790]  loss:14.274143  pct:-1.425113633\n",
      "[Iter.  800]  loss:14.071827  pct:-1.417362017\n",
      "[Iter.  810]  loss:13.873457  pct:-1.409695989\n",
      "[Iter.  820]  loss:13.678938  pct:-1.402094976\n",
      "[Iter.  830]  loss:13.488165  pct:-1.394647826\n",
      "[Iter.  840]  loss:13.301082  pct:-1.387017772\n",
      "[Iter.  850]  loss:13.117571  pct:-1.379668098\n",
      "[Iter.  860]  loss:12.937558  pct:-1.372302118\n",
      "[Iter.  870]  loss:12.760992  pct:-1.364756174\n",
      "[Iter.  880]  loss:12.587746  pct:-1.357624728\n",
      "[Iter.  890]  loss:12.417761  pct:-1.350399206\n",
      "[Iter.  900]  loss:12.250941  pct:-1.343394952\n",
      "[Iter.  910]  loss:12.087244  pct:-1.336201350\n",
      "[Iter.  920]  loss:11.926618  pct:-1.328891938\n",
      "[Iter.  930]  loss:11.768954  pct:-1.321945168\n",
      "[Iter.  940]  loss:11.614236  pct:-1.314631661\n",
      "[Iter.  950]  loss:11.462377  pct:-1.307527116\n",
      "[Iter.  960]  loss:11.313296  pct:-1.300605291\n",
      "[Iter.  970]  loss:11.166940  pct:-1.293668782\n",
      "[Iter.  980]  loss:11.023258  pct:-1.286668770\n",
      "[Iter.  990]  loss:10.882182  pct:-1.279803895\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.768043  pct:100.000000000\n",
      "[Iter.    2]  loss:2.768112  pct:0.002480617\n",
      "[Iter.    4]  loss:2.768111  pct:-0.000017226\n",
      "[Iter.    6]  loss:2.768111  pct:0.000008613\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.768111\n",
      "Best loss: 2.768111 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 10%|█         | 1050/10000 [00:25<03:35, 41.44it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5345.810059  pct:100.000000000\n",
      "[Iter.   10]  loss:889.733093  pct:-83.356443637\n",
      "[Iter.   20]  loss:877.541138  pct:-1.370293592\n",
      "[Iter.   30]  loss:866.088745  pct:-1.305054782\n",
      "[Iter.   40]  loss:856.119934  pct:-1.151014961\n",
      "[Iter.   50]  loss:846.825684  pct:-1.085624819\n",
      "[Iter.   60]  loss:837.882263  pct:-1.056111143\n",
      "[Iter.   70]  loss:829.227051  pct:-1.032986707\n",
      "[Iter.   80]  loss:820.837463  pct:-1.011735856\n",
      "[Iter.   90]  loss:812.697266  pct:-0.991694229\n",
      "[Iter.  100]  loss:804.790649  pct:-0.972885790\n",
      "[Iter.  110]  loss:797.103638  pct:-0.955156689\n",
      "[Iter.  120]  loss:789.622192  pct:-0.938578744\n",
      "[Iter.  130]  loss:782.333435  pct:-0.923068956\n",
      "[Iter.  140]  loss:775.225525  pct:-0.908552522\n",
      "[Iter.  150]  loss:768.288818  pct:-0.894798523\n",
      "[Iter.  160]  loss:761.517700  pct:-0.881324575\n",
      "[Iter.  170]  loss:754.910706  pct:-0.867608806\n",
      "[Iter.  180]  loss:748.465515  pct:-0.853768582\n",
      "[Iter.  190]  loss:742.179382  pct:-0.839869398\n",
      "[Iter.  200]  loss:736.047058  pct:-0.826259037\n",
      "[Iter.  210]  loss:730.062317  pct:-0.813092199\n",
      "[Iter.  220]  loss:724.218872  pct:-0.800403567\n",
      "[Iter.  230]  loss:718.510254  pct:-0.788244878\n",
      "[Iter.  240]  loss:712.930237  pct:-0.776609249\n",
      "[Iter.  250]  loss:707.473206  pct:-0.765436920\n",
      "[Iter.  260]  loss:702.133179  pct:-0.754802700\n",
      "[Iter.  270]  loss:696.904846  pct:-0.744635445\n",
      "[Iter.  280]  loss:691.783936  pct:-0.734807725\n",
      "[Iter.  290]  loss:686.765137  pct:-0.725486466\n",
      "[Iter.  300]  loss:681.844421  pct:-0.716506280\n",
      "[Iter.  310]  loss:677.018127  pct:-0.707829205\n",
      "[Iter.  320]  loss:672.282349  pct:-0.699505466\n",
      "[Iter.  330]  loss:667.634949  pct:-0.691286914\n",
      "[Iter.  340]  loss:663.071350  pct:-0.683546995\n",
      "[Iter.  350]  loss:658.589233  pct:-0.675962956\n",
      "[Iter.  360]  loss:654.186096  pct:-0.668571089\n",
      "[Iter.  370]  loss:649.859741  pct:-0.661333985\n",
      "[Iter.  380]  loss:645.607300  pct:-0.654362955\n",
      "[Iter.  390]  loss:641.426636  pct:-0.647555265\n",
      "[Iter.  400]  loss:637.315491  pct:-0.640937683\n",
      "[Iter.  410]  loss:633.271667  pct:-0.634508858\n",
      "[Iter.  420]  loss:629.292969  pct:-0.628276763\n",
      "[Iter.  430]  loss:625.377380  pct:-0.622220265\n",
      "[Iter.  440]  loss:621.523682  pct:-0.616219718\n",
      "[Iter.  450]  loss:617.729248  pct:-0.610505071\n",
      "[Iter.  460]  loss:613.992859  pct:-0.604858710\n",
      "[Iter.  470]  loss:610.312805  pct:-0.599364253\n",
      "[Iter.  480]  loss:606.687439  pct:-0.594017720\n",
      "[Iter.  490]  loss:603.115234  pct:-0.588804772\n",
      "[Iter.  500]  loss:599.594482  pct:-0.583761071\n",
      "[Iter.  510]  loss:596.124023  pct:-0.578801021\n",
      "[Iter.  520]  loss:592.702820  pct:-0.573908026\n",
      "[Iter.  530]  loss:589.329346  pct:-0.569167888\n",
      "[Iter.  540]  loss:586.002747  pct:-0.564471996\n",
      "[Iter.  550]  loss:582.721375  pct:-0.559958480\n",
      "[Iter.  560]  loss:579.484375  pct:-0.555496958\n",
      "[Iter.  570]  loss:576.290771  pct:-0.551111238\n",
      "[Iter.  580]  loss:573.139587  pct:-0.546804537\n",
      "[Iter.  590]  loss:570.028870  pct:-0.542750465\n",
      "[Iter.  600]  loss:566.958191  pct:-0.538688280\n",
      "[Iter.  610]  loss:563.926636  pct:-0.534705244\n",
      "[Iter.  620]  loss:560.933044  pct:-0.530847653\n",
      "[Iter.  630]  loss:557.976746  pct:-0.527032390\n",
      "[Iter.  640]  loss:555.056946  pct:-0.523283421\n",
      "[Iter.  650]  loss:552.172668  pct:-0.519636294\n",
      "[Iter.  660]  loss:549.323059  pct:-0.516072152\n",
      "[Iter.  670]  loss:546.507324  pct:-0.512582681\n",
      "[Iter.  680]  loss:543.725098  pct:-0.509092274\n",
      "[Iter.  690]  loss:540.975037  pct:-0.505781515\n",
      "[Iter.  700]  loss:538.256653  pct:-0.502497085\n",
      "[Iter.  710]  loss:535.569397  pct:-0.499251769\n",
      "[Iter.  720]  loss:532.912354  pct:-0.496115624\n",
      "[Iter.  730]  loss:530.285217  pct:-0.492977168\n",
      "[Iter.  740]  loss:527.687256  pct:-0.489917754\n",
      "[Iter.  750]  loss:525.117981  pct:-0.486893491\n",
      "[Iter.  760]  loss:522.576538  pct:-0.483975595\n",
      "[Iter.  770]  loss:520.062988  pct:-0.480991706\n",
      "[Iter.  780]  loss:517.576172  pct:-0.478176002\n",
      "[Iter.  790]  loss:515.116028  pct:-0.475320190\n",
      "[Iter.  800]  loss:512.681824  pct:-0.472554526\n",
      "[Iter.  810]  loss:510.273102  pct:-0.469827837\n",
      "[Iter.  820]  loss:507.889374  pct:-0.467147498\n",
      "[Iter.  830]  loss:505.529724  pct:-0.464599139\n",
      "[Iter.  840]  loss:503.194336  pct:-0.461968520\n",
      "[Iter.  850]  loss:500.882294  pct:-0.459473025\n",
      "[Iter.  860]  loss:498.593536  pct:-0.456945145\n",
      "[Iter.  870]  loss:496.327362  pct:-0.454513376\n",
      "[Iter.  880]  loss:494.083374  pct:-0.452118543\n",
      "[Iter.  890]  loss:491.861542  pct:-0.449687723\n",
      "[Iter.  900]  loss:489.661072  pct:-0.447375894\n",
      "[Iter.  910]  loss:487.481812  pct:-0.445054831\n",
      "[Iter.  920]  loss:485.322906  pct:-0.442868837\n",
      "[Iter.  930]  loss:483.184937  pct:-0.440525255\n",
      "[Iter.  940]  loss:481.066589  pct:-0.438413330\n",
      "[Iter.  950]  loss:478.967682  pct:-0.436302898\n",
      "[Iter.  960]  loss:476.888489  pct:-0.434098833\n",
      "[Iter.  970]  loss:474.828125  pct:-0.432043091\n",
      "[Iter.  980]  loss:472.786560  pct:-0.429958723\n",
      "[Iter.  990]  loss:470.763489  pct:-0.427903722\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.838184  pct:100.000000000\n",
      "[Iter.    2]  loss:2.837023  pct:-0.040893110\n",
      "[Iter.    4]  loss:2.836447  pct:-0.020320458\n",
      "[Iter.    6]  loss:2.836122  pct:-0.011465152\n",
      "[Iter.    8]  loss:2.835937  pct:-0.006506632\n",
      "[Iter.   10]  loss:2.835828  pct:-0.003833614\n",
      "[Iter.   12]  loss:2.835725  pct:-0.003657206\n",
      "[Iter.   14]  loss:2.835671  pct:-0.001900135\n",
      "[Iter.   16]  loss:2.835649  pct:-0.000781929\n",
      "[Iter.   18]  loss:2.835609  pct:-0.001387304\n",
      "[Iter.   20]  loss:2.835583  pct:-0.000933290\n",
      "[Iter.   22]  loss:2.835553  pct:-0.001059420\n",
      "[Iter.   24]  loss:2.835535  pct:-0.000630614\n",
      "[Iter.   26]  loss:2.835529  pct:-0.000218614\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.835529\n",
      "Best loss: 2.835529 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 80%|████████  | 8000/10000 [02:19<00:34, 57.42it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:330.911285  pct:100.000000000\n",
      "[Iter.   10]  loss:64.196182  pct:-80.600178857\n",
      "[Iter.   20]  loss:60.039036  pct:-6.475691090\n",
      "[Iter.   30]  loss:56.734020  pct:-5.504777883\n",
      "[Iter.   40]  loss:53.826805  pct:-5.124288930\n",
      "[Iter.   50]  loss:51.243595  pct:-4.799114467\n",
      "[Iter.   60]  loss:48.937717  pct:-4.499835892\n",
      "[Iter.   70]  loss:46.870201  pct:-4.224791092\n",
      "[Iter.   80]  loss:44.991364  pct:-4.008597234\n",
      "[Iter.   90]  loss:43.272366  pct:-3.820728737\n",
      "[Iter.  100]  loss:41.688904  pct:-3.659290960\n",
      "[Iter.  110]  loss:40.221035  pct:-3.521006001\n",
      "[Iter.  120]  loss:38.853363  pct:-3.400389787\n",
      "[Iter.  130]  loss:37.574055  pct:-3.292657878\n",
      "[Iter.  140]  loss:36.373390  pct:-3.195461680\n",
      "[Iter.  150]  loss:35.243282  pct:-3.106963287\n",
      "[Iter.  160]  loss:34.176735  pct:-3.026243084\n",
      "[Iter.  170]  loss:33.167747  pct:-2.952263957\n",
      "[Iter.  180]  loss:32.211155  pct:-2.884104686\n",
      "[Iter.  190]  loss:31.302443  pct:-2.821110851\n",
      "[Iter.  200]  loss:30.437658  pct:-2.762673358\n",
      "[Iter.  210]  loss:29.613344  pct:-2.708204781\n",
      "[Iter.  220]  loss:28.826366  pct:-2.657510624\n",
      "[Iter.  230]  loss:28.074041  pct:-2.609850464\n",
      "[Iter.  240]  loss:27.353924  pct:-2.565065569\n",
      "[Iter.  250]  loss:26.663830  pct:-2.522833650\n",
      "[Iter.  260]  loss:26.001730  pct:-2.483138556\n",
      "[Iter.  270]  loss:25.365854  pct:-2.445513059\n",
      "[Iter.  280]  loss:24.754622  pct:-2.409667545\n",
      "[Iter.  290]  loss:24.166504  pct:-2.375789100\n",
      "[Iter.  300]  loss:23.600136  pct:-2.343607934\n",
      "[Iter.  310]  loss:23.054323  pct:-2.312751975\n",
      "[Iter.  320]  loss:22.527889  pct:-2.283450007\n",
      "[Iter.  330]  loss:22.019775  pct:-2.255488099\n",
      "[Iter.  340]  loss:21.529089  pct:-2.228389745\n",
      "[Iter.  350]  loss:21.054913  pct:-2.202491742\n",
      "[Iter.  360]  loss:20.596361  pct:-2.177883216\n",
      "[Iter.  370]  loss:20.152721  pct:-2.153971528\n",
      "[Iter.  380]  loss:19.723291  pct:-2.130878502\n",
      "[Iter.  390]  loss:19.307383  pct:-2.108719103\n",
      "[Iter.  400]  loss:18.904390  pct:-2.087244331\n",
      "[Iter.  410]  loss:18.513754  pct:-2.066379487\n",
      "[Iter.  420]  loss:18.134893  pct:-2.046373069\n",
      "[Iter.  430]  loss:17.767296  pct:-2.027018144\n",
      "[Iter.  440]  loss:17.410543  pct:-2.007916111\n",
      "[Iter.  450]  loss:17.064152  pct:-1.989551211\n",
      "[Iter.  460]  loss:16.727739  pct:-1.971457090\n",
      "[Iter.  470]  loss:16.400887  pct:-1.953956790\n",
      "[Iter.  480]  loss:16.083218  pct:-1.936900875\n",
      "[Iter.  490]  loss:15.774383  pct:-1.920231616\n",
      "[Iter.  500]  loss:15.474043  pct:-1.903971183\n",
      "[Iter.  510]  loss:15.181890  pct:-1.888022157\n",
      "[Iter.  520]  loss:14.897597  pct:-1.872574691\n",
      "[Iter.  530]  loss:14.620925  pct:-1.857160973\n",
      "[Iter.  540]  loss:14.351587  pct:-1.842138271\n",
      "[Iter.  550]  loss:14.089344  pct:-1.827277119\n",
      "[Iter.  560]  loss:13.833920  pct:-1.812884584\n",
      "[Iter.  570]  loss:13.585102  pct:-1.798610870\n",
      "[Iter.  580]  loss:13.342680  pct:-1.784470241\n",
      "[Iter.  590]  loss:13.106415  pct:-1.770747578\n",
      "[Iter.  600]  loss:12.876155  pct:-1.756848833\n",
      "[Iter.  610]  loss:12.651661  pct:-1.743486174\n",
      "[Iter.  620]  loss:12.432750  pct:-1.730295906\n",
      "[Iter.  630]  loss:12.219257  pct:-1.717177598\n",
      "[Iter.  640]  loss:12.011035  pct:-1.704051099\n",
      "[Iter.  650]  loss:11.807890  pct:-1.691319922\n",
      "[Iter.  660]  loss:11.609689  pct:-1.678548670\n",
      "[Iter.  670]  loss:11.416309  pct:-1.665672579\n",
      "[Iter.  680]  loss:11.227582  pct:-1.653138269\n",
      "[Iter.  690]  loss:11.043357  pct:-1.640825983\n",
      "[Iter.  700]  loss:10.863527  pct:-1.628396141\n",
      "[Iter.  710]  loss:10.687947  pct:-1.616234027\n",
      "[Iter.  720]  loss:10.516485  pct:-1.604256221\n",
      "[Iter.  730]  loss:10.349060  pct:-1.592025779\n",
      "[Iter.  740]  loss:10.185539  pct:-1.580054730\n",
      "[Iter.  750]  loss:10.025809  pct:-1.568203251\n",
      "[Iter.  760]  loss:9.869786  pct:-1.556213778\n",
      "[Iter.  770]  loss:9.717358  pct:-1.544396433\n",
      "[Iter.  780]  loss:9.568415  pct:-1.532751525\n",
      "[Iter.  790]  loss:9.422879  pct:-1.520998763\n",
      "[Iter.  800]  loss:9.280675  pct:-1.509138357\n",
      "[Iter.  810]  loss:9.141688  pct:-1.497591377\n",
      "[Iter.  820]  loss:9.005839  pct:-1.486038398\n",
      "[Iter.  830]  loss:8.873046  pct:-1.474525820\n",
      "[Iter.  840]  loss:8.743238  pct:-1.462941513\n",
      "[Iter.  850]  loss:8.616347  pct:-1.451305908\n",
      "[Iter.  860]  loss:8.492280  pct:-1.439906053\n",
      "[Iter.  870]  loss:8.370987  pct:-1.428274478\n",
      "[Iter.  880]  loss:8.252384  pct:-1.416831176\n",
      "[Iter.  890]  loss:8.136418  pct:-1.405240481\n",
      "[Iter.  900]  loss:8.023000  pct:-1.393961991\n",
      "[Iter.  910]  loss:7.912078  pct:-1.382548461\n",
      "[Iter.  920]  loss:7.803596  pct:-1.371098239\n",
      "[Iter.  930]  loss:7.697500  pct:-1.359569617\n",
      "[Iter.  940]  loss:7.593718  pct:-1.348258186\n",
      "[Iter.  950]  loss:7.492207  pct:-1.336782295\n",
      "[Iter.  960]  loss:7.392892  pct:-1.325566932\n",
      "[Iter.  970]  loss:7.295749  pct:-1.314007678\n",
      "[Iter.  980]  loss:7.200721  pct:-1.302510834\n",
      "[Iter.  990]  loss:7.107718  pct:-1.291576118\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.749439  pct:100.000000000\n",
      "[Iter.    2]  loss:2.749500  pct:0.002237257\n",
      "[Iter.    4]  loss:2.749500  pct:-0.000008671\n",
      "[Iter.    6]  loss:2.749500  pct:-0.000008671\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.749500\n",
      "Best loss: 2.749500 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1757/10000 [00:43<03:22, 40.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:282.357819  pct:100.000000000\n",
      "[Iter.   10]  loss:61.400108  pct:-78.254501232\n",
      "[Iter.   20]  loss:55.526020  pct:-9.566902154\n",
      "[Iter.   30]  loss:51.912758  pct:-6.507331470\n",
      "[Iter.   40]  loss:48.961044  pct:-5.685911677\n",
      "[Iter.   50]  loss:46.405090  pct:-5.220382889\n",
      "[Iter.   60]  loss:44.144295  pct:-4.871869825\n",
      "[Iter.   70]  loss:42.120018  pct:-4.585590834\n",
      "[Iter.   80]  loss:40.290077  pct:-4.344586927\n",
      "[Iter.   90]  loss:38.622841  pct:-4.138081740\n",
      "[Iter.  100]  loss:37.093624  pct:-3.959358585\n",
      "[Iter.  110]  loss:35.683125  pct:-3.802539133\n",
      "[Iter.  120]  loss:34.375904  pct:-3.663413661\n",
      "[Iter.  130]  loss:33.159119  pct:-3.539646341\n",
      "[Iter.  140]  loss:32.022221  pct:-3.428613567\n",
      "[Iter.  150]  loss:30.956430  pct:-3.328283161\n",
      "[Iter.  160]  loss:29.954344  pct:-3.237087175\n",
      "[Iter.  170]  loss:29.009691  pct:-3.153641301\n",
      "[Iter.  180]  loss:28.117002  pct:-3.077208730\n",
      "[Iter.  190]  loss:27.271626  pct:-3.006639733\n",
      "[Iter.  200]  loss:26.469458  pct:-2.941401098\n",
      "[Iter.  210]  loss:25.706938  pct:-2.880753536\n",
      "[Iter.  220]  loss:24.980906  pct:-2.824265819\n",
      "[Iter.  230]  loss:24.288570  pct:-2.771457295\n",
      "[Iter.  240]  loss:23.627474  pct:-2.721842257\n",
      "[Iter.  250]  loss:22.995399  pct:-2.675166887\n",
      "[Iter.  260]  loss:22.390339  pct:-2.631224468\n",
      "[Iter.  270]  loss:21.810501  pct:-2.589678529\n",
      "[Iter.  280]  loss:21.254259  pct:-2.550340254\n",
      "[Iter.  290]  loss:20.720173  pct:-2.512843307\n",
      "[Iter.  300]  loss:20.206854  pct:-2.477387705\n",
      "[Iter.  310]  loss:19.713097  pct:-2.443513727\n",
      "[Iter.  320]  loss:19.237795  pct:-2.411096297\n",
      "[Iter.  330]  loss:18.779922  pct:-2.380066914\n",
      "[Iter.  340]  loss:18.338556  pct:-2.350202436\n",
      "[Iter.  350]  loss:17.912790  pct:-2.321698527\n",
      "[Iter.  360]  loss:17.501829  pct:-2.294233027\n",
      "[Iter.  370]  loss:17.104980  pct:-2.267469733\n",
      "[Iter.  380]  loss:16.721472  pct:-2.242087812\n",
      "[Iter.  390]  loss:16.350712  pct:-2.217268723\n",
      "[Iter.  400]  loss:15.992084  pct:-2.193349604\n",
      "[Iter.  410]  loss:15.645013  pct:-2.170265637\n",
      "[Iter.  420]  loss:15.309030  pct:-2.147542348\n",
      "[Iter.  430]  loss:14.983624  pct:-2.125582636\n",
      "[Iter.  440]  loss:14.668321  pct:-2.104316414\n",
      "[Iter.  450]  loss:14.362701  pct:-2.083532580\n",
      "[Iter.  460]  loss:14.066390  pct:-2.063061606\n",
      "[Iter.  470]  loss:13.778976  pct:-2.043264806\n",
      "[Iter.  480]  loss:13.500117  pct:-2.023801548\n",
      "[Iter.  490]  loss:13.229467  pct:-2.004796728\n",
      "[Iter.  500]  loss:12.966727  pct:-1.986022017\n",
      "[Iter.  510]  loss:12.711571  pct:-1.967778854\n",
      "[Iter.  520]  loss:12.463718  pct:-1.949816671\n",
      "[Iter.  530]  loss:12.222882  pct:-1.932297694\n",
      "[Iter.  540]  loss:11.988846  pct:-1.914740242\n",
      "[Iter.  550]  loss:11.761336  pct:-1.897676406\n",
      "[Iter.  560]  loss:11.540141  pct:-1.880698033\n",
      "[Iter.  570]  loss:11.325032  pct:-1.864005557\n",
      "[Iter.  580]  loss:11.115806  pct:-1.847470311\n",
      "[Iter.  590]  loss:10.912226  pct:-1.831445327\n",
      "[Iter.  600]  loss:10.714155  pct:-1.815124899\n",
      "[Iter.  610]  loss:10.521390  pct:-1.799164118\n",
      "[Iter.  620]  loss:10.333744  pct:-1.783470747\n",
      "[Iter.  630]  loss:10.151072  pct:-1.767728132\n",
      "[Iter.  640]  loss:9.973207  pct:-1.752179832\n",
      "[Iter.  650]  loss:9.799987  pct:-1.736850435\n",
      "[Iter.  660]  loss:9.631271  pct:-1.721588812\n",
      "[Iter.  670]  loss:9.466929  pct:-1.706336790\n",
      "[Iter.  680]  loss:9.306815  pct:-1.691301170\n",
      "[Iter.  690]  loss:9.150812  pct:-1.676223239\n",
      "[Iter.  700]  loss:8.998775  pct:-1.661455447\n",
      "[Iter.  710]  loss:8.850595  pct:-1.646668574\n",
      "[Iter.  720]  loss:8.706156  pct:-1.631977167\n",
      "[Iter.  730]  loss:8.565376  pct:-1.617010984\n",
      "[Iter.  740]  loss:8.428123  pct:-1.602414221\n",
      "[Iter.  750]  loss:8.294286  pct:-1.587989311\n",
      "[Iter.  760]  loss:8.163795  pct:-1.573255451\n",
      "[Iter.  770]  loss:8.036551  pct:-1.558649403\n",
      "[Iter.  780]  loss:7.912442  pct:-1.544304251\n",
      "[Iter.  790]  loss:7.791379  pct:-1.530030295\n",
      "[Iter.  800]  loss:7.673291  pct:-1.515627025\n",
      "[Iter.  810]  loss:7.558096  pct:-1.501243750\n",
      "[Iter.  820]  loss:7.445727  pct:-1.486737685\n",
      "[Iter.  830]  loss:7.336077  pct:-1.472652168\n",
      "[Iter.  840]  loss:7.229089  pct:-1.458387458\n",
      "[Iter.  850]  loss:7.124681  pct:-1.444279733\n",
      "[Iter.  860]  loss:7.022813  pct:-1.429779183\n",
      "[Iter.  870]  loss:6.923407  pct:-1.415476075\n",
      "[Iter.  880]  loss:6.826381  pct:-1.401424862\n",
      "[Iter.  890]  loss:6.731691  pct:-1.387109406\n",
      "[Iter.  900]  loss:6.639250  pct:-1.373229310\n",
      "[Iter.  910]  loss:6.549023  pct:-1.358995817\n",
      "[Iter.  920]  loss:6.460953  pct:-1.344772240\n",
      "[Iter.  930]  loss:6.374979  pct:-1.330666490\n",
      "[Iter.  940]  loss:6.291037  pct:-1.316747974\n",
      "[Iter.  950]  loss:6.209082  pct:-1.302733267\n",
      "[Iter.  960]  loss:6.129080  pct:-1.288464793\n",
      "[Iter.  970]  loss:6.050971  pct:-1.274404424\n",
      "[Iter.  980]  loss:5.974687  pct:-1.260689191\n",
      "[Iter.  990]  loss:5.900206  pct:-1.246609490\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.726021  pct:100.000000000\n",
      "[Iter.    2]  loss:2.726068  pct:0.001722968\n",
      "[Iter.    4]  loss:2.726065  pct:-0.000122442\n",
      "[Iter.    6]  loss:2.726064  pct:-0.000017492\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.726064\n",
      "Best loss: 2.726064 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 14%|█▍        | 1381/10000 [00:31<03:13, 44.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:622.550598  pct:100.000000000\n",
      "[Iter.   10]  loss:103.776039  pct:-83.330501598\n",
      "[Iter.   20]  loss:98.230331  pct:-5.343919222\n",
      "[Iter.   30]  loss:94.436707  pct:-3.861968929\n",
      "[Iter.   40]  loss:91.000114  pct:-3.639042728\n",
      "[Iter.   50]  loss:87.859535  pct:-3.451181620\n",
      "[Iter.   60]  loss:84.970360  pct:-3.288402799\n",
      "[Iter.   70]  loss:82.297318  pct:-3.145852629\n",
      "[Iter.   80]  loss:79.811783  pct:-3.020189167\n",
      "[Iter.   90]  loss:77.490402  pct:-2.908568801\n",
      "[Iter.  100]  loss:75.313614  pct:-2.809107022\n",
      "[Iter.  110]  loss:73.264626  pct:-2.720608183\n",
      "[Iter.  120]  loss:71.329041  pct:-2.641909390\n",
      "[Iter.  130]  loss:69.494164  pct:-2.572412303\n",
      "[Iter.  140]  loss:67.749840  pct:-2.510029105\n",
      "[Iter.  150]  loss:66.089432  pct:-2.450792541\n",
      "[Iter.  160]  loss:64.508324  pct:-2.392376589\n",
      "[Iter.  170]  loss:63.001312  pct:-2.336150326\n",
      "[Iter.  180]  loss:61.562321  pct:-2.284065990\n",
      "[Iter.  190]  loss:60.185863  pct:-2.235876098\n",
      "[Iter.  200]  loss:58.867313  pct:-2.190797030\n",
      "[Iter.  210]  loss:57.602406  pct:-2.148743953\n",
      "[Iter.  220]  loss:56.386879  pct:-2.110201074\n",
      "[Iter.  230]  loss:55.218136  pct:-2.072721802\n",
      "[Iter.  240]  loss:54.093357  pct:-2.036973416\n",
      "[Iter.  250]  loss:53.009308  pct:-2.004033921\n",
      "[Iter.  260]  loss:51.963234  pct:-1.973377801\n",
      "[Iter.  270]  loss:50.952011  pct:-1.946035230\n",
      "[Iter.  280]  loss:49.973129  pct:-1.921183904\n",
      "[Iter.  290]  loss:49.024723  pct:-1.897832362\n",
      "[Iter.  300]  loss:48.106266  pct:-1.873456848\n",
      "[Iter.  310]  loss:47.217991  pct:-1.846485333\n",
      "[Iter.  320]  loss:46.358807  pct:-1.819612078\n",
      "[Iter.  330]  loss:45.526630  pct:-1.795076857\n",
      "[Iter.  340]  loss:44.719505  pct:-1.772863672\n",
      "[Iter.  350]  loss:43.935802  pct:-1.752485509\n",
      "[Iter.  360]  loss:43.174282  pct:-1.733257032\n",
      "[Iter.  370]  loss:42.433792  pct:-1.715118177\n",
      "[Iter.  380]  loss:41.713470  pct:-1.697518933\n",
      "[Iter.  390]  loss:41.012398  pct:-1.680686563\n",
      "[Iter.  400]  loss:40.329704  pct:-1.664602702\n",
      "[Iter.  410]  loss:39.664654  pct:-1.649033928\n",
      "[Iter.  420]  loss:39.016560  pct:-1.633933781\n",
      "[Iter.  430]  loss:38.384640  pct:-1.619619637\n",
      "[Iter.  440]  loss:37.768250  pct:-1.605825227\n",
      "[Iter.  450]  loss:37.166931  pct:-1.592126633\n",
      "[Iter.  460]  loss:36.580013  pct:-1.579140002\n",
      "[Iter.  470]  loss:36.006996  pct:-1.566475977\n",
      "[Iter.  480]  loss:35.447380  pct:-1.554187099\n",
      "[Iter.  490]  loss:34.900715  pct:-1.542187859\n",
      "[Iter.  500]  loss:34.366432  pct:-1.530864586\n",
      "[Iter.  510]  loss:33.844166  pct:-1.519699179\n",
      "[Iter.  520]  loss:33.333439  pct:-1.509054564\n",
      "[Iter.  530]  loss:32.833954  pct:-1.498450303\n",
      "[Iter.  540]  loss:32.345268  pct:-1.488354433\n",
      "[Iter.  550]  loss:31.867071  pct:-1.478414382\n",
      "[Iter.  560]  loss:31.399038  pct:-1.468703649\n",
      "[Iter.  570]  loss:30.940826  pct:-1.459318257\n",
      "[Iter.  580]  loss:30.492060  pct:-1.450403109\n",
      "[Iter.  590]  loss:30.052563  pct:-1.441348988\n",
      "[Iter.  600]  loss:29.622036  pct:-1.432579103\n",
      "[Iter.  610]  loss:29.200159  pct:-1.424199564\n",
      "[Iter.  620]  loss:28.786697  pct:-1.415956962\n",
      "[Iter.  630]  loss:28.381416  pct:-1.407876219\n",
      "[Iter.  640]  loss:27.984125  pct:-1.399828603\n",
      "[Iter.  650]  loss:27.594606  pct:-1.391927516\n",
      "[Iter.  660]  loss:27.212587  pct:-1.384397507\n",
      "[Iter.  670]  loss:26.837847  pct:-1.377085522\n",
      "[Iter.  680]  loss:26.470249  pct:-1.369698483\n",
      "[Iter.  690]  loss:26.109570  pct:-1.362584931\n",
      "[Iter.  700]  loss:25.755636  pct:-1.355569396\n",
      "[Iter.  710]  loss:25.408215  pct:-1.348915023\n",
      "[Iter.  720]  loss:25.067219  pct:-1.342069068\n",
      "[Iter.  730]  loss:24.732430  pct:-1.335566100\n",
      "[Iter.  740]  loss:24.403706  pct:-1.329120972\n",
      "[Iter.  750]  loss:24.080921  pct:-1.322686108\n",
      "[Iter.  760]  loss:23.763926  pct:-1.316376639\n",
      "[Iter.  770]  loss:23.452515  pct:-1.310435447\n",
      "[Iter.  780]  loss:23.146608  pct:-1.304364587\n",
      "[Iter.  790]  loss:22.846100  pct:-1.298283077\n",
      "[Iter.  800]  loss:22.550814  pct:-1.292501479\n",
      "[Iter.  810]  loss:22.260660  pct:-1.286665340\n",
      "[Iter.  820]  loss:21.975487  pct:-1.281064505\n",
      "[Iter.  830]  loss:21.695198  pct:-1.275460696\n",
      "[Iter.  840]  loss:21.419687  pct:-1.269915984\n",
      "[Iter.  850]  loss:21.148857  pct:-1.264398266\n",
      "[Iter.  860]  loss:20.882601  pct:-1.258963219\n",
      "[Iter.  870]  loss:20.620720  pct:-1.254062544\n",
      "[Iter.  880]  loss:20.363169  pct:-1.248992248\n",
      "[Iter.  890]  loss:20.109911  pct:-1.243705020\n",
      "[Iter.  900]  loss:19.860809  pct:-1.238700854\n",
      "[Iter.  910]  loss:19.615837  pct:-1.233445349\n",
      "[Iter.  920]  loss:19.374847  pct:-1.228546525\n",
      "[Iter.  930]  loss:19.137783  pct:-1.223567631\n",
      "[Iter.  940]  loss:18.904541  pct:-1.218751588\n",
      "[Iter.  950]  loss:18.675024  pct:-1.214083869\n",
      "[Iter.  960]  loss:18.449245  pct:-1.208986823\n",
      "[Iter.  970]  loss:18.227026  pct:-1.204490816\n",
      "[Iter.  980]  loss:18.008377  pct:-1.199586321\n",
      "[Iter.  990]  loss:17.793142  pct:-1.195192413\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.769568  pct:100.000000000\n",
      "[Iter.    2]  loss:2.769483  pct:-0.003081847\n",
      "[Iter.    4]  loss:2.769447  pct:-0.001299925\n",
      "[Iter.    6]  loss:2.769432  pct:-0.000542360\n",
      "[Iter.    8]  loss:2.769425  pct:-0.000249659\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.769425\n",
      "Best loss: 2.769425 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 27%|██▋       | 2725/10000 [00:37<01:41, 71.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:404.518127  pct:100.000000000\n",
      "[Iter.   10]  loss:79.511147  pct:-80.344233417\n",
      "[Iter.   20]  loss:74.640717  pct:-6.125468194\n",
      "[Iter.   30]  loss:70.672264  pct:-5.316739491\n",
      "[Iter.   40]  loss:67.333466  pct:-4.724340681\n",
      "[Iter.   50]  loss:64.372978  pct:-4.396754779\n",
      "[Iter.   60]  loss:61.713867  pct:-4.130787633\n",
      "[Iter.   70]  loss:59.302765  pct:-3.906905214\n",
      "[Iter.   80]  loss:57.099525  pct:-3.715238986\n",
      "[Iter.   90]  loss:55.073021  pct:-3.549074183\n",
      "[Iter.  100]  loss:53.198734  pct:-3.403275542\n",
      "[Iter.  110]  loss:51.456844  pct:-3.274307137\n",
      "[Iter.  120]  loss:49.831203  pct:-3.159231566\n",
      "[Iter.  130]  loss:48.308350  pct:-3.056024630\n",
      "[Iter.  140]  loss:46.877098  pct:-2.962741508\n",
      "[Iter.  150]  loss:45.527954  pct:-2.878045009\n",
      "[Iter.  160]  loss:44.252838  pct:-2.800731972\n",
      "[Iter.  170]  loss:43.044827  pct:-2.729794694\n",
      "[Iter.  180]  loss:41.897903  pct:-2.664485278\n",
      "[Iter.  190]  loss:40.806824  pct:-2.604139163\n",
      "[Iter.  200]  loss:39.766975  pct:-2.548221676\n",
      "[Iter.  210]  loss:38.774334  pct:-2.496145203\n",
      "[Iter.  220]  loss:37.825241  pct:-2.447734798\n",
      "[Iter.  230]  loss:36.916523  pct:-2.402411942\n",
      "[Iter.  240]  loss:36.045380  pct:-2.359765413\n",
      "[Iter.  250]  loss:35.209183  pct:-2.319844895\n",
      "[Iter.  260]  loss:34.405659  pct:-2.282143335\n",
      "[Iter.  270]  loss:33.632698  pct:-2.246609109\n",
      "[Iter.  280]  loss:32.888412  pct:-2.212982087\n",
      "[Iter.  290]  loss:32.171032  pct:-2.181256162\n",
      "[Iter.  300]  loss:31.479042  pct:-2.150972029\n",
      "[Iter.  310]  loss:30.810980  pct:-2.122244409\n",
      "[Iter.  320]  loss:30.165531  pct:-2.094865817\n",
      "[Iter.  330]  loss:29.541494  pct:-2.068708108\n",
      "[Iter.  340]  loss:28.937754  pct:-2.043703966\n",
      "[Iter.  350]  loss:28.353193  pct:-2.020061408\n",
      "[Iter.  360]  loss:27.786963  pct:-1.997061736\n",
      "[Iter.  370]  loss:27.238110  pct:-1.975217408\n",
      "[Iter.  380]  loss:26.705856  pct:-1.954075644\n",
      "[Iter.  390]  loss:26.189377  pct:-1.933955931\n",
      "[Iter.  400]  loss:25.687988  pct:-1.914473006\n",
      "[Iter.  410]  loss:25.201021  pct:-1.895699583\n",
      "[Iter.  420]  loss:24.727848  pct:-1.877595109\n",
      "[Iter.  430]  loss:24.267849  pct:-1.860247133\n",
      "[Iter.  440]  loss:23.820515  pct:-1.843320725\n",
      "[Iter.  450]  loss:23.385294  pct:-1.827083605\n",
      "[Iter.  460]  loss:22.961725  pct:-1.811261070\n",
      "[Iter.  470]  loss:22.549364  pct:-1.795863076\n",
      "[Iter.  480]  loss:22.147751  pct:-1.781040183\n",
      "[Iter.  490]  loss:21.756489  pct:-1.766599494\n",
      "[Iter.  500]  loss:21.375204  pct:-1.752510330\n",
      "[Iter.  510]  loss:21.003532  pct:-1.738798260\n",
      "[Iter.  520]  loss:20.641138  pct:-1.725397070\n",
      "[Iter.  530]  loss:20.287638  pct:-1.712601141\n",
      "[Iter.  540]  loss:19.942768  pct:-1.699900297\n",
      "[Iter.  550]  loss:19.606209  pct:-1.687625780\n",
      "[Iter.  560]  loss:19.277693  pct:-1.675571294\n",
      "[Iter.  570]  loss:18.956955  pct:-1.663777103\n",
      "[Iter.  580]  loss:18.643709  pct:-1.652405537\n",
      "[Iter.  590]  loss:18.337740  pct:-1.641139299\n",
      "[Iter.  600]  loss:18.038836  pct:-1.629995953\n",
      "[Iter.  610]  loss:17.746748  pct:-1.619215135\n",
      "[Iter.  620]  loss:17.461279  pct:-1.608571078\n",
      "[Iter.  630]  loss:17.182236  pct:-1.598068498\n",
      "[Iter.  640]  loss:16.909380  pct:-1.588010799\n",
      "[Iter.  650]  loss:16.642576  pct:-1.577844617\n",
      "[Iter.  660]  loss:16.381639  pct:-1.567886688\n",
      "[Iter.  670]  loss:16.126398  pct:-1.558094319\n",
      "[Iter.  680]  loss:15.876681  pct:-1.548496802\n",
      "[Iter.  690]  loss:15.632357  pct:-1.538890144\n",
      "[Iter.  700]  loss:15.393241  pct:-1.529620392\n",
      "[Iter.  710]  loss:15.159202  pct:-1.520403063\n",
      "[Iter.  720]  loss:14.930091  pct:-1.511364012\n",
      "[Iter.  730]  loss:14.705767  pct:-1.502497391\n",
      "[Iter.  740]  loss:14.486115  pct:-1.493646545\n",
      "[Iter.  750]  loss:14.271013  pct:-1.484878792\n",
      "[Iter.  760]  loss:14.060354  pct:-1.476132236\n",
      "[Iter.  770]  loss:13.854009  pct:-1.467570125\n",
      "[Iter.  780]  loss:13.651890  pct:-1.458919785\n",
      "[Iter.  790]  loss:13.453881  pct:-1.450411190\n",
      "[Iter.  800]  loss:13.259860  pct:-1.442120836\n",
      "[Iter.  810]  loss:13.069734  pct:-1.433849366\n",
      "[Iter.  820]  loss:12.883393  pct:-1.425739326\n",
      "[Iter.  830]  loss:12.700757  pct:-1.417609918\n",
      "[Iter.  840]  loss:12.521722  pct:-1.409641854\n",
      "[Iter.  850]  loss:12.346250  pct:-1.401342896\n",
      "[Iter.  860]  loss:12.174227  pct:-1.393320444\n",
      "[Iter.  870]  loss:12.005563  pct:-1.385418408\n",
      "[Iter.  880]  loss:11.840179  pct:-1.377555904\n",
      "[Iter.  890]  loss:11.678010  pct:-1.369653705\n",
      "[Iter.  900]  loss:11.518986  pct:-1.361740902\n",
      "[Iter.  910]  loss:11.363002  pct:-1.354146348\n",
      "[Iter.  920]  loss:11.210000  pct:-1.346490898\n",
      "[Iter.  930]  loss:11.059931  pct:-1.338708619\n",
      "[Iter.  940]  loss:10.912735  pct:-1.330892740\n",
      "[Iter.  950]  loss:10.768322  pct:-1.323343732\n",
      "[Iter.  960]  loss:10.626649  pct:-1.315646841\n",
      "[Iter.  970]  loss:10.487633  pct:-1.308184289\n",
      "[Iter.  980]  loss:10.351230  pct:-1.300608889\n",
      "[Iter.  990]  loss:10.217422  pct:-1.292678651\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.756257  pct:100.000000000\n",
      "[Iter.    2]  loss:2.756281  pct:0.000865009\n",
      "[Iter.    4]  loss:2.756279  pct:-0.000077850\n",
      "[Iter.    6]  loss:2.756279  pct:-0.000008650\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.756279\n",
      "Best loss: 2.756279 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 20%|█▉        | 1986/10000 [00:28<01:53, 70.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:655.524170  pct:100.000000000\n",
      "[Iter.   10]  loss:107.548576  pct:-83.593498315\n",
      "[Iter.   20]  loss:100.988808  pct:-6.099354263\n",
      "[Iter.   30]  loss:96.728943  pct:-4.218155363\n",
      "[Iter.   40]  loss:93.387169  pct:-3.454781876\n",
      "[Iter.   50]  loss:90.353287  pct:-3.248714119\n",
      "[Iter.   60]  loss:87.550522  pct:-3.102006572\n",
      "[Iter.   70]  loss:84.945923  pct:-2.974966847\n",
      "[Iter.   80]  loss:82.514717  pct:-2.862062908\n",
      "[Iter.   90]  loss:80.236298  pct:-2.761228027\n",
      "[Iter.  100]  loss:78.094200  pct:-2.669736188\n",
      "[Iter.  110]  loss:76.074005  pct:-2.586869452\n",
      "[Iter.  120]  loss:74.163208  pct:-2.511760904\n",
      "[Iter.  130]  loss:72.351456  pct:-2.442926038\n",
      "[Iter.  140]  loss:70.629669  pct:-2.379753776\n",
      "[Iter.  150]  loss:68.990128  pct:-2.321321400\n",
      "[Iter.  160]  loss:67.425873  pct:-2.267360296\n",
      "[Iter.  170]  loss:65.930969  pct:-2.217106731\n",
      "[Iter.  180]  loss:64.500023  pct:-2.170370566\n",
      "[Iter.  190]  loss:63.128277  pct:-2.126737328\n",
      "[Iter.  200]  loss:61.811523  pct:-2.085837684\n",
      "[Iter.  210]  loss:60.545910  pct:-2.047536585\n",
      "[Iter.  220]  loss:59.328049  pct:-2.011467295\n",
      "[Iter.  230]  loss:58.154819  pct:-1.977528746\n",
      "[Iter.  240]  loss:57.023491  pct:-1.945373733\n",
      "[Iter.  250]  loss:55.931438  pct:-1.915092258\n",
      "[Iter.  260]  loss:54.876316  pct:-1.886456713\n",
      "[Iter.  270]  loss:53.856033  pct:-1.859240595\n",
      "[Iter.  280]  loss:52.868664  pct:-1.833349908\n",
      "[Iter.  290]  loss:51.912422  pct:-1.808711511\n",
      "[Iter.  300]  loss:50.985584  pct:-1.785387547\n",
      "[Iter.  310]  loss:50.086716  pct:-1.762985702\n",
      "[Iter.  320]  loss:49.214371  pct:-1.741669340\n",
      "[Iter.  330]  loss:48.367249  pct:-1.721290306\n",
      "[Iter.  340]  loss:47.544144  pct:-1.701781440\n",
      "[Iter.  350]  loss:46.743938  pct:-1.683078438\n",
      "[Iter.  360]  loss:45.964352  pct:-1.667781573\n",
      "[Iter.  370]  loss:45.205185  pct:-1.651642393\n",
      "[Iter.  380]  loss:44.467682  pct:-1.631456774\n",
      "[Iter.  390]  loss:43.749401  pct:-1.615287242\n",
      "[Iter.  400]  loss:43.049366  pct:-1.600102122\n",
      "[Iter.  410]  loss:42.366920  pct:-1.585262664\n",
      "[Iter.  420]  loss:41.701359  pct:-1.570946551\n",
      "[Iter.  430]  loss:41.051945  pct:-1.557297127\n",
      "[Iter.  440]  loss:40.418087  pct:-1.544038245\n",
      "[Iter.  450]  loss:39.799206  pct:-1.531198707\n",
      "[Iter.  460]  loss:39.194733  pct:-1.518806977\n",
      "[Iter.  470]  loss:38.604164  pct:-1.506754868\n",
      "[Iter.  480]  loss:38.027035  pct:-1.494992515\n",
      "[Iter.  490]  loss:37.462795  pct:-1.483785169\n",
      "[Iter.  500]  loss:36.911007  pct:-1.472896847\n",
      "[Iter.  510]  loss:36.371284  pct:-1.462226278\n",
      "[Iter.  520]  loss:35.843178  pct:-1.451988009\n",
      "[Iter.  530]  loss:35.326328  pct:-1.441974595\n",
      "[Iter.  540]  loss:34.820335  pct:-1.432339318\n",
      "[Iter.  550]  loss:34.324898  pct:-1.422839891\n",
      "[Iter.  560]  loss:33.839649  pct:-1.413692676\n",
      "[Iter.  570]  loss:33.364281  pct:-1.404767812\n",
      "[Iter.  580]  loss:32.898495  pct:-1.396061808\n",
      "[Iter.  590]  loss:32.442017  pct:-1.387534970\n",
      "[Iter.  600]  loss:31.994534  pct:-1.379331835\n",
      "[Iter.  610]  loss:31.555801  pct:-1.371272210\n",
      "[Iter.  620]  loss:31.125601  pct:-1.363301066\n",
      "[Iter.  630]  loss:30.703581  pct:-1.355861244\n",
      "[Iter.  640]  loss:30.289658  pct:-1.348127000\n",
      "[Iter.  650]  loss:29.883524  pct:-1.340832759\n",
      "[Iter.  660]  loss:29.484987  pct:-1.333633486\n",
      "[Iter.  670]  loss:29.093840  pct:-1.326599229\n",
      "[Iter.  680]  loss:28.709902  pct:-1.319653371\n",
      "[Iter.  690]  loss:28.332991  pct:-1.312826375\n",
      "[Iter.  700]  loss:27.962910  pct:-1.306183849\n",
      "[Iter.  710]  loss:27.599442  pct:-1.299822422\n",
      "[Iter.  720]  loss:27.242487  pct:-1.293339846\n",
      "[Iter.  730]  loss:26.891809  pct:-1.287244776\n",
      "[Iter.  740]  loss:26.547310  pct:-1.281057671\n",
      "[Iter.  750]  loss:26.208813  pct:-1.275071423\n",
      "[Iter.  760]  loss:25.876171  pct:-1.269197522\n",
      "[Iter.  770]  loss:25.549265  pct:-1.263348441\n",
      "[Iter.  780]  loss:25.227894  pct:-1.257848630\n",
      "[Iter.  790]  loss:24.911947  pct:-1.252370020\n",
      "[Iter.  800]  loss:24.601376  pct:-1.246677618\n",
      "[Iter.  810]  loss:24.296001  pct:-1.241288905\n",
      "[Iter.  820]  loss:23.995726  pct:-1.235906260\n",
      "[Iter.  830]  loss:23.700453  pct:-1.230522601\n",
      "[Iter.  840]  loss:23.410019  pct:-1.225436012\n",
      "[Iter.  850]  loss:23.124317  pct:-1.220425121\n",
      "[Iter.  860]  loss:22.843315  pct:-1.215179859\n",
      "[Iter.  870]  loss:22.566851  pct:-1.210264188\n",
      "[Iter.  880]  loss:22.294828  pct:-1.205406334\n",
      "[Iter.  890]  loss:22.027145  pct:-1.200650771\n",
      "[Iter.  900]  loss:21.763727  pct:-1.195879870\n",
      "[Iter.  910]  loss:21.504408  pct:-1.191520658\n",
      "[Iter.  920]  loss:21.249168  pct:-1.186917064\n",
      "[Iter.  930]  loss:20.997982  pct:-1.182099771\n",
      "[Iter.  940]  loss:20.750696  pct:-1.177664799\n",
      "[Iter.  950]  loss:20.507212  pct:-1.173379895\n",
      "[Iter.  960]  loss:20.267511  pct:-1.168858649\n",
      "[Iter.  970]  loss:20.031502  pct:-1.164472507\n",
      "[Iter.  980]  loss:19.799049  pct:-1.160434176\n",
      "[Iter.  990]  loss:19.570185  pct:-1.155937669\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.765984  pct:100.000000000\n",
      "[Iter.    2]  loss:2.765998  pct:0.000508560\n",
      "[Iter.    4]  loss:2.765989  pct:-0.000301687\n",
      "[Iter.    6]  loss:2.765987  pct:-0.000068957\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.765987\n",
      "Best loss: 2.765987 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 26%|██▌       | 2573/10000 [00:34<01:40, 73.99it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:989.205200  pct:100.000000000\n",
      "[Iter.   10]  loss:175.562698  pct:-82.252147171\n",
      "[Iter.   20]  loss:166.458923  pct:-5.185483653\n",
      "[Iter.   30]  loss:160.788910  pct:-3.406253816\n",
      "[Iter.   40]  loss:155.860321  pct:-3.065254233\n",
      "[Iter.   50]  loss:151.366898  pct:-2.882981013\n",
      "[Iter.   60]  loss:147.214127  pct:-2.743513319\n",
      "[Iter.   70]  loss:143.349289  pct:-2.625317105\n",
      "[Iter.   80]  loss:139.736206  pct:-2.520474927\n",
      "[Iter.   90]  loss:136.345062  pct:-2.426818285\n",
      "[Iter.  100]  loss:133.151093  pct:-2.342563547\n",
      "[Iter.  110]  loss:130.133713  pct:-2.266132184\n",
      "[Iter.  120]  loss:127.275482  pct:-2.196379808\n",
      "[Iter.  130]  loss:124.561234  pct:-2.132577784\n",
      "[Iter.  140]  loss:121.978073  pct:-2.073807659\n",
      "[Iter.  150]  loss:119.514641  pct:-2.019569787\n",
      "[Iter.  160]  loss:117.161049  pct:-1.969291714\n",
      "[Iter.  170]  loss:114.908691  pct:-1.922445646\n",
      "[Iter.  180]  loss:112.749657  pct:-1.878913338\n",
      "[Iter.  190]  loss:110.677078  pct:-1.838212631\n",
      "[Iter.  200]  loss:108.684799  pct:-1.800082803\n",
      "[Iter.  210]  loss:106.767197  pct:-1.764370504\n",
      "[Iter.  220]  loss:104.919350  pct:-1.730725394\n",
      "[Iter.  230]  loss:103.136650  pct:-1.699114215\n",
      "[Iter.  240]  loss:101.415146  pct:-1.669148853\n",
      "[Iter.  250]  loss:99.751137  pct:-1.640789529\n",
      "[Iter.  260]  loss:98.141121  pct:-1.614032603\n",
      "[Iter.  270]  loss:96.582100  pct:-1.588550224\n",
      "[Iter.  280]  loss:95.071121  pct:-1.564450038\n",
      "[Iter.  290]  loss:93.605637  pct:-1.541461382\n",
      "[Iter.  300]  loss:92.183258  pct:-1.519543685\n",
      "[Iter.  310]  loss:90.801750  pct:-1.498653771\n",
      "[Iter.  320]  loss:89.459053  pct:-1.478712845\n",
      "[Iter.  330]  loss:88.153290  pct:-1.459621134\n",
      "[Iter.  340]  loss:86.882614  pct:-1.441438728\n",
      "[Iter.  350]  loss:85.645470  pct:-1.423926389\n",
      "[Iter.  360]  loss:84.440277  pct:-1.407187760\n",
      "[Iter.  370]  loss:83.265663  pct:-1.391058856\n",
      "[Iter.  380]  loss:82.120255  pct:-1.375607408\n",
      "[Iter.  390]  loss:81.002785  pct:-1.360772436\n",
      "[Iter.  400]  loss:79.912033  pct:-1.346560679\n",
      "[Iter.  410]  loss:78.846962  pct:-1.332804416\n",
      "[Iter.  420]  loss:77.806534  pct:-1.319553900\n",
      "[Iter.  430]  loss:76.789795  pct:-1.306752585\n",
      "[Iter.  440]  loss:75.795860  pct:-1.294357710\n",
      "[Iter.  450]  loss:74.823830  pct:-1.282432360\n",
      "[Iter.  460]  loss:73.872917  pct:-1.270868492\n",
      "[Iter.  470]  loss:72.942230  pct:-1.259848651\n",
      "[Iter.  480]  loss:72.031212  pct:-1.248958757\n",
      "[Iter.  490]  loss:71.139038  pct:-1.238593304\n",
      "[Iter.  500]  loss:70.265106  pct:-1.228484259\n",
      "[Iter.  510]  loss:69.408768  pct:-1.218725122\n",
      "[Iter.  520]  loss:68.569481  pct:-1.209194216\n",
      "[Iter.  530]  loss:67.746567  pct:-1.200117184\n",
      "[Iter.  540]  loss:66.939590  pct:-1.191169319\n",
      "[Iter.  550]  loss:66.148056  pct:-1.182460810\n",
      "[Iter.  560]  loss:65.371437  pct:-1.174061649\n",
      "[Iter.  570]  loss:64.609352  pct:-1.165776668\n",
      "[Iter.  580]  loss:63.861202  pct:-1.157959099\n",
      "[Iter.  590]  loss:63.126625  pct:-1.150271453\n",
      "[Iter.  600]  loss:62.405342  pct:-1.142597055\n",
      "[Iter.  610]  loss:61.696892  pct:-1.135239859\n",
      "[Iter.  620]  loss:61.000839  pct:-1.128180904\n",
      "[Iter.  630]  loss:60.316875  pct:-1.121236665\n",
      "[Iter.  640]  loss:59.644585  pct:-1.114598190\n",
      "[Iter.  650]  loss:58.983776  pct:-1.107910411\n",
      "[Iter.  660]  loss:58.334053  pct:-1.101528413\n",
      "[Iter.  670]  loss:57.695274  pct:-1.095035667\n",
      "[Iter.  680]  loss:57.066910  pct:-1.089109238\n",
      "[Iter.  690]  loss:56.448895  pct:-1.082966103\n",
      "[Iter.  700]  loss:55.840839  pct:-1.077178074\n",
      "[Iter.  710]  loss:55.242470  pct:-1.071562686\n",
      "[Iter.  720]  loss:54.653683  pct:-1.065823235\n",
      "[Iter.  730]  loss:54.074089  pct:-1.060484179\n",
      "[Iter.  740]  loss:53.503494  pct:-1.055209246\n",
      "[Iter.  750]  loss:52.941692  pct:-1.050028448\n",
      "[Iter.  760]  loss:52.388496  pct:-1.044915508\n",
      "[Iter.  770]  loss:51.843700  pct:-1.039915301\n",
      "[Iter.  780]  loss:51.307114  pct:-1.035008607\n",
      "[Iter.  790]  loss:50.778507  pct:-1.030278995\n",
      "[Iter.  800]  loss:50.257736  pct:-1.025573722\n",
      "[Iter.  810]  loss:49.744583  pct:-1.021042958\n",
      "[Iter.  820]  loss:49.238911  pct:-1.016537727\n",
      "[Iter.  830]  loss:48.740524  pct:-1.012179953\n",
      "[Iter.  840]  loss:48.249386  pct:-1.007659366\n",
      "[Iter.  850]  loss:47.765182  pct:-1.003543009\n",
      "[Iter.  860]  loss:47.287804  pct:-0.999428496\n",
      "[Iter.  870]  loss:46.817154  pct:-0.995287755\n",
      "[Iter.  880]  loss:46.352989  pct:-0.991441587\n",
      "[Iter.  890]  loss:45.895248  pct:-0.987510820\n",
      "[Iter.  900]  loss:45.443871  pct:-0.983495861\n",
      "[Iter.  910]  loss:44.998547  pct:-0.979942815\n",
      "[Iter.  920]  loss:44.559269  pct:-0.976204082\n",
      "[Iter.  930]  loss:44.125946  pct:-0.972464129\n",
      "[Iter.  940]  loss:43.698402  pct:-0.968916654\n",
      "[Iter.  950]  loss:43.276642  pct:-0.965162422\n",
      "[Iter.  960]  loss:42.860367  pct:-0.961893083\n",
      "[Iter.  970]  loss:42.449532  pct:-0.958543514\n",
      "[Iter.  980]  loss:42.044025  pct:-0.955266452\n",
      "[Iter.  990]  loss:41.643776  pct:-0.951977070\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.748972  pct:100.000000000\n",
      "[Iter.    2]  loss:2.748985  pct:0.000468342\n",
      "[Iter.    4]  loss:2.748978  pct:-0.000251516\n",
      "[Iter.    6]  loss:2.748972  pct:-0.000216825\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.748972\n",
      "Best loss: 2.748972 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 21%|██        | 2099/10000 [00:46<02:56, 44.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:465.407288  pct:100.000000000\n",
      "[Iter.   10]  loss:77.587555  pct:-83.329106140\n",
      "[Iter.   20]  loss:73.604111  pct:-5.134127783\n",
      "[Iter.   30]  loss:70.442123  pct:-4.295938466\n",
      "[Iter.   40]  loss:67.550095  pct:-4.105538942\n",
      "[Iter.   50]  loss:64.968742  pct:-3.821389517\n",
      "[Iter.   60]  loss:62.621090  pct:-3.613510666\n",
      "[Iter.   70]  loss:60.459240  pct:-3.452271396\n",
      "[Iter.   80]  loss:58.456036  pct:-3.313313808\n",
      "[Iter.   90]  loss:56.590935  pct:-3.190604428\n",
      "[Iter.  100]  loss:54.847290  pct:-3.081137857\n",
      "[Iter.  110]  loss:53.211403  pct:-2.982621648\n",
      "[Iter.  120]  loss:51.671757  pct:-2.893451525\n",
      "[Iter.  130]  loss:50.218544  pct:-2.812392745\n",
      "[Iter.  140]  loss:48.843441  pct:-2.738237486\n",
      "[Iter.  150]  loss:47.539234  pct:-2.670178065\n",
      "[Iter.  160]  loss:46.299698  pct:-2.607396411\n",
      "[Iter.  170]  loss:45.119354  pct:-2.549354925\n",
      "[Iter.  180]  loss:43.993439  pct:-2.495415872\n",
      "[Iter.  190]  loss:42.917683  pct:-2.445264804\n",
      "[Iter.  200]  loss:41.888325  pct:-2.398447089\n",
      "[Iter.  210]  loss:40.902016  pct:-2.354615654\n",
      "[Iter.  220]  loss:39.955738  pct:-2.313523191\n",
      "[Iter.  230]  loss:39.046783  pct:-2.274903842\n",
      "[Iter.  240]  loss:38.172760  pct:-2.238400606\n",
      "[Iter.  250]  loss:37.331432  pct:-2.204000096\n",
      "[Iter.  260]  loss:36.520782  pct:-2.171494156\n",
      "[Iter.  270]  loss:35.738987  pct:-2.140686614\n",
      "[Iter.  280]  loss:34.984402  pct:-2.111378442\n",
      "[Iter.  290]  loss:34.255497  pct:-2.083513476\n",
      "[Iter.  300]  loss:33.550838  pct:-2.057066954\n",
      "[Iter.  310]  loss:32.869217  pct:-2.031608099\n",
      "[Iter.  320]  loss:32.209339  pct:-2.007585939\n",
      "[Iter.  330]  loss:31.570150  pct:-1.984482711\n",
      "[Iter.  340]  loss:30.950628  pct:-1.962366626\n",
      "[Iter.  350]  loss:30.349766  pct:-1.941358016\n",
      "[Iter.  360]  loss:29.766766  pct:-1.920937998\n",
      "[Iter.  370]  loss:29.200769  pct:-1.901436581\n",
      "[Iter.  380]  loss:28.651016  pct:-1.882666792\n",
      "[Iter.  390]  loss:28.116816  pct:-1.864508623\n",
      "[Iter.  400]  loss:27.597452  pct:-1.847162962\n",
      "[Iter.  410]  loss:27.092358  pct:-1.830221591\n",
      "[Iter.  420]  loss:26.600941  pct:-1.813858128\n",
      "[Iter.  430]  loss:26.122604  pct:-1.798193303\n",
      "[Iter.  440]  loss:25.656855  pct:-1.782937620\n",
      "[Iter.  450]  loss:25.203171  pct:-1.768275417\n",
      "[Iter.  460]  loss:24.761118  pct:-1.753957250\n",
      "[Iter.  470]  loss:24.330250  pct:-1.740099740\n",
      "[Iter.  480]  loss:23.910147  pct:-1.726669791\n",
      "[Iter.  490]  loss:23.500425  pct:-1.713587873\n",
      "[Iter.  500]  loss:23.100723  pct:-1.700829097\n",
      "[Iter.  510]  loss:22.710680  pct:-1.688446090\n",
      "[Iter.  520]  loss:22.329935  pct:-1.676501690\n",
      "[Iter.  530]  loss:21.958221  pct:-1.664642719\n",
      "[Iter.  540]  loss:21.595226  pct:-1.653117256\n",
      "[Iter.  550]  loss:21.240669  pct:-1.641830619\n",
      "[Iter.  560]  loss:20.894245  pct:-1.630947211\n",
      "[Iter.  570]  loss:20.555723  pct:-1.620168400\n",
      "[Iter.  580]  loss:20.224806  pct:-1.609855101\n",
      "[Iter.  590]  loss:19.901331  pct:-1.599396734\n",
      "[Iter.  600]  loss:19.584976  pct:-1.589616053\n",
      "[Iter.  610]  loss:19.275572  pct:-1.579804693\n",
      "[Iter.  620]  loss:18.972946  pct:-1.569995738\n",
      "[Iter.  630]  loss:18.676880  pct:-1.560465526\n",
      "[Iter.  640]  loss:18.387205  pct:-1.550980467\n",
      "[Iter.  650]  loss:18.103664  pct:-1.542054509\n",
      "[Iter.  660]  loss:17.826153  pct:-1.532902901\n",
      "[Iter.  670]  loss:17.554457  pct:-1.524143172\n",
      "[Iter.  680]  loss:17.288420  pct:-1.515495419\n",
      "[Iter.  690]  loss:17.027893  pct:-1.506943152\n",
      "[Iter.  700]  loss:16.772726  pct:-1.498523666\n",
      "[Iter.  710]  loss:16.522820  pct:-1.489957798\n",
      "[Iter.  720]  loss:16.278021  pct:-1.481579218\n",
      "[Iter.  730]  loss:16.038145  pct:-1.473617681\n",
      "[Iter.  740]  loss:15.803094  pct:-1.465575689\n",
      "[Iter.  750]  loss:15.572770  pct:-1.457460120\n",
      "[Iter.  760]  loss:15.347012  pct:-1.449700669\n",
      "[Iter.  770]  loss:15.125719  pct:-1.441925646\n",
      "[Iter.  780]  loss:14.908772  pct:-1.434289510\n",
      "[Iter.  790]  loss:14.696061  pct:-1.426752837\n",
      "[Iter.  800]  loss:14.487499  pct:-1.419168683\n",
      "[Iter.  810]  loss:14.282993  pct:-1.411602631\n",
      "[Iter.  820]  loss:14.082438  pct:-1.404151380\n",
      "[Iter.  830]  loss:13.885753  pct:-1.396674244\n",
      "[Iter.  840]  loss:13.692846  pct:-1.389239634\n",
      "[Iter.  850]  loss:13.503599  pct:-1.382087604\n",
      "[Iter.  860]  loss:13.317942  pct:-1.374874202\n",
      "[Iter.  870]  loss:13.135787  pct:-1.367738800\n",
      "[Iter.  880]  loss:12.957044  pct:-1.360735845\n",
      "[Iter.  890]  loss:12.781652  pct:-1.353635922\n",
      "[Iter.  900]  loss:12.609546  pct:-1.346514025\n",
      "[Iter.  910]  loss:12.440619  pct:-1.339677072\n",
      "[Iter.  920]  loss:12.274827  pct:-1.332662933\n",
      "[Iter.  930]  loss:12.112076  pct:-1.325894025\n",
      "[Iter.  940]  loss:11.952343  pct:-1.318789786\n",
      "[Iter.  950]  loss:11.795500  pct:-1.312237990\n",
      "[Iter.  960]  loss:11.641552  pct:-1.305140374\n",
      "[Iter.  970]  loss:11.490412  pct:-1.298282337\n",
      "[Iter.  980]  loss:11.342019  pct:-1.291447865\n",
      "[Iter.  990]  loss:11.196312  pct:-1.284666596\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.742767  pct:100.000000000\n",
      "[Iter.    2]  loss:2.742810  pct:0.001564674\n",
      "[Iter.    4]  loss:2.742801  pct:-0.000321622\n",
      "[Iter.    6]  loss:2.742797  pct:-0.000139080\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.742797\n",
      "Best loss: 2.742797 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▌        | 1605/10000 [00:29<02:34, 54.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3971.165039  pct:100.000000000\n",
      "[Iter.   10]  loss:635.628479  pct:-83.993905775\n",
      "[Iter.   20]  loss:617.416016  pct:-2.865268625\n",
      "[Iter.   30]  loss:608.870667  pct:-1.384050447\n",
      "[Iter.   40]  loss:601.098328  pct:-1.276517214\n",
      "[Iter.   50]  loss:593.638550  pct:-1.241024553\n",
      "[Iter.   60]  loss:586.528992  pct:-1.197624061\n",
      "[Iter.   70]  loss:579.745850  pct:-1.156488799\n",
      "[Iter.   80]  loss:573.237061  pct:-1.122696966\n",
      "[Iter.   90]  loss:566.952637  pct:-1.096304524\n",
      "[Iter.  100]  loss:560.868774  pct:-1.073081226\n",
      "[Iter.  110]  loss:554.973083  pct:-1.051171181\n",
      "[Iter.  120]  loss:549.247437  pct:-1.031698139\n",
      "[Iter.  130]  loss:543.681458  pct:-1.013382791\n",
      "[Iter.  140]  loss:538.266846  pct:-0.995916219\n",
      "[Iter.  150]  loss:532.996643  pct:-0.979105936\n",
      "[Iter.  160]  loss:527.863708  pct:-0.963033189\n",
      "[Iter.  170]  loss:522.861450  pct:-0.947642018\n",
      "[Iter.  180]  loss:517.984070  pct:-0.932824627\n",
      "[Iter.  190]  loss:513.225952  pct:-0.918583785\n",
      "[Iter.  200]  loss:508.581604  pct:-0.904932443\n",
      "[Iter.  210]  loss:504.046387  pct:-0.891738366\n",
      "[Iter.  220]  loss:499.615509  pct:-0.879061492\n",
      "[Iter.  230]  loss:495.284576  pct:-0.866853118\n",
      "[Iter.  240]  loss:491.049469  pct:-0.855085667\n",
      "[Iter.  250]  loss:486.906250  pct:-0.843747780\n",
      "[Iter.  260]  loss:482.851379  pct:-0.832782616\n",
      "[Iter.  270]  loss:478.881256  pct:-0.822224697\n",
      "[Iter.  280]  loss:474.992798  pct:-0.811987983\n",
      "[Iter.  290]  loss:471.182922  pct:-0.802091212\n",
      "[Iter.  300]  loss:467.448517  pct:-0.792559607\n",
      "[Iter.  310]  loss:463.787079  pct:-0.783281550\n",
      "[Iter.  320]  loss:460.195953  pct:-0.774304773\n",
      "[Iter.  330]  loss:456.672516  pct:-0.765638523\n",
      "[Iter.  340]  loss:453.214752  pct:-0.757164829\n",
      "[Iter.  350]  loss:449.820038  pct:-0.749029977\n",
      "[Iter.  360]  loss:446.486481  pct:-0.741086846\n",
      "[Iter.  370]  loss:443.211975  pct:-0.733394124\n",
      "[Iter.  380]  loss:439.994812  pct:-0.725874585\n",
      "[Iter.  390]  loss:436.832886  pct:-0.718628080\n",
      "[Iter.  400]  loss:433.724487  pct:-0.711576106\n",
      "[Iter.  410]  loss:430.668030  pct:-0.704700244\n",
      "[Iter.  420]  loss:427.661987  pct:-0.697995271\n",
      "[Iter.  430]  loss:424.704803  pct:-0.691476897\n",
      "[Iter.  440]  loss:421.795105  pct:-0.685110802\n",
      "[Iter.  450]  loss:418.931213  pct:-0.678976965\n",
      "[Iter.  460]  loss:416.112122  pct:-0.672924744\n",
      "[Iter.  470]  loss:413.336334  pct:-0.667076783\n",
      "[Iter.  480]  loss:410.602539  pct:-0.661397254\n",
      "[Iter.  490]  loss:407.909790  pct:-0.655804280\n",
      "[Iter.  500]  loss:405.256714  pct:-0.650407575\n",
      "[Iter.  510]  loss:402.642761  pct:-0.645011556\n",
      "[Iter.  520]  loss:400.066406  pct:-0.639861244\n",
      "[Iter.  530]  loss:397.526672  pct:-0.634828080\n",
      "[Iter.  540]  loss:395.022858  pct:-0.629848227\n",
      "[Iter.  550]  loss:392.553955  pct:-0.625002463\n",
      "[Iter.  560]  loss:390.118744  pct:-0.620350693\n",
      "[Iter.  570]  loss:387.716736  pct:-0.615712035\n",
      "[Iter.  580]  loss:385.347229  pct:-0.611143811\n",
      "[Iter.  590]  loss:383.009186  pct:-0.606736739\n",
      "[Iter.  600]  loss:380.701813  pct:-0.602432822\n",
      "[Iter.  610]  loss:378.424377  pct:-0.598220241\n",
      "[Iter.  620]  loss:376.176300  pct:-0.594062520\n",
      "[Iter.  630]  loss:373.956757  pct:-0.590027457\n",
      "[Iter.  640]  loss:371.765228  pct:-0.586037899\n",
      "[Iter.  650]  loss:369.601013  pct:-0.582145646\n",
      "[Iter.  660]  loss:367.463470  pct:-0.578337896\n",
      "[Iter.  670]  loss:365.352112  pct:-0.574576471\n",
      "[Iter.  680]  loss:363.266266  pct:-0.570913888\n",
      "[Iter.  690]  loss:361.205200  pct:-0.567370512\n",
      "[Iter.  700]  loss:359.168579  pct:-0.563840469\n",
      "[Iter.  710]  loss:357.155975  pct:-0.560350731\n",
      "[Iter.  720]  loss:355.166748  pct:-0.556963185\n",
      "[Iter.  730]  loss:353.200409  pct:-0.553638290\n",
      "[Iter.  740]  loss:351.256317  pct:-0.550421729\n",
      "[Iter.  750]  loss:349.334137  pct:-0.547230066\n",
      "[Iter.  760]  loss:347.433441  pct:-0.544091058\n",
      "[Iter.  770]  loss:345.553802  pct:-0.541006837\n",
      "[Iter.  780]  loss:343.694916  pct:-0.537944223\n",
      "[Iter.  790]  loss:341.855988  pct:-0.535046676\n",
      "[Iter.  800]  loss:340.036987  pct:-0.532095476\n",
      "[Iter.  810]  loss:338.237427  pct:-0.529224941\n",
      "[Iter.  820]  loss:336.456940  pct:-0.526401551\n",
      "[Iter.  830]  loss:334.695190  pct:-0.523618050\n",
      "[Iter.  840]  loss:332.951752  pct:-0.520903428\n",
      "[Iter.  850]  loss:331.226105  pct:-0.518287399\n",
      "[Iter.  860]  loss:329.518311  pct:-0.515597704\n",
      "[Iter.  870]  loss:327.827850  pct:-0.513009490\n",
      "[Iter.  880]  loss:326.154572  pct:-0.510413867\n",
      "[Iter.  890]  loss:324.497833  pct:-0.507961079\n",
      "[Iter.  900]  loss:322.857605  pct:-0.505466633\n",
      "[Iter.  910]  loss:321.233337  pct:-0.503091008\n",
      "[Iter.  920]  loss:319.624939  pct:-0.500694744\n",
      "[Iter.  930]  loss:318.032288  pct:-0.498287578\n",
      "[Iter.  940]  loss:316.454926  pct:-0.495975447\n",
      "[Iter.  950]  loss:314.892700  pct:-0.493664410\n",
      "[Iter.  960]  loss:313.345093  pct:-0.491471355\n",
      "[Iter.  970]  loss:311.812073  pct:-0.489243347\n",
      "[Iter.  980]  loss:310.293213  pct:-0.487107459\n",
      "[Iter.  990]  loss:308.788513  pct:-0.484928334\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.797353  pct:100.000000000\n",
      "[Iter.    2]  loss:2.796902  pct:-0.016142573\n",
      "[Iter.    4]  loss:2.796779  pct:-0.004398581\n",
      "[Iter.    6]  loss:2.796723  pct:-0.002003318\n",
      "[Iter.    8]  loss:2.796689  pct:-0.001210540\n",
      "[Iter.   10]  loss:2.796690  pct:0.000025575\n",
      "[Iter.   12]  loss:2.796654  pct:-0.001287279\n",
      "[Iter.   14]  loss:2.796653  pct:-0.000008525\n",
      "[Iter.   16]  loss:2.796639  pct:-0.000502983\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.796639\n",
      "Best loss: 2.796639 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "100%|██████████| 10000/10000 [03:23<00:00, 49.22it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:459.749542  pct:100.000000000\n",
      "[Iter.   10]  loss:75.249870  pct:-83.632419172\n",
      "[Iter.   20]  loss:71.374924  pct:-5.149439566\n",
      "[Iter.   30]  loss:68.264381  pct:-4.358032395\n",
      "[Iter.   40]  loss:65.514503  pct:-4.028276347\n",
      "[Iter.   50]  loss:63.045616  pct:-3.768459193\n",
      "[Iter.   60]  loss:60.786057  pct:-3.584007532\n",
      "[Iter.   70]  loss:58.701889  pct:-3.428693355\n",
      "[Iter.   80]  loss:56.768837  pct:-3.292998053\n",
      "[Iter.   90]  loss:54.967705  pct:-3.172748110\n",
      "[Iter.  100]  loss:53.282825  pct:-3.065216767\n",
      "[Iter.  110]  loss:51.701164  pct:-2.968425962\n",
      "[Iter.  120]  loss:50.211811  pct:-2.880695632\n",
      "[Iter.  130]  loss:48.805492  pct:-2.800772636\n",
      "[Iter.  140]  loss:47.474216  pct:-2.727717465\n",
      "[Iter.  150]  loss:46.211151  pct:-2.660529088\n",
      "[Iter.  160]  loss:45.010262  pct:-2.598700872\n",
      "[Iter.  170]  loss:43.866379  pct:-2.541382148\n",
      "[Iter.  180]  loss:42.774891  pct:-2.488210595\n",
      "[Iter.  190]  loss:41.731804  pct:-2.438549775\n",
      "[Iter.  200]  loss:40.733395  pct:-2.392442162\n",
      "[Iter.  210]  loss:39.776573  pct:-2.348985275\n",
      "[Iter.  220]  loss:38.858387  pct:-2.308359203\n",
      "[Iter.  230]  loss:37.976261  pct:-2.270104147\n",
      "[Iter.  240]  loss:37.127865  pct:-2.234017451\n",
      "[Iter.  250]  loss:36.311085  pct:-2.199911290\n",
      "[Iter.  260]  loss:35.523991  pct:-2.167641429\n",
      "[Iter.  270]  loss:34.764820  pct:-2.137064330\n",
      "[Iter.  280]  loss:34.031982  pct:-2.107986392\n",
      "[Iter.  290]  loss:33.323936  pct:-2.080531045\n",
      "[Iter.  300]  loss:32.639431  pct:-2.054095450\n",
      "[Iter.  310]  loss:31.977144  pct:-2.029100196\n",
      "[Iter.  320]  loss:31.335972  pct:-2.005095903\n",
      "[Iter.  330]  loss:30.714849  pct:-1.982138494\n",
      "[Iter.  340]  loss:30.112804  pct:-1.960110727\n",
      "[Iter.  350]  loss:29.528908  pct:-1.939031081\n",
      "[Iter.  360]  loss:28.962349  pct:-1.918658293\n",
      "[Iter.  370]  loss:28.412252  pct:-1.899350474\n",
      "[Iter.  380]  loss:27.877953  pct:-1.880526199\n",
      "[Iter.  390]  loss:27.358696  pct:-1.862606626\n",
      "[Iter.  400]  loss:26.853855  pct:-1.845266496\n",
      "[Iter.  410]  loss:26.362879  pct:-1.828327185\n",
      "[Iter.  420]  loss:25.885132  pct:-1.812195728\n",
      "[Iter.  430]  loss:25.420132  pct:-1.796398626\n",
      "[Iter.  440]  loss:24.967356  pct:-1.781170770\n",
      "[Iter.  450]  loss:24.526300  pct:-1.766527872\n",
      "[Iter.  460]  loss:24.096588  pct:-1.752046937\n",
      "[Iter.  470]  loss:23.677700  pct:-1.738370966\n",
      "[Iter.  480]  loss:23.269299  pct:-1.724835979\n",
      "[Iter.  490]  loss:22.870968  pct:-1.711829377\n",
      "[Iter.  500]  loss:22.482382  pct:-1.699036292\n",
      "[Iter.  510]  loss:22.103170  pct:-1.686704855\n",
      "[Iter.  520]  loss:21.733011  pct:-1.674688031\n",
      "[Iter.  530]  loss:21.371662  pct:-1.662673901\n",
      "[Iter.  540]  loss:21.018732  pct:-1.651392702\n",
      "[Iter.  550]  loss:20.674057  pct:-1.639847080\n",
      "[Iter.  560]  loss:20.337276  pct:-1.629000771\n",
      "[Iter.  570]  loss:20.008163  pct:-1.618274734\n",
      "[Iter.  580]  loss:19.686522  pct:-1.607553451\n",
      "[Iter.  590]  loss:19.372040  pct:-1.597446937\n",
      "[Iter.  600]  loss:19.064529  pct:-1.587392857\n",
      "[Iter.  610]  loss:18.763813  pct:-1.577360729\n",
      "[Iter.  620]  loss:18.469666  pct:-1.567631756\n",
      "[Iter.  630]  loss:18.181911  pct:-1.557981970\n",
      "[Iter.  640]  loss:17.900356  pct:-1.548545522\n",
      "[Iter.  650]  loss:17.624821  pct:-1.539274297\n",
      "[Iter.  660]  loss:17.355133  pct:-1.530158275\n",
      "[Iter.  670]  loss:17.091085  pct:-1.521438192\n",
      "[Iter.  680]  loss:16.832603  pct:-1.512378956\n",
      "[Iter.  690]  loss:16.579489  pct:-1.503716885\n",
      "[Iter.  700]  loss:16.331570  pct:-1.495336113\n",
      "[Iter.  710]  loss:16.088757  pct:-1.486771420\n",
      "[Iter.  720]  loss:15.850912  pct:-1.478327217\n",
      "[Iter.  730]  loss:15.617875  pct:-1.470180350\n",
      "[Iter.  740]  loss:15.389550  pct:-1.461945935\n",
      "[Iter.  750]  loss:15.165796  pct:-1.453934170\n",
      "[Iter.  760]  loss:14.946499  pct:-1.445999966\n",
      "[Iter.  770]  loss:14.731556  pct:-1.438082149\n",
      "[Iter.  780]  loss:14.520847  pct:-1.430321543\n",
      "[Iter.  790]  loss:14.314299  pct:-1.422428638\n",
      "[Iter.  800]  loss:14.111767  pct:-1.414891640\n",
      "[Iter.  810]  loss:13.913159  pct:-1.407388936\n",
      "[Iter.  820]  loss:13.718391  pct:-1.399882994\n",
      "[Iter.  830]  loss:13.527410  pct:-1.392159322\n",
      "[Iter.  840]  loss:13.340064  pct:-1.384932599\n",
      "[Iter.  850]  loss:13.156326  pct:-1.377337876\n",
      "[Iter.  860]  loss:12.976093  pct:-1.369934111\n",
      "[Iter.  870]  loss:12.799274  pct:-1.362650866\n",
      "[Iter.  880]  loss:12.625785  pct:-1.355464104\n",
      "[Iter.  890]  loss:12.455543  pct:-1.348370111\n",
      "[Iter.  900]  loss:12.288491  pct:-1.341180558\n",
      "[Iter.  910]  loss:12.124599  pct:-1.333709262\n",
      "[Iter.  920]  loss:11.963747  pct:-1.326654062\n",
      "[Iter.  930]  loss:11.805875  pct:-1.319588250\n",
      "[Iter.  940]  loss:11.650922  pct:-1.312507588\n",
      "[Iter.  950]  loss:11.498806  pct:-1.305611901\n",
      "[Iter.  960]  loss:11.349492  pct:-1.298516791\n",
      "[Iter.  970]  loss:11.202915  pct:-1.291484063\n",
      "[Iter.  980]  loss:11.059008  pct:-1.284554462\n",
      "[Iter.  990]  loss:10.917705  pct:-1.277719186\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.773760  pct:100.000000000\n",
      "[Iter.    2]  loss:2.773829  pct:0.002475504\n",
      "[Iter.    4]  loss:2.773829  pct:0.000000000\n",
      "[Iter.    6]  loss:2.773829  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.773829\n",
      "Best loss: 2.773829 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  5%|▌         | 512/10000 [00:06<02:08, 74.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5359.464355  pct:100.000000000\n",
      "[Iter.   10]  loss:896.504089  pct:-83.272509214\n",
      "[Iter.   20]  loss:885.046265  pct:-1.278056045\n",
      "[Iter.   30]  loss:874.042542  pct:-1.243293552\n",
      "[Iter.   40]  loss:863.720581  pct:-1.180944858\n",
      "[Iter.   50]  loss:854.031250  pct:-1.121813150\n",
      "[Iter.   60]  loss:844.818481  pct:-1.078739046\n",
      "[Iter.   70]  loss:835.964905  pct:-1.047985674\n",
      "[Iter.   80]  loss:827.411682  pct:-1.023155710\n",
      "[Iter.   90]  loss:819.127075  pct:-1.001267823\n",
      "[Iter.  100]  loss:811.092163  pct:-0.980911552\n",
      "[Iter.  110]  loss:803.292786  pct:-0.961589545\n",
      "[Iter.  120]  loss:795.716980  pct:-0.943093950\n",
      "[Iter.  130]  loss:788.353394  pct:-0.925402701\n",
      "[Iter.  140]  loss:781.191956  pct:-0.908404536\n",
      "[Iter.  150]  loss:774.222839  pct:-0.892113156\n",
      "[Iter.  160]  loss:767.435242  pct:-0.876698195\n",
      "[Iter.  170]  loss:760.820435  pct:-0.861936847\n",
      "[Iter.  180]  loss:754.369873  pct:-0.847842833\n",
      "[Iter.  190]  loss:748.074829  pct:-0.834477114\n",
      "[Iter.  200]  loss:741.927185  pct:-0.821795334\n",
      "[Iter.  210]  loss:735.920349  pct:-0.809626074\n",
      "[Iter.  220]  loss:730.048279  pct:-0.797921992\n",
      "[Iter.  230]  loss:724.305603  pct:-0.786615892\n",
      "[Iter.  240]  loss:718.686462  pct:-0.775796929\n",
      "[Iter.  250]  loss:713.186340  pct:-0.765302028\n",
      "[Iter.  260]  loss:707.800903  pct:-0.755123410\n",
      "[Iter.  270]  loss:702.525452  pct:-0.745329885\n",
      "[Iter.  280]  loss:697.355957  pct:-0.735844462\n",
      "[Iter.  290]  loss:692.288757  pct:-0.726630303\n",
      "[Iter.  300]  loss:687.320007  pct:-0.717727964\n",
      "[Iter.  310]  loss:682.445984  pct:-0.709134520\n",
      "[Iter.  320]  loss:677.663696  pct:-0.700756941\n",
      "[Iter.  330]  loss:672.969543  pct:-0.692696519\n",
      "[Iter.  340]  loss:668.360718  pct:-0.684849074\n",
      "[Iter.  350]  loss:663.834717  pct:-0.677179382\n",
      "[Iter.  360]  loss:659.388489  pct:-0.669779377\n",
      "[Iter.  370]  loss:655.019592  pct:-0.662567903\n",
      "[Iter.  380]  loss:650.725098  pct:-0.655628424\n",
      "[Iter.  390]  loss:646.502930  pct:-0.648840499\n",
      "[Iter.  400]  loss:642.351318  pct:-0.642164349\n",
      "[Iter.  410]  loss:638.267578  pct:-0.635748712\n",
      "[Iter.  420]  loss:634.249817  pct:-0.629479135\n",
      "[Iter.  430]  loss:630.296814  pct:-0.623256456\n",
      "[Iter.  440]  loss:626.405884  pct:-0.617317126\n",
      "[Iter.  450]  loss:622.575378  pct:-0.611505331\n",
      "[Iter.  460]  loss:618.803650  pct:-0.605826804\n",
      "[Iter.  470]  loss:615.088806  pct:-0.600326735\n",
      "[Iter.  480]  loss:611.429504  pct:-0.594922509\n",
      "[Iter.  490]  loss:607.824097  pct:-0.589668586\n",
      "[Iter.  500]  loss:604.270874  pct:-0.584580749\n",
      "[Iter.  510]  loss:600.768311  pct:-0.579634668\n",
      "[Iter.  520]  loss:597.315552  pct:-0.574723854\n",
      "[Iter.  530]  loss:593.911560  pct:-0.569881646\n",
      "[Iter.  540]  loss:590.554565  pct:-0.565234768\n",
      "[Iter.  550]  loss:587.243408  pct:-0.560686077\n",
      "[Iter.  560]  loss:583.977173  pct:-0.556197874\n",
      "[Iter.  570]  loss:580.754272  pct:-0.551888077\n",
      "[Iter.  580]  loss:577.573730  pct:-0.547657098\n",
      "[Iter.  590]  loss:574.434753  pct:-0.543476423\n",
      "[Iter.  600]  loss:571.336243  pct:-0.539401686\n",
      "[Iter.  610]  loss:568.277039  pct:-0.535447233\n",
      "[Iter.  620]  loss:565.256409  pct:-0.531541779\n",
      "[Iter.  630]  loss:562.273438  pct:-0.527720013\n",
      "[Iter.  640]  loss:559.327515  pct:-0.523930646\n",
      "[Iter.  650]  loss:556.417358  pct:-0.520295565\n",
      "[Iter.  660]  loss:553.542358  pct:-0.516698474\n",
      "[Iter.  670]  loss:550.701782  pct:-0.513163289\n",
      "[Iter.  680]  loss:547.895142  pct:-0.509648001\n",
      "[Iter.  690]  loss:545.121094  pct:-0.506309993\n",
      "[Iter.  700]  loss:542.379211  pct:-0.502985916\n",
      "[Iter.  710]  loss:539.668945  pct:-0.499699483\n",
      "[Iter.  720]  loss:536.989624  pct:-0.496474980\n",
      "[Iter.  730]  loss:534.340332  pct:-0.493359997\n",
      "[Iter.  740]  loss:531.721069  pct:-0.490186224\n",
      "[Iter.  750]  loss:529.130554  pct:-0.487194374\n",
      "[Iter.  760]  loss:526.568542  pct:-0.484192738\n",
      "[Iter.  770]  loss:524.034180  pct:-0.481297797\n",
      "[Iter.  780]  loss:521.527161  pct:-0.478407543\n",
      "[Iter.  790]  loss:519.046814  pct:-0.475593002\n",
      "[Iter.  800]  loss:516.592529  pct:-0.472844569\n",
      "[Iter.  810]  loss:514.164001  pct:-0.470105101\n",
      "[Iter.  820]  loss:511.760620  pct:-0.467434776\n",
      "[Iter.  830]  loss:509.381989  pct:-0.464793792\n",
      "[Iter.  840]  loss:507.027618  pct:-0.462201289\n",
      "[Iter.  850]  loss:504.697418  pct:-0.459580526\n",
      "[Iter.  860]  loss:502.390350  pct:-0.457119016\n",
      "[Iter.  870]  loss:500.105927  pct:-0.454710929\n",
      "[Iter.  880]  loss:497.844543  pct:-0.452180815\n",
      "[Iter.  890]  loss:495.605164  pct:-0.449815090\n",
      "[Iter.  900]  loss:493.387360  pct:-0.447494118\n",
      "[Iter.  910]  loss:491.191071  pct:-0.445144980\n",
      "[Iter.  920]  loss:489.015442  pct:-0.442929197\n",
      "[Iter.  930]  loss:486.860474  pct:-0.440674890\n",
      "[Iter.  940]  loss:484.725708  pct:-0.438475855\n",
      "[Iter.  950]  loss:482.610779  pct:-0.436314634\n",
      "[Iter.  960]  loss:480.515442  pct:-0.434167036\n",
      "[Iter.  970]  loss:478.439178  pct:-0.432090885\n",
      "[Iter.  980]  loss:476.382050  pct:-0.429966650\n",
      "[Iter.  990]  loss:474.343506  pct:-0.427922022\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.892188  pct:100.000000000\n",
      "[Iter.    2]  loss:2.891250  pct:-0.032430070\n",
      "[Iter.    4]  loss:2.890719  pct:-0.018380802\n",
      "[Iter.    6]  loss:2.890335  pct:-0.013287087\n",
      "[Iter.    8]  loss:2.890104  pct:-0.008001357\n",
      "[Iter.   10]  loss:2.889925  pct:-0.006162363\n",
      "[Iter.   12]  loss:2.889815  pct:-0.003836246\n",
      "[Iter.   14]  loss:2.889732  pct:-0.002862857\n",
      "[Iter.   16]  loss:2.889632  pct:-0.003456978\n",
      "[Iter.   18]  loss:2.889610  pct:-0.000775578\n",
      "[Iter.   20]  loss:2.889541  pct:-0.002384508\n",
      "[Iter.   22]  loss:2.889505  pct:-0.001221161\n",
      "[Iter.   24]  loss:2.889473  pct:-0.001130413\n",
      "[Iter.   26]  loss:2.889450  pct:-0.000800374\n",
      "[Iter.   28]  loss:2.889404  pct:-0.001576008\n",
      "[Iter.   30]  loss:2.889393  pct:-0.000396071\n",
      "[Iter.   32]  loss:2.889362  pct:-0.001064445\n",
      "[Iter.   34]  loss:2.889331  pct:-0.001064456\n",
      "[Iter.   36]  loss:2.889316  pct:-0.000511605\n",
      "[Iter.   38]  loss:2.889292  pct:-0.000833425\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.889292\n",
      "Best loss: 2.889292 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 16%|█▌        | 1588/10000 [00:31<02:44, 51.02it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:329.511688  pct:100.000000000\n",
      "[Iter.   10]  loss:64.852058  pct:-80.318740398\n",
      "[Iter.   20]  loss:60.419876  pct:-6.834297046\n",
      "[Iter.   30]  loss:57.096737  pct:-5.500076143\n",
      "[Iter.   40]  loss:54.206970  pct:-5.061176609\n",
      "[Iter.   50]  loss:51.649723  pct:-4.717561509\n",
      "[Iter.   60]  loss:49.359211  pct:-4.434703517\n",
      "[Iter.   70]  loss:47.287167  pct:-4.197887956\n",
      "[Iter.   80]  loss:45.399048  pct:-3.992877729\n",
      "[Iter.   90]  loss:43.667469  pct:-3.814130271\n",
      "[Iter.  100]  loss:42.069347  pct:-3.659753310\n",
      "[Iter.  110]  loss:40.586689  pct:-3.524319911\n",
      "[Iter.  120]  loss:39.205044  pct:-3.404183088\n",
      "[Iter.  130]  loss:37.912209  pct:-3.297624771\n",
      "[Iter.  140]  loss:36.697906  pct:-3.202931481\n",
      "[Iter.  150]  loss:35.553406  pct:-3.118708509\n",
      "[Iter.  160]  loss:34.471500  pct:-3.043042830\n",
      "[Iter.  170]  loss:33.446419  pct:-2.973707621\n",
      "[Iter.  180]  loss:32.473804  pct:-2.907977369\n",
      "[Iter.  190]  loss:31.550016  pct:-2.844717721\n",
      "[Iter.  200]  loss:30.671251  pct:-2.785307922\n",
      "[Iter.  210]  loss:29.833389  pct:-2.731750350\n",
      "[Iter.  220]  loss:29.032362  pct:-2.685002667\n",
      "[Iter.  230]  loss:28.264582  pct:-2.644567136\n",
      "[Iter.  240]  loss:27.526720  pct:-2.610552110\n",
      "[Iter.  250]  loss:26.816839  pct:-2.578879095\n",
      "[Iter.  260]  loss:26.138336  pct:-2.530137989\n",
      "[Iter.  270]  loss:25.489820  pct:-2.481090215\n",
      "[Iter.  280]  loss:24.868658  pct:-2.436903842\n",
      "[Iter.  290]  loss:24.273186  pct:-2.394469111\n",
      "[Iter.  300]  loss:23.699991  pct:-2.361430882\n",
      "[Iter.  310]  loss:23.148012  pct:-2.329026453\n",
      "[Iter.  320]  loss:22.613541  pct:-2.308930495\n",
      "[Iter.  330]  loss:22.099981  pct:-2.271025796\n",
      "[Iter.  340]  loss:21.605631  pct:-2.236881681\n",
      "[Iter.  350]  loss:21.126503  pct:-2.217606543\n",
      "[Iter.  360]  loss:20.662771  pct:-2.195023786\n",
      "[Iter.  370]  loss:20.215775  pct:-2.163294962\n",
      "[Iter.  380]  loss:19.783579  pct:-2.137912958\n",
      "[Iter.  390]  loss:19.365883  pct:-2.111326782\n",
      "[Iter.  400]  loss:18.961212  pct:-2.089606335\n",
      "[Iter.  410]  loss:18.569084  pct:-2.068053389\n",
      "[Iter.  420]  loss:18.188730  pct:-2.048318184\n",
      "[Iter.  430]  loss:17.819715  pct:-2.028809791\n",
      "[Iter.  440]  loss:17.461653  pct:-2.009362855\n",
      "[Iter.  450]  loss:17.114054  pct:-1.990642206\n",
      "[Iter.  460]  loss:16.776472  pct:-1.972540462\n",
      "[Iter.  470]  loss:16.448515  pct:-1.954863642\n",
      "[Iter.  480]  loss:16.129816  pct:-1.937554145\n",
      "[Iter.  490]  loss:15.819998  pct:-1.920779919\n",
      "[Iter.  500]  loss:15.518732  pct:-1.904334758\n",
      "[Iter.  510]  loss:15.225677  pct:-1.888392553\n",
      "[Iter.  520]  loss:14.940592  pct:-1.872400609\n",
      "[Iter.  530]  loss:14.663112  pct:-1.857223120\n",
      "[Iter.  540]  loss:14.393015  pct:-1.842015424\n",
      "[Iter.  550]  loss:14.130028  pct:-1.827185885\n",
      "[Iter.  560]  loss:13.873902  pct:-1.812632320\n",
      "[Iter.  570]  loss:13.624423  pct:-1.798191223\n",
      "[Iter.  580]  loss:13.381355  pct:-1.784058972\n",
      "[Iter.  590]  loss:13.144477  pct:-1.770212284\n",
      "[Iter.  600]  loss:12.913606  pct:-1.756412237\n",
      "[Iter.  610]  loss:12.688542  pct:-1.742838750\n",
      "[Iter.  620]  loss:12.469091  pct:-1.729520573\n",
      "[Iter.  630]  loss:12.255074  pct:-1.716387032\n",
      "[Iter.  640]  loss:12.046365  pct:-1.703039662\n",
      "[Iter.  650]  loss:11.842747  pct:-1.690286267\n",
      "[Iter.  660]  loss:11.644073  pct:-1.677594335\n",
      "[Iter.  670]  loss:11.450214  pct:-1.664873556\n",
      "[Iter.  680]  loss:11.261034  pct:-1.652199407\n",
      "[Iter.  690]  loss:11.076389  pct:-1.639678016\n",
      "[Iter.  700]  loss:10.896141  pct:-1.627319656\n",
      "[Iter.  710]  loss:10.720159  pct:-1.615089915\n",
      "[Iter.  720]  loss:10.548337  pct:-1.602789670\n",
      "[Iter.  730]  loss:10.380554  pct:-1.590608868\n",
      "[Iter.  740]  loss:10.216696  pct:-1.578513156\n",
      "[Iter.  750]  loss:10.056643  pct:-1.566575954\n",
      "[Iter.  760]  loss:9.900294  pct:-1.554685540\n",
      "[Iter.  770]  loss:9.747555  pct:-1.542777620\n",
      "[Iter.  780]  loss:9.598305  pct:-1.531153545\n",
      "[Iter.  790]  loss:9.452519  pct:-1.518865420\n",
      "[Iter.  800]  loss:9.310015  pct:-1.507584230\n",
      "[Iter.  810]  loss:9.170761  pct:-1.495740022\n",
      "[Iter.  820]  loss:9.034668  pct:-1.483989584\n",
      "[Iter.  830]  loss:8.901649  pct:-1.472322478\n",
      "[Iter.  840]  loss:8.771616  pct:-1.460769194\n",
      "[Iter.  850]  loss:8.644501  pct:-1.449165694\n",
      "[Iter.  860]  loss:8.520240  pct:-1.437456092\n",
      "[Iter.  870]  loss:8.398745  pct:-1.425960411\n",
      "[Iter.  880]  loss:8.279939  pct:-1.414567191\n",
      "[Iter.  890]  loss:8.163768  pct:-1.403040378\n",
      "[Iter.  900]  loss:8.050172  pct:-1.391464886\n",
      "[Iter.  910]  loss:7.939063  pct:-1.380209749\n",
      "[Iter.  920]  loss:7.830405  pct:-1.368642189\n",
      "[Iter.  930]  loss:7.724137  pct:-1.357125241\n",
      "[Iter.  940]  loss:7.620202  pct:-1.345590375\n",
      "[Iter.  950]  loss:7.518520  pct:-1.334370333\n",
      "[Iter.  960]  loss:7.419078  pct:-1.322621073\n",
      "[Iter.  970]  loss:7.321789  pct:-1.311342968\n",
      "[Iter.  980]  loss:7.226608  pct:-1.299969052\n",
      "[Iter.  990]  loss:7.133478  pct:-1.288711305\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.773465  pct:100.000000000\n",
      "[Iter.    2]  loss:2.773526  pct:0.002200682\n",
      "[Iter.    4]  loss:2.773525  pct:-0.000017192\n",
      "[Iter.    6]  loss:2.773526  pct:0.000008596\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.773526\n",
      "Best loss: 2.773526 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  2%|▏         | 194/10000 [00:03<02:57, 55.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:280.467896  pct:100.000000000\n",
      "[Iter.   10]  loss:61.551235  pct:-78.054087473\n",
      "[Iter.   20]  loss:55.544838  pct:-9.758369963\n",
      "[Iter.   30]  loss:52.074680  pct:-6.247488968\n",
      "[Iter.   40]  loss:49.101353  pct:-5.709737665\n",
      "[Iter.   50]  loss:46.495075  pct:-5.307954512\n",
      "[Iter.   60]  loss:44.190559  pct:-4.956472976\n",
      "[Iter.   70]  loss:42.135071  pct:-4.651420156\n",
      "[Iter.   80]  loss:40.284504  pct:-4.391987076\n",
      "[Iter.   90]  loss:38.603523  pct:-4.172772451\n",
      "[Iter.  100]  loss:37.064022  pct:-3.987981045\n",
      "[Iter.  110]  loss:35.644077  pct:-3.831059567\n",
      "[Iter.  120]  loss:34.328609  pct:-3.690564980\n",
      "[Iter.  130]  loss:33.105972  pct:-3.561569185\n",
      "[Iter.  140]  loss:31.965399  pct:-3.445219768\n",
      "[Iter.  150]  loss:30.897243  pct:-3.341598507\n",
      "[Iter.  160]  loss:29.893599  pct:-3.248331662\n",
      "[Iter.  170]  loss:28.947828  pct:-3.163788601\n",
      "[Iter.  180]  loss:28.054399  pct:-3.086341378\n",
      "[Iter.  190]  loss:27.208582  pct:-3.014919518\n",
      "[Iter.  200]  loss:26.406176  pct:-2.949092728\n",
      "[Iter.  210]  loss:25.643629  pct:-2.887758343\n",
      "[Iter.  220]  loss:24.917700  pct:-2.830836689\n",
      "[Iter.  230]  loss:24.225561  pct:-2.777698893\n",
      "[Iter.  240]  loss:23.564747  pct:-2.727756362\n",
      "[Iter.  250]  loss:22.933004  pct:-2.680879541\n",
      "[Iter.  260]  loss:22.328346  pct:-2.636628489\n",
      "[Iter.  270]  loss:21.748953  pct:-2.594878189\n",
      "[Iter.  280]  loss:21.193218  pct:-2.555224786\n",
      "[Iter.  290]  loss:20.659636  pct:-2.517704869\n",
      "[Iter.  300]  loss:20.146893  pct:-2.481858865\n",
      "[Iter.  310]  loss:19.653748  pct:-2.447747154\n",
      "[Iter.  320]  loss:19.179070  pct:-2.415203707\n",
      "[Iter.  330]  loss:18.721834  pct:-2.384032947\n",
      "[Iter.  340]  loss:18.281059  pct:-2.354336190\n",
      "[Iter.  350]  loss:17.855953  pct:-2.325390681\n",
      "[Iter.  360]  loss:17.445700  pct:-2.297572803\n",
      "[Iter.  370]  loss:17.049486  pct:-2.271124337\n",
      "[Iter.  380]  loss:16.666664  pct:-2.245358207\n",
      "[Iter.  390]  loss:16.296579  pct:-2.220508914\n",
      "[Iter.  400]  loss:15.938651  pct:-2.196339907\n",
      "[Iter.  410]  loss:15.592301  pct:-2.173017744\n",
      "[Iter.  420]  loss:15.257019  pct:-2.150306859\n",
      "[Iter.  430]  loss:14.932312  pct:-2.128246877\n",
      "[Iter.  440]  loss:14.617709  pct:-2.106859618\n",
      "[Iter.  450]  loss:14.312791  pct:-2.085951265\n",
      "[Iter.  460]  loss:14.017167  pct:-2.065451679\n",
      "[Iter.  470]  loss:13.730426  pct:-2.045643423\n",
      "[Iter.  480]  loss:13.452239  pct:-2.026060965\n",
      "[Iter.  490]  loss:13.182261  pct:-2.006934079\n",
      "[Iter.  500]  loss:12.920179  pct:-1.988142176\n",
      "[Iter.  510]  loss:12.665681  pct:-1.969775144\n",
      "[Iter.  520]  loss:12.418491  pct:-1.951648111\n",
      "[Iter.  530]  loss:12.178340  pct:-1.933821092\n",
      "[Iter.  540]  loss:11.944962  pct:-1.916340085\n",
      "[Iter.  550]  loss:11.718111  pct:-1.899131351\n",
      "[Iter.  560]  loss:11.497566  pct:-1.882085042\n",
      "[Iter.  570]  loss:11.283081  pct:-1.865483219\n",
      "[Iter.  580]  loss:11.074490  pct:-1.848710119\n",
      "[Iter.  590]  loss:10.871557  pct:-1.832430796\n",
      "[Iter.  600]  loss:10.674098  pct:-1.816291968\n",
      "[Iter.  610]  loss:10.481942  pct:-1.800206797\n",
      "[Iter.  620]  loss:10.294901  pct:-1.784414372\n",
      "[Iter.  630]  loss:10.112825  pct:-1.768598866\n",
      "[Iter.  640]  loss:9.935543  pct:-1.753044540\n",
      "[Iter.  650]  loss:9.762904  pct:-1.737588898\n",
      "[Iter.  660]  loss:9.594766  pct:-1.722218114\n",
      "[Iter.  670]  loss:9.430982  pct:-1.707014354\n",
      "[Iter.  680]  loss:9.271422  pct:-1.691862587\n",
      "[Iter.  690]  loss:9.115977  pct:-1.676604650\n",
      "[Iter.  700]  loss:8.964497  pct:-1.661705267\n",
      "[Iter.  710]  loss:8.816870  pct:-1.646794942\n",
      "[Iter.  720]  loss:8.672987  pct:-1.631902884\n",
      "[Iter.  730]  loss:8.532721  pct:-1.617279245\n",
      "[Iter.  740]  loss:8.395982  pct:-1.602522620\n",
      "[Iter.  750]  loss:8.262676  pct:-1.587730333\n",
      "[Iter.  760]  loss:8.132692  pct:-1.573145289\n",
      "[Iter.  770]  loss:8.005938  pct:-1.558583007\n",
      "[Iter.  780]  loss:7.882325  pct:-1.544009089\n",
      "[Iter.  790]  loss:7.761768  pct:-1.529457726\n",
      "[Iter.  800]  loss:7.644176  pct:-1.515026041\n",
      "[Iter.  810]  loss:7.529457  pct:-1.500730023\n",
      "[Iter.  820]  loss:7.417549  pct:-1.486274966\n",
      "[Iter.  830]  loss:7.308375  pct:-1.471830921\n",
      "[Iter.  840]  loss:7.201861  pct:-1.457417031\n",
      "[Iter.  850]  loss:7.097946  pct:-1.442893844\n",
      "[Iter.  860]  loss:6.996533  pct:-1.428768937\n",
      "[Iter.  870]  loss:6.897577  pct:-1.414359216\n",
      "[Iter.  880]  loss:6.801002  pct:-1.400126247\n",
      "[Iter.  890]  loss:6.706741  pct:-1.385982422\n",
      "[Iter.  900]  loss:6.614737  pct:-1.371825324\n",
      "[Iter.  910]  loss:6.524923  pct:-1.357774896\n",
      "[Iter.  920]  loss:6.437273  pct:-1.343315388\n",
      "[Iter.  930]  loss:6.351723  pct:-1.328976277\n",
      "[Iter.  940]  loss:6.268207  pct:-1.314857691\n",
      "[Iter.  950]  loss:6.186649  pct:-1.301133638\n",
      "[Iter.  960]  loss:6.107028  pct:-1.286978363\n",
      "[Iter.  970]  loss:6.029301  pct:-1.272751847\n",
      "[Iter.  980]  loss:5.953406  pct:-1.258774564\n",
      "[Iter.  990]  loss:5.879298  pct:-1.244794135\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.730177  pct:100.000000000\n",
      "[Iter.    2]  loss:2.730226  pct:0.001781474\n",
      "[Iter.    4]  loss:2.730222  pct:-0.000157186\n",
      "[Iter.    6]  loss:2.730221  pct:-0.000043663\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.730221\n",
      "Best loss: 2.730221 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 33%|███▎      | 3274/10000 [00:44<01:31, 73.61it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:623.263672  pct:100.000000000\n",
      "[Iter.   10]  loss:104.343445  pct:-83.258538957\n",
      "[Iter.   20]  loss:99.790649  pct:-4.363278803\n",
      "[Iter.   30]  loss:95.824806  pct:-3.974163134\n",
      "[Iter.   40]  loss:92.211143  pct:-3.771114039\n",
      "[Iter.   50]  loss:88.923973  pct:-3.564829895\n",
      "[Iter.   60]  loss:85.924660  pct:-3.372896251\n",
      "[Iter.   70]  loss:83.162308  pct:-3.214853569\n",
      "[Iter.   80]  loss:80.601425  pct:-3.079378913\n",
      "[Iter.   90]  loss:78.215034  pct:-2.960730137\n",
      "[Iter.  100]  loss:75.981804  pct:-2.855244654\n",
      "[Iter.  110]  loss:73.884247  pct:-2.760604461\n",
      "[Iter.  120]  loss:71.907951  pct:-2.674853648\n",
      "[Iter.  130]  loss:70.040962  pct:-2.596359791\n",
      "[Iter.  140]  loss:68.272644  pct:-2.524691438\n",
      "[Iter.  150]  loss:66.593758  pct:-2.459090954\n",
      "[Iter.  160]  loss:64.996292  pct:-2.398821709\n",
      "[Iter.  170]  loss:63.473347  pct:-2.343126592\n",
      "[Iter.  180]  loss:62.018867  pct:-2.291480272\n",
      "[Iter.  190]  loss:60.627598  pct:-2.243300692\n",
      "[Iter.  200]  loss:59.294769  pct:-2.198385834\n",
      "[Iter.  210]  loss:58.016243  pct:-2.156221066\n",
      "[Iter.  220]  loss:56.788223  pct:-2.116682590\n",
      "[Iter.  230]  loss:55.607281  pct:-2.079555351\n",
      "[Iter.  240]  loss:54.470341  pct:-2.044588384\n",
      "[Iter.  250]  loss:53.374550  pct:-2.011720229\n",
      "[Iter.  260]  loss:52.317509  pct:-1.980421701\n",
      "[Iter.  270]  loss:51.296856  pct:-1.950881830\n",
      "[Iter.  280]  loss:50.310440  pct:-1.922955794\n",
      "[Iter.  290]  loss:49.356403  pct:-1.896299677\n",
      "[Iter.  300]  loss:48.432873  pct:-1.871146429\n",
      "[Iter.  310]  loss:47.538353  pct:-1.846927004\n",
      "[Iter.  320]  loss:46.671177  pct:-1.824160918\n",
      "[Iter.  330]  loss:45.830048  pct:-1.802245751\n",
      "[Iter.  340]  loss:45.013721  pct:-1.781202909\n",
      "[Iter.  350]  loss:44.220997  pct:-1.761073254\n",
      "[Iter.  360]  loss:43.450722  pct:-1.741876418\n",
      "[Iter.  370]  loss:42.701931  pct:-1.723310249\n",
      "[Iter.  380]  loss:41.973621  pct:-1.705566035\n",
      "[Iter.  390]  loss:41.264805  pct:-1.688719022\n",
      "[Iter.  400]  loss:40.574749  pct:-1.672262476\n",
      "[Iter.  410]  loss:39.902618  pct:-1.656524320\n",
      "[Iter.  420]  loss:39.247723  pct:-1.641235108\n",
      "[Iter.  430]  loss:38.609268  pct:-1.626729896\n",
      "[Iter.  440]  loss:37.986671  pct:-1.612557735\n",
      "[Iter.  450]  loss:37.379314  pct:-1.598868766\n",
      "[Iter.  460]  loss:36.786621  pct:-1.585618511\n",
      "[Iter.  470]  loss:36.207970  pct:-1.572994233\n",
      "[Iter.  480]  loss:35.642883  pct:-1.560668466\n",
      "[Iter.  490]  loss:35.090958  pct:-1.548487687\n",
      "[Iter.  500]  loss:34.551586  pct:-1.537066888\n",
      "[Iter.  510]  loss:34.024433  pct:-1.525698452\n",
      "[Iter.  520]  loss:33.509026  pct:-1.514816015\n",
      "[Iter.  530]  loss:33.004959  pct:-1.504270741\n",
      "[Iter.  540]  loss:32.511856  pct:-1.494027082\n",
      "[Iter.  550]  loss:32.029369  pct:-1.484033159\n",
      "[Iter.  560]  loss:31.557159  pct:-1.474302929\n",
      "[Iter.  570]  loss:31.094887  pct:-1.464874065\n",
      "[Iter.  580]  loss:30.642282  pct:-1.455558584\n",
      "[Iter.  590]  loss:30.199045  pct:-1.446489193\n",
      "[Iter.  600]  loss:29.764845  pct:-1.437794752\n",
      "[Iter.  610]  loss:29.339443  pct:-1.429208481\n",
      "[Iter.  620]  loss:28.922626  pct:-1.420670149\n",
      "[Iter.  630]  loss:28.513988  pct:-1.412866154\n",
      "[Iter.  640]  loss:28.113476  pct:-1.404618282\n",
      "[Iter.  650]  loss:27.720760  pct:-1.396893991\n",
      "[Iter.  660]  loss:27.335600  pct:-1.389429588\n",
      "[Iter.  670]  loss:26.957838  pct:-1.381940920\n",
      "[Iter.  680]  loss:26.587282  pct:-1.374575650\n",
      "[Iter.  690]  loss:26.223749  pct:-1.367319223\n",
      "[Iter.  700]  loss:25.866983  pct:-1.360468120\n",
      "[Iter.  710]  loss:25.516880  pct:-1.353475868\n",
      "[Iter.  720]  loss:25.173239  pct:-1.346721389\n",
      "[Iter.  730]  loss:24.835911  pct:-1.340026051\n",
      "[Iter.  740]  loss:24.504719  pct:-1.333520720\n",
      "[Iter.  750]  loss:24.179512  pct:-1.327118909\n",
      "[Iter.  760]  loss:23.860100  pct:-1.321003630\n",
      "[Iter.  770]  loss:23.546431  pct:-1.314618159\n",
      "[Iter.  780]  loss:23.238287  pct:-1.308663810\n",
      "[Iter.  790]  loss:22.935528  pct:-1.302846337\n",
      "[Iter.  800]  loss:22.637980  pct:-1.297324817\n",
      "[Iter.  810]  loss:22.345709  pct:-1.291063367\n",
      "[Iter.  820]  loss:22.058458  pct:-1.285484031\n",
      "[Iter.  830]  loss:21.776131  pct:-1.279906545\n",
      "[Iter.  840]  loss:21.498617  pct:-1.274393087\n",
      "[Iter.  850]  loss:21.225805  pct:-1.268974127\n",
      "[Iter.  860]  loss:20.957603  pct:-1.263564913\n",
      "[Iter.  870]  loss:20.693863  pct:-1.258447991\n",
      "[Iter.  880]  loss:20.434526  pct:-1.253204743\n",
      "[Iter.  890]  loss:20.179493  pct:-1.248051888\n",
      "[Iter.  900]  loss:19.928707  pct:-1.242775665\n",
      "[Iter.  910]  loss:19.681997  pct:-1.237962012\n",
      "[Iter.  920]  loss:19.439320  pct:-1.232993201\n",
      "[Iter.  930]  loss:19.200613  pct:-1.227957529\n",
      "[Iter.  940]  loss:18.965740  pct:-1.223256871\n",
      "[Iter.  950]  loss:18.734667  pct:-1.218372587\n",
      "[Iter.  960]  loss:18.507322  pct:-1.213496429\n",
      "[Iter.  970]  loss:18.283590  pct:-1.208883656\n",
      "[Iter.  980]  loss:18.063389  pct:-1.204366804\n",
      "[Iter.  990]  loss:17.846724  pct:-1.199471871\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.729117  pct:100.000000000\n",
      "[Iter.    2]  loss:2.729051  pct:-0.002411166\n",
      "[Iter.    4]  loss:2.729027  pct:-0.000891104\n",
      "[Iter.    6]  loss:2.729017  pct:-0.000340719\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.729017\n",
      "Best loss: 2.729017 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  2%|▏         | 235/10000 [00:05<03:30, 46.33it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:404.233124  pct:100.000000000\n",
      "[Iter.   10]  loss:80.929939  pct:-79.979391376\n",
      "[Iter.   20]  loss:75.668991  pct:-6.500620449\n",
      "[Iter.   30]  loss:71.666397  pct:-5.289609306\n",
      "[Iter.   40]  loss:68.179420  pct:-4.865567079\n",
      "[Iter.   50]  loss:65.118759  pct:-4.489127797\n",
      "[Iter.   60]  loss:62.380024  pct:-4.205754585\n",
      "[Iter.   70]  loss:59.908192  pct:-3.962538195\n",
      "[Iter.   80]  loss:57.659031  pct:-3.754345948\n",
      "[Iter.   90]  loss:55.594856  pct:-3.579967647\n",
      "[Iter.  100]  loss:53.686638  pct:-3.432364992\n",
      "[Iter.  110]  loss:51.914993  pct:-3.299973070\n",
      "[Iter.  120]  loss:50.264084  pct:-3.180024342\n",
      "[Iter.  130]  loss:48.718689  pct:-3.074551009\n",
      "[Iter.  140]  loss:47.267120  pct:-2.979490283\n",
      "[Iter.  150]  loss:45.899593  pct:-2.893188749\n",
      "[Iter.  160]  loss:44.607754  pct:-2.814490293\n",
      "[Iter.  170]  loss:43.384392  pct:-2.742487272\n",
      "[Iter.  180]  loss:42.223385  pct:-2.676093590\n",
      "[Iter.  190]  loss:41.119301  pct:-2.614863822\n",
      "[Iter.  200]  loss:40.067432  pct:-2.558089309\n",
      "[Iter.  210]  loss:39.063610  pct:-2.505332302\n",
      "[Iter.  220]  loss:38.104126  pct:-2.456209496\n",
      "[Iter.  230]  loss:37.185722  pct:-2.410247190\n",
      "[Iter.  240]  loss:36.305477  pct:-2.367159095\n",
      "[Iter.  250]  loss:35.460735  pct:-2.326761381\n",
      "[Iter.  260]  loss:34.649151  pct:-2.288684838\n",
      "[Iter.  270]  loss:33.868599  pct:-2.252730273\n",
      "[Iter.  280]  loss:33.117157  pct:-2.218698084\n",
      "[Iter.  290]  loss:32.393047  pct:-2.186509096\n",
      "[Iter.  300]  loss:31.694685  pct:-2.155901985\n",
      "[Iter.  310]  loss:31.020550  pct:-2.126966110\n",
      "[Iter.  320]  loss:30.369303  pct:-2.099405166\n",
      "[Iter.  330]  loss:29.739769  pct:-2.072927959\n",
      "[Iter.  340]  loss:29.130756  pct:-2.047805429\n",
      "[Iter.  350]  loss:28.541224  pct:-2.023747151\n",
      "[Iter.  360]  loss:27.970181  pct:-2.000765714\n",
      "[Iter.  370]  loss:27.416748  pct:-1.978651744\n",
      "[Iter.  380]  loss:26.880068  pct:-1.957490438\n",
      "[Iter.  390]  loss:26.359364  pct:-1.937138971\n",
      "[Iter.  400]  loss:25.853943  pct:-1.917423703\n",
      "[Iter.  410]  loss:25.363054  pct:-1.898699158\n",
      "[Iter.  420]  loss:24.886129  pct:-1.880392208\n",
      "[Iter.  430]  loss:24.422518  pct:-1.862931739\n",
      "[Iter.  440]  loss:23.971693  pct:-1.845938824\n",
      "[Iter.  450]  loss:23.533098  pct:-1.829636386\n",
      "[Iter.  460]  loss:23.106289  pct:-1.813655418\n",
      "[Iter.  470]  loss:22.690775  pct:-1.798272297\n",
      "[Iter.  480]  loss:22.286102  pct:-1.783423546\n",
      "[Iter.  490]  loss:21.891909  pct:-1.768786861\n",
      "[Iter.  500]  loss:21.507784  pct:-1.754642604\n",
      "[Iter.  510]  loss:21.133358  pct:-1.740885486\n",
      "[Iter.  520]  loss:20.768261  pct:-1.727586529\n",
      "[Iter.  530]  loss:20.412193  pct:-1.714479889\n",
      "[Iter.  540]  loss:20.064806  pct:-1.701861768\n",
      "[Iter.  550]  loss:19.725811  pct:-1.689500412\n",
      "[Iter.  560]  loss:19.394903  pct:-1.677537221\n",
      "[Iter.  570]  loss:19.071819  pct:-1.665818460\n",
      "[Iter.  580]  loss:18.756359  pct:-1.654064565\n",
      "[Iter.  590]  loss:18.448225  pct:-1.642824587\n",
      "[Iter.  600]  loss:18.147139  pct:-1.632061759\n",
      "[Iter.  610]  loss:17.852991  pct:-1.620902876\n",
      "[Iter.  620]  loss:17.565514  pct:-1.610248342\n",
      "[Iter.  630]  loss:17.284513  pct:-1.599726279\n",
      "[Iter.  640]  loss:17.009758  pct:-1.589604928\n",
      "[Iter.  650]  loss:16.741072  pct:-1.579600925\n",
      "[Iter.  660]  loss:16.478319  pct:-1.569508438\n",
      "[Iter.  670]  loss:16.221281  pct:-1.559856402\n",
      "[Iter.  680]  loss:15.969834  pct:-1.550103985\n",
      "[Iter.  690]  loss:15.723801  pct:-1.540615034\n",
      "[Iter.  700]  loss:15.483052  pct:-1.531108227\n",
      "[Iter.  710]  loss:15.247387  pct:-1.522085681\n",
      "[Iter.  720]  loss:15.016704  pct:-1.512936792\n",
      "[Iter.  730]  loss:14.790833  pct:-1.504132279\n",
      "[Iter.  740]  loss:14.569695  pct:-1.495101781\n",
      "[Iter.  750]  loss:14.353140  pct:-1.486336185\n",
      "[Iter.  760]  loss:14.141040  pct:-1.477725646\n",
      "[Iter.  770]  loss:13.933272  pct:-1.469251829\n",
      "[Iter.  780]  loss:13.729727  pct:-1.460859769\n",
      "[Iter.  790]  loss:13.530332  pct:-1.452288037\n",
      "[Iter.  800]  loss:13.334939  pct:-1.444108055\n",
      "[Iter.  810]  loss:13.143490  pct:-1.435695846\n",
      "[Iter.  820]  loss:12.955849  pct:-1.427635629\n",
      "[Iter.  830]  loss:12.771952  pct:-1.419413137\n",
      "[Iter.  840]  loss:12.591711  pct:-1.411222307\n",
      "[Iter.  850]  loss:12.414990  pct:-1.403467873\n",
      "[Iter.  860]  loss:12.241774  pct:-1.395223144\n",
      "[Iter.  870]  loss:12.071925  pct:-1.387449626\n",
      "[Iter.  880]  loss:11.905394  pct:-1.379494658\n",
      "[Iter.  890]  loss:11.742118  pct:-1.371443265\n",
      "[Iter.  900]  loss:11.581969  pct:-1.363881901\n",
      "[Iter.  910]  loss:11.424905  pct:-1.356111679\n",
      "[Iter.  920]  loss:11.270827  pct:-1.348611059\n",
      "[Iter.  930]  loss:11.119735  pct:-1.340562901\n",
      "[Iter.  940]  loss:10.971503  pct:-1.333048939\n",
      "[Iter.  950]  loss:10.826080  pct:-1.325460441\n",
      "[Iter.  960]  loss:10.683423  pct:-1.317718655\n",
      "[Iter.  970]  loss:10.543457  pct:-1.310123268\n",
      "[Iter.  980]  loss:10.406118  pct:-1.302595893\n",
      "[Iter.  990]  loss:10.271367  pct:-1.294923955\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.749406  pct:100.000000000\n",
      "[Iter.    2]  loss:2.749430  pct:0.000867164\n",
      "[Iter.    4]  loss:2.749428  pct:-0.000069373\n",
      "[Iter.    6]  loss:2.749428  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.749428\n",
      "Best loss: 2.749428 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1827/10000 [00:36<02:44, 49.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:662.279236  pct:100.000000000\n",
      "[Iter.   10]  loss:105.718307  pct:-84.037198053\n",
      "[Iter.   20]  loss:99.811676  pct:-5.587141536\n",
      "[Iter.   30]  loss:96.178055  pct:-3.640477107\n",
      "[Iter.   40]  loss:92.950943  pct:-3.355351512\n",
      "[Iter.   50]  loss:89.983986  pct:-3.191960185\n",
      "[Iter.   60]  loss:87.237610  pct:-3.052071999\n",
      "[Iter.   70]  loss:84.681465  pct:-2.930094851\n",
      "[Iter.   80]  loss:82.291870  pct:-2.821863117\n",
      "[Iter.   90]  loss:80.049629  pct:-2.724741706\n",
      "[Iter.  100]  loss:77.938873  pct:-2.636809116\n",
      "[Iter.  110]  loss:75.946045  pct:-2.556911955\n",
      "[Iter.  120]  loss:74.059608  pct:-2.483916660\n",
      "[Iter.  130]  loss:72.269600  pct:-2.416983538\n",
      "[Iter.  140]  loss:70.567375  pct:-2.355381424\n",
      "[Iter.  150]  loss:68.945389  pct:-2.298493298\n",
      "[Iter.  160]  loss:67.397026  pct:-2.245781421\n",
      "[Iter.  170]  loss:65.916458  pct:-2.196785257\n",
      "[Iter.  180]  loss:64.498543  pct:-2.151079388\n",
      "[Iter.  190]  loss:63.138645  pct:-2.108416027\n",
      "[Iter.  200]  loss:61.832577  pct:-2.068572135\n",
      "[Iter.  210]  loss:60.576759  pct:-2.030996409\n",
      "[Iter.  220]  loss:59.367840  pct:-1.995682071\n",
      "[Iter.  230]  loss:58.202785  pct:-1.962433407\n",
      "[Iter.  240]  loss:57.078831  pct:-1.931101344\n",
      "[Iter.  250]  loss:55.993515  pct:-1.901432967\n",
      "[Iter.  260]  loss:54.944626  pct:-1.873233284\n",
      "[Iter.  270]  loss:53.930080  pct:-1.846487122\n",
      "[Iter.  280]  loss:52.947941  pct:-1.821135032\n",
      "[Iter.  290]  loss:51.996532  pct:-1.796875141\n",
      "[Iter.  300]  loss:51.074211  pct:-1.773813130\n",
      "[Iter.  310]  loss:50.179455  pct:-1.751874963\n",
      "[Iter.  320]  loss:49.310913  pct:-1.730871172\n",
      "[Iter.  330]  loss:48.467316  pct:-1.710772239\n",
      "[Iter.  340]  loss:47.647495  pct:-1.691491251\n",
      "[Iter.  350]  loss:46.850288  pct:-1.673134913\n",
      "[Iter.  360]  loss:46.074688  pct:-1.655486999\n",
      "[Iter.  370]  loss:45.319752  pct:-1.638505331\n",
      "[Iter.  380]  loss:44.584602  pct:-1.622139035\n",
      "[Iter.  390]  loss:43.868397  pct:-1.606396736\n",
      "[Iter.  400]  loss:43.170368  pct:-1.591187771\n",
      "[Iter.  410]  loss:42.489727  pct:-1.576639725\n",
      "[Iter.  420]  loss:41.825760  pct:-1.562653326\n",
      "[Iter.  430]  loss:41.177826  pct:-1.549126571\n",
      "[Iter.  440]  loss:40.545357  pct:-1.535946017\n",
      "[Iter.  450]  loss:39.927700  pct:-1.523372236\n",
      "[Iter.  460]  loss:39.324326  pct:-1.511167637\n",
      "[Iter.  470]  loss:38.734753  pct:-1.499257516\n",
      "[Iter.  480]  loss:38.158424  pct:-1.487884233\n",
      "[Iter.  490]  loss:37.594975  pct:-1.476606723\n",
      "[Iter.  500]  loss:37.043888  pct:-1.465851308\n",
      "[Iter.  510]  loss:36.504776  pct:-1.455333440\n",
      "[Iter.  520]  loss:35.977219  pct:-1.445173566\n",
      "[Iter.  530]  loss:35.460865  pct:-1.435223808\n",
      "[Iter.  540]  loss:34.955357  pct:-1.425538894\n",
      "[Iter.  550]  loss:34.460293  pct:-1.416274442\n",
      "[Iter.  560]  loss:33.975395  pct:-1.407119830\n",
      "[Iter.  570]  loss:33.500317  pct:-1.398301859\n",
      "[Iter.  580]  loss:33.034782  pct:-1.389641225\n",
      "[Iter.  590]  loss:32.578506  pct:-1.381198563\n",
      "[Iter.  600]  loss:32.131229  pct:-1.372920731\n",
      "[Iter.  610]  loss:31.692633  pct:-1.365016943\n",
      "[Iter.  620]  loss:31.262491  pct:-1.357228519\n",
      "[Iter.  630]  loss:30.840599  pct:-1.349515504\n",
      "[Iter.  640]  loss:30.426695  pct:-1.342075714\n",
      "[Iter.  650]  loss:30.020546  pct:-1.334843999\n",
      "[Iter.  660]  loss:29.621981  pct:-1.327641719\n",
      "[Iter.  670]  loss:29.230753  pct:-1.320734513\n",
      "[Iter.  680]  loss:28.846704  pct:-1.313850733\n",
      "[Iter.  690]  loss:28.469608  pct:-1.307241790\n",
      "[Iter.  700]  loss:28.099283  pct:-1.300773388\n",
      "[Iter.  710]  loss:27.735580  pct:-1.294348938\n",
      "[Iter.  720]  loss:27.378315  pct:-1.288112478\n",
      "[Iter.  730]  loss:27.027342  pct:-1.281938387\n",
      "[Iter.  740]  loss:26.682533  pct:-1.275776880\n",
      "[Iter.  750]  loss:26.343702  pct:-1.269860491\n",
      "[Iter.  760]  loss:26.010681  pct:-1.264139565\n",
      "[Iter.  770]  loss:25.683376  pct:-1.258347823\n",
      "[Iter.  780]  loss:25.361629  pct:-1.252743495\n",
      "[Iter.  790]  loss:25.045326  pct:-1.247172440\n",
      "[Iter.  800]  loss:24.734320  pct:-1.241774785\n",
      "[Iter.  810]  loss:24.428522  pct:-1.236329039\n",
      "[Iter.  820]  loss:24.127785  pct:-1.231091180\n",
      "[Iter.  830]  loss:23.831970  pct:-1.226032632\n",
      "[Iter.  840]  loss:23.540998  pct:-1.220930344\n",
      "[Iter.  850]  loss:23.254755  pct:-1.215935846\n",
      "[Iter.  860]  loss:22.973167  pct:-1.210881819\n",
      "[Iter.  870]  loss:22.696121  pct:-1.205955620\n",
      "[Iter.  880]  loss:22.423491  pct:-1.201221517\n",
      "[Iter.  890]  loss:22.155214  pct:-1.196407019\n",
      "[Iter.  900]  loss:21.891182  pct:-1.191739155\n",
      "[Iter.  910]  loss:21.631313  pct:-1.187092695\n",
      "[Iter.  920]  loss:21.375574  pct:-1.182263916\n",
      "[Iter.  930]  loss:21.123768  pct:-1.178009338\n",
      "[Iter.  940]  loss:20.875900  pct:-1.173406117\n",
      "[Iter.  950]  loss:20.631876  pct:-1.168928159\n",
      "[Iter.  960]  loss:20.391611  pct:-1.164532458\n",
      "[Iter.  970]  loss:20.155041  pct:-1.160135691\n",
      "[Iter.  980]  loss:19.922047  pct:-1.156008974\n",
      "[Iter.  990]  loss:19.692608  pct:-1.151682785\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.772447  pct:100.000000000\n",
      "[Iter.    2]  loss:2.772462  pct:0.000524574\n",
      "[Iter.    4]  loss:2.772454  pct:-0.000283784\n",
      "[Iter.    6]  loss:2.772451  pct:-0.000111794\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.772451\n",
      "Best loss: 2.772451 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 27%|██▋       | 2715/10000 [00:48<02:11, 55.54it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:994.694702  pct:100.000000000\n",
      "[Iter.   10]  loss:156.776703  pct:-84.238708313\n",
      "[Iter.   20]  loss:151.282639  pct:-3.504388235\n",
      "[Iter.   30]  loss:147.074326  pct:-2.781755414\n",
      "[Iter.   40]  loss:143.036407  pct:-2.745494889\n",
      "[Iter.   50]  loss:139.267914  pct:-2.634639473\n",
      "[Iter.   60]  loss:135.834137  pct:-2.465590789\n",
      "[Iter.   70]  loss:132.633972  pct:-2.355935604\n",
      "[Iter.   80]  loss:129.620117  pct:-2.272309975\n",
      "[Iter.   90]  loss:126.768562  pct:-2.199932335\n",
      "[Iter.  100]  loss:124.061852  pct:-2.135159353\n",
      "[Iter.  110]  loss:121.486229  pct:-2.076079413\n",
      "[Iter.  120]  loss:119.030174  pct:-2.021673328\n",
      "[Iter.  130]  loss:116.683762  pct:-1.971275497\n",
      "[Iter.  140]  loss:114.438339  pct:-1.924365767\n",
      "[Iter.  150]  loss:112.286140  pct:-1.880662378\n",
      "[Iter.  160]  loss:110.220177  pct:-1.839909838\n",
      "[Iter.  170]  loss:108.234245  pct:-1.801785713\n",
      "[Iter.  180]  loss:106.322830  pct:-1.765998455\n",
      "[Iter.  190]  loss:104.481010  pct:-1.732290007\n",
      "[Iter.  200]  loss:102.704231  pct:-1.700576179\n",
      "[Iter.  210]  loss:100.988411  pct:-1.670642282\n",
      "[Iter.  220]  loss:99.329941  pct:-1.642238093\n",
      "[Iter.  230]  loss:97.725410  pct:-1.615354164\n",
      "[Iter.  240]  loss:96.171638  pct:-1.589936502\n",
      "[Iter.  250]  loss:94.665916  pct:-1.565661217\n",
      "[Iter.  260]  loss:93.205437  pct:-1.542772511\n",
      "[Iter.  270]  loss:91.787994  pct:-1.520772148\n",
      "[Iter.  280]  loss:90.411285  pct:-1.499879144\n",
      "[Iter.  290]  loss:89.073265  pct:-1.479926227\n",
      "[Iter.  300]  loss:87.772026  pct:-1.460863720\n",
      "[Iter.  310]  loss:86.505852  pct:-1.442571595\n",
      "[Iter.  320]  loss:85.273033  pct:-1.425127409\n",
      "[Iter.  330]  loss:84.072151  pct:-1.408278695\n",
      "[Iter.  340]  loss:82.901604  pct:-1.392312994\n",
      "[Iter.  350]  loss:81.760178  pct:-1.376844398\n",
      "[Iter.  360]  loss:80.646652  pct:-1.361941012\n",
      "[Iter.  370]  loss:79.559792  pct:-1.347682299\n",
      "[Iter.  380]  loss:78.498596  pct:-1.333833778\n",
      "[Iter.  390]  loss:77.461983  pct:-1.320550321\n",
      "[Iter.  400]  loss:76.448982  pct:-1.307738910\n",
      "[Iter.  410]  loss:75.458694  pct:-1.295357704\n",
      "[Iter.  420]  loss:74.490204  pct:-1.283471186\n",
      "[Iter.  430]  loss:73.542694  pct:-1.271992445\n",
      "[Iter.  440]  loss:72.615425  pct:-1.260858054\n",
      "[Iter.  450]  loss:71.707626  pct:-1.250145910\n",
      "[Iter.  460]  loss:70.818687  pct:-1.239671356\n",
      "[Iter.  470]  loss:69.947891  pct:-1.229613588\n",
      "[Iter.  480]  loss:69.094688  pct:-1.219769181\n",
      "[Iter.  490]  loss:68.258415  pct:-1.210329206\n",
      "[Iter.  500]  loss:67.438591  pct:-1.201059556\n",
      "[Iter.  510]  loss:66.634567  pct:-1.192230933\n",
      "[Iter.  520]  loss:65.845917  pct:-1.183545636\n",
      "[Iter.  530]  loss:65.072197  pct:-1.175045965\n",
      "[Iter.  540]  loss:64.312881  pct:-1.166881596\n",
      "[Iter.  550]  loss:63.567501  pct:-1.158990834\n",
      "[Iter.  560]  loss:62.835655  pct:-1.151289328\n",
      "[Iter.  570]  loss:62.116982  pct:-1.143735517\n",
      "[Iter.  580]  loss:61.411026  pct:-1.136493578\n",
      "[Iter.  590]  loss:60.717495  pct:-1.129326575\n",
      "[Iter.  600]  loss:60.035980  pct:-1.122435536\n",
      "[Iter.  610]  loss:59.366219  pct:-1.115600437\n",
      "[Iter.  620]  loss:58.707867  pct:-1.108967211\n",
      "[Iter.  630]  loss:58.060589  pct:-1.102540202\n",
      "[Iter.  640]  loss:57.424099  pct:-1.096251142\n",
      "[Iter.  650]  loss:56.798077  pct:-1.090173551\n",
      "[Iter.  660]  loss:56.182358  pct:-1.084048753\n",
      "[Iter.  670]  loss:55.576527  pct:-1.078329871\n",
      "[Iter.  680]  loss:54.980423  pct:-1.072581725\n",
      "[Iter.  690]  loss:54.393673  pct:-1.067198102\n",
      "[Iter.  700]  loss:53.816208  pct:-1.061640125\n",
      "[Iter.  710]  loss:53.247746  pct:-1.056303285\n",
      "[Iter.  720]  loss:52.688026  pct:-1.051160158\n",
      "[Iter.  730]  loss:52.136974  pct:-1.045877272\n",
      "[Iter.  740]  loss:51.594238  pct:-1.040981109\n",
      "[Iter.  750]  loss:51.059692  pct:-1.036057351\n",
      "[Iter.  760]  loss:50.533146  pct:-1.031237075\n",
      "[Iter.  770]  loss:50.014378  pct:-1.026590174\n",
      "[Iter.  780]  loss:49.503155  pct:-1.022151757\n",
      "[Iter.  790]  loss:48.999413  pct:-1.017596193\n",
      "[Iter.  800]  loss:48.502907  pct:-1.013289163\n",
      "[Iter.  810]  loss:48.013626  pct:-1.008765728\n",
      "[Iter.  820]  loss:47.531303  pct:-1.004553774\n",
      "[Iter.  830]  loss:47.055733  pct:-1.000542053\n",
      "[Iter.  840]  loss:46.586876  pct:-0.996386167\n",
      "[Iter.  850]  loss:46.124554  pct:-0.992387289\n",
      "[Iter.  860]  loss:45.668686  pct:-0.988340766\n",
      "[Iter.  870]  loss:45.219002  pct:-0.984666263\n",
      "[Iter.  880]  loss:44.775494  pct:-0.980800395\n",
      "[Iter.  890]  loss:44.337948  pct:-0.977199224\n",
      "[Iter.  900]  loss:43.906345  pct:-0.973438102\n",
      "[Iter.  910]  loss:43.480492  pct:-0.969913860\n",
      "[Iter.  920]  loss:43.060326  pct:-0.966332256\n",
      "[Iter.  930]  loss:42.645672  pct:-0.962960154\n",
      "[Iter.  940]  loss:42.236439  pct:-0.959612255\n",
      "[Iter.  950]  loss:41.832542  pct:-0.956274591\n",
      "[Iter.  960]  loss:41.433887  pct:-0.952978028\n",
      "[Iter.  970]  loss:41.040367  pct:-0.949754848\n",
      "[Iter.  980]  loss:40.651886  pct:-0.946583004\n",
      "[Iter.  990]  loss:40.268368  pct:-0.943420483\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.740638  pct:100.000000000\n",
      "[Iter.    2]  loss:2.740651  pct:0.000487165\n",
      "[Iter.    4]  loss:2.740644  pct:-0.000278379\n",
      "[Iter.    6]  loss:2.740639  pct:-0.000156589\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.740639\n",
      "Best loss: 2.740639 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 24%|██▍       | 2443/10000 [00:39<02:02, 61.90it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:464.746063  pct:100.000000000\n",
      "[Iter.   10]  loss:78.107101  pct:-83.193599180\n",
      "[Iter.   20]  loss:74.214928  pct:-4.983124063\n",
      "[Iter.   30]  loss:71.020096  pct:-4.304837245\n",
      "[Iter.   40]  loss:68.149963  pct:-4.041296218\n",
      "[Iter.   50]  loss:65.543030  pct:-3.825289794\n",
      "[Iter.   60]  loss:63.156361  pct:-3.641377530\n",
      "[Iter.   70]  loss:60.957558  pct:-3.481522567\n",
      "[Iter.   80]  loss:58.921036  pct:-3.340885018\n",
      "[Iter.   90]  loss:57.026077  pct:-3.216098413\n",
      "[Iter.  100]  loss:55.255688  pct:-3.104526283\n",
      "[Iter.  110]  loss:53.595810  pct:-3.003994423\n",
      "[Iter.  120]  loss:52.034565  pct:-2.912998174\n",
      "[Iter.  130]  loss:50.561951  pct:-2.830069376\n",
      "[Iter.  140]  loss:49.169384  pct:-2.754179105\n",
      "[Iter.  150]  loss:47.849434  pct:-2.684495913\n",
      "[Iter.  160]  loss:46.595676  pct:-2.620213814\n",
      "[Iter.  170]  loss:45.402447  pct:-2.560816297\n",
      "[Iter.  180]  loss:44.264790  pct:-2.505717747\n",
      "[Iter.  190]  loss:43.178352  pct:-2.454405038\n",
      "[Iter.  200]  loss:42.139179  pct:-2.406699352\n",
      "[Iter.  210]  loss:41.143787  pct:-2.362152904\n",
      "[Iter.  220]  loss:40.189125  pct:-2.320307351\n",
      "[Iter.  230]  loss:39.272373  pct:-2.281094351\n",
      "[Iter.  240]  loss:38.391037  pct:-2.244163366\n",
      "[Iter.  250]  loss:37.542923  pct:-2.209145885\n",
      "[Iter.  260]  loss:36.725883  pct:-2.176280974\n",
      "[Iter.  270]  loss:35.938126  pct:-2.144966435\n",
      "[Iter.  280]  loss:35.177898  pct:-2.115378002\n",
      "[Iter.  290]  loss:34.443642  pct:-2.087267226\n",
      "[Iter.  300]  loss:33.733986  pct:-2.060338941\n",
      "[Iter.  310]  loss:33.047531  pct:-2.034905614\n",
      "[Iter.  320]  loss:32.383121  pct:-2.010466788\n",
      "[Iter.  330]  loss:31.739635  pct:-1.987103137\n",
      "[Iter.  340]  loss:31.116009  pct:-1.964820011\n",
      "[Iter.  350]  loss:30.511297  pct:-1.943409700\n",
      "[Iter.  360]  loss:29.924591  pct:-1.922914510\n",
      "[Iter.  370]  loss:29.355078  pct:-1.903161583\n",
      "[Iter.  380]  loss:28.801964  pct:-1.884218949\n",
      "[Iter.  390]  loss:28.264559  pct:-1.865862403\n",
      "[Iter.  400]  loss:27.742165  pct:-1.848230443\n",
      "[Iter.  410]  loss:27.234135  pct:-1.831255581\n",
      "[Iter.  420]  loss:26.739859  pct:-1.814913720\n",
      "[Iter.  430]  loss:26.258793  pct:-1.799058689\n",
      "[Iter.  440]  loss:25.790436  pct:-1.783620018\n",
      "[Iter.  450]  loss:25.334227  pct:-1.768908391\n",
      "[Iter.  460]  loss:24.889769  pct:-1.754377644\n",
      "[Iter.  470]  loss:24.456596  pct:-1.740362608\n",
      "[Iter.  480]  loss:24.034250  pct:-1.726921067\n",
      "[Iter.  490]  loss:23.622370  pct:-1.713723077\n",
      "[Iter.  500]  loss:23.220579  pct:-1.700890397\n",
      "[Iter.  510]  loss:22.828535  pct:-1.688347499\n",
      "[Iter.  520]  loss:22.445902  pct:-1.676118103\n",
      "[Iter.  530]  loss:22.072340  pct:-1.664276451\n",
      "[Iter.  540]  loss:21.707535  pct:-1.652770940\n",
      "[Iter.  550]  loss:21.351227  pct:-1.641402337\n",
      "[Iter.  560]  loss:21.003111  pct:-1.630425849\n",
      "[Iter.  570]  loss:20.662909  pct:-1.619771154\n",
      "[Iter.  580]  loss:20.330395  pct:-1.609230416\n",
      "[Iter.  590]  loss:20.005243  pct:-1.599336597\n",
      "[Iter.  600]  loss:19.687336  pct:-1.589120055\n",
      "[Iter.  610]  loss:19.376493  pct:-1.578895766\n",
      "[Iter.  620]  loss:19.072477  pct:-1.568994483\n",
      "[Iter.  630]  loss:18.775026  pct:-1.559582502\n",
      "[Iter.  640]  loss:18.484022  pct:-1.549953518\n",
      "[Iter.  650]  loss:18.199249  pct:-1.540643431\n",
      "[Iter.  660]  loss:17.920462  pct:-1.531863259\n",
      "[Iter.  670]  loss:17.647573  pct:-1.522774295\n",
      "[Iter.  680]  loss:17.380394  pct:-1.513972953\n",
      "[Iter.  690]  loss:17.118807  pct:-1.505070272\n",
      "[Iter.  700]  loss:16.862589  pct:-1.496704525\n",
      "[Iter.  710]  loss:16.611656  pct:-1.488103014\n",
      "[Iter.  720]  loss:16.365833  pct:-1.479821781\n",
      "[Iter.  730]  loss:16.125017  pct:-1.471456492\n",
      "[Iter.  740]  loss:15.889036  pct:-1.463446427\n",
      "[Iter.  750]  loss:15.657796  pct:-1.455344868\n",
      "[Iter.  760]  loss:15.431161  pct:-1.447425810\n",
      "[Iter.  770]  loss:15.209052  pct:-1.439352762\n",
      "[Iter.  780]  loss:14.991332  pct:-1.431516116\n",
      "[Iter.  790]  loss:14.777842  pct:-1.424092838\n",
      "[Iter.  800]  loss:14.568542  pct:-1.416309955\n",
      "[Iter.  810]  loss:14.363322  pct:-1.408646627\n",
      "[Iter.  820]  loss:14.162056  pct:-1.401251640\n",
      "[Iter.  830]  loss:13.964648  pct:-1.393919943\n",
      "[Iter.  840]  loss:13.771055  pct:-1.386307924\n",
      "[Iter.  850]  loss:13.581143  pct:-1.379065288\n",
      "[Iter.  860]  loss:13.394825  pct:-1.371890365\n",
      "[Iter.  870]  loss:13.212041  pct:-1.364587300\n",
      "[Iter.  880]  loss:13.032722  pct:-1.357234884\n",
      "[Iter.  890]  loss:12.856760  pct:-1.350158791\n",
      "[Iter.  900]  loss:12.684063  pct:-1.343239408\n",
      "[Iter.  910]  loss:12.514593  pct:-1.336084770\n",
      "[Iter.  920]  loss:12.348257  pct:-1.329136776\n",
      "[Iter.  930]  loss:12.185003  pct:-1.322079572\n",
      "[Iter.  940]  loss:12.024744  pct:-1.315217100\n",
      "[Iter.  950]  loss:11.867425  pct:-1.308294534\n",
      "[Iter.  960]  loss:11.712996  pct:-1.301280459\n",
      "[Iter.  970]  loss:11.561358  pct:-1.294613477\n",
      "[Iter.  980]  loss:11.412479  pct:-1.287729741\n",
      "[Iter.  990]  loss:11.266300  pct:-1.280871527\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.792368  pct:100.000000000\n",
      "[Iter.    2]  loss:2.792409  pct:0.001460036\n",
      "[Iter.    4]  loss:2.792393  pct:-0.000546438\n",
      "[Iter.    6]  loss:2.792380  pct:-0.000469598\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.792380\n",
      "Best loss: 2.792380 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1664/10000 [00:21<01:49, 75.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3950.229736  pct:100.000000000\n",
      "[Iter.   10]  loss:669.363159  pct:-83.055085861\n",
      "[Iter.   20]  loss:654.118774  pct:-2.277446041\n",
      "[Iter.   30]  loss:644.685547  pct:-1.442127624\n",
      "[Iter.   40]  loss:635.808594  pct:-1.376943095\n",
      "[Iter.   50]  loss:627.268616  pct:-1.343168072\n",
      "[Iter.   60]  loss:619.036316  pct:-1.312404223\n",
      "[Iter.   70]  loss:611.100891  pct:-1.281899721\n",
      "[Iter.   80]  loss:603.456604  pct:-1.250904265\n",
      "[Iter.   90]  loss:596.098877  pct:-1.219263656\n",
      "[Iter.  100]  loss:589.022034  pct:-1.187192853\n",
      "[Iter.  110]  loss:582.205139  pct:-1.157324199\n",
      "[Iter.  120]  loss:575.623596  pct:-1.130450854\n",
      "[Iter.  130]  loss:569.256592  pct:-1.106105524\n",
      "[Iter.  140]  loss:563.088440  pct:-1.083545091\n",
      "[Iter.  150]  loss:557.106079  pct:-1.062419403\n",
      "[Iter.  160]  loss:551.298523  pct:-1.042450688\n",
      "[Iter.  170]  loss:545.656250  pct:-1.023451490\n",
      "[Iter.  180]  loss:540.170532  pct:-1.005343158\n",
      "[Iter.  190]  loss:534.833557  pct:-0.988016706\n",
      "[Iter.  200]  loss:529.637573  pct:-0.971514187\n",
      "[Iter.  210]  loss:524.575928  pct:-0.955680972\n",
      "[Iter.  220]  loss:519.642822  pct:-0.940398750\n",
      "[Iter.  230]  loss:514.831665  pct:-0.925858497\n",
      "[Iter.  240]  loss:510.137299  pct:-0.911825510\n",
      "[Iter.  250]  loss:505.554474  pct:-0.898351232\n",
      "[Iter.  260]  loss:501.078339  pct:-0.885391285\n",
      "[Iter.  270]  loss:496.704376  pct:-0.872909896\n",
      "[Iter.  280]  loss:492.428131  pct:-0.860923584\n",
      "[Iter.  290]  loss:488.245728  pct:-0.849342940\n",
      "[Iter.  300]  loss:484.153412  pct:-0.838167227\n",
      "[Iter.  310]  loss:480.147675  pct:-0.827369426\n",
      "[Iter.  320]  loss:476.225128  pct:-0.816945826\n",
      "[Iter.  330]  loss:472.382751  pct:-0.806840396\n",
      "[Iter.  340]  loss:468.617279  pct:-0.797123180\n",
      "[Iter.  350]  loss:464.925842  pct:-0.787729547\n",
      "[Iter.  360]  loss:461.306183  pct:-0.778545543\n",
      "[Iter.  370]  loss:457.755554  pct:-0.769690239\n",
      "[Iter.  380]  loss:454.271332  pct:-0.761153498\n",
      "[Iter.  390]  loss:450.851501  pct:-0.752816672\n",
      "[Iter.  400]  loss:447.493683  pct:-0.744772634\n",
      "[Iter.  410]  loss:444.195862  pct:-0.736953653\n",
      "[Iter.  420]  loss:440.956116  pct:-0.729350805\n",
      "[Iter.  430]  loss:437.772522  pct:-0.721975189\n",
      "[Iter.  440]  loss:434.643127  pct:-0.714844896\n",
      "[Iter.  450]  loss:431.566589  pct:-0.707830837\n",
      "[Iter.  460]  loss:428.540955  pct:-0.701081789\n",
      "[Iter.  470]  loss:425.564972  pct:-0.694445335\n",
      "[Iter.  480]  loss:422.636963  pct:-0.688028674\n",
      "[Iter.  490]  loss:419.755463  pct:-0.681790874\n",
      "[Iter.  500]  loss:416.919128  pct:-0.675711094\n",
      "[Iter.  510]  loss:414.126709  pct:-0.669774842\n",
      "[Iter.  520]  loss:411.377014  pct:-0.663974278\n",
      "[Iter.  530]  loss:408.668854  pct:-0.658315926\n",
      "[Iter.  540]  loss:406.001068  pct:-0.652798866\n",
      "[Iter.  550]  loss:403.372375  pct:-0.647459535\n",
      "[Iter.  560]  loss:400.781921  pct:-0.642199183\n",
      "[Iter.  570]  loss:398.228638  pct:-0.637075565\n",
      "[Iter.  580]  loss:395.711548  pct:-0.632071530\n",
      "[Iter.  590]  loss:393.229614  pct:-0.627207775\n",
      "[Iter.  600]  loss:390.782104  pct:-0.622412371\n",
      "[Iter.  610]  loss:388.368073  pct:-0.617743739\n",
      "[Iter.  620]  loss:385.986481  pct:-0.613230583\n",
      "[Iter.  630]  loss:383.636841  pct:-0.608736318\n",
      "[Iter.  640]  loss:381.318329  pct:-0.604350708\n",
      "[Iter.  650]  loss:379.030060  pct:-0.600094165\n",
      "[Iter.  660]  loss:376.771332  pct:-0.595923191\n",
      "[Iter.  670]  loss:374.541595  pct:-0.591800952\n",
      "[Iter.  680]  loss:372.339905  pct:-0.587836091\n",
      "[Iter.  690]  loss:370.165741  pct:-0.583919099\n",
      "[Iter.  700]  loss:368.018616  pct:-0.580044290\n",
      "[Iter.  710]  loss:365.897827  pct:-0.576272092\n",
      "[Iter.  720]  loss:363.802887  pct:-0.572547862\n",
      "[Iter.  730]  loss:361.732941  pct:-0.568974674\n",
      "[Iter.  740]  loss:359.687714  pct:-0.565396960\n",
      "[Iter.  750]  loss:357.666412  pct:-0.561960054\n",
      "[Iter.  760]  loss:355.668823  pct:-0.558506206\n",
      "[Iter.  770]  loss:353.694275  pct:-0.555164864\n",
      "[Iter.  780]  loss:351.742371  pct:-0.551861999\n",
      "[Iter.  790]  loss:349.812592  pct:-0.548634232\n",
      "[Iter.  800]  loss:347.904388  pct:-0.545492979\n",
      "[Iter.  810]  loss:346.017487  pct:-0.542362189\n",
      "[Iter.  820]  loss:344.151276  pct:-0.539340065\n",
      "[Iter.  830]  loss:342.305603  pct:-0.536296896\n",
      "[Iter.  840]  loss:340.479645  pct:-0.533429262\n",
      "[Iter.  850]  loss:338.673218  pct:-0.530553597\n",
      "[Iter.  860]  loss:336.885986  pct:-0.527715612\n",
      "[Iter.  870]  loss:335.117706  pct:-0.524889755\n",
      "[Iter.  880]  loss:333.368011  pct:-0.522113512\n",
      "[Iter.  890]  loss:331.636322  pct:-0.519452795\n",
      "[Iter.  900]  loss:329.922363  pct:-0.516818764\n",
      "[Iter.  910]  loss:328.225952  pct:-0.514184948\n",
      "[Iter.  920]  loss:326.546722  pct:-0.511607850\n",
      "[Iter.  930]  loss:324.884338  pct:-0.509079993\n",
      "[Iter.  940]  loss:323.238586  pct:-0.506565494\n",
      "[Iter.  950]  loss:321.609039  pct:-0.504131372\n",
      "[Iter.  960]  loss:319.995331  pct:-0.501760927\n",
      "[Iter.  970]  loss:318.397369  pct:-0.499370232\n",
      "[Iter.  980]  loss:316.814880  pct:-0.497016988\n",
      "[Iter.  990]  loss:315.247589  pct:-0.494702540\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.808747  pct:100.000000000\n",
      "[Iter.    2]  loss:2.808311  pct:-0.015525341\n",
      "[Iter.    4]  loss:2.808180  pct:-0.004669362\n",
      "[Iter.    6]  loss:2.808118  pct:-0.002198948\n",
      "[Iter.    8]  loss:2.808090  pct:-0.001001859\n",
      "[Iter.   10]  loss:2.808081  pct:-0.000322636\n",
      "[Iter.   12]  loss:2.808067  pct:-0.000517917\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.808067\n",
      "Best loss: 2.808067 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1772/10000 [00:23<01:50, 74.38it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:462.507111  pct:100.000000000\n",
      "[Iter.   10]  loss:75.685333  pct:-83.635855208\n",
      "[Iter.   20]  loss:71.899750  pct:-5.001739879\n",
      "[Iter.   30]  loss:68.868843  pct:-4.215462067\n",
      "[Iter.   40]  loss:66.135071  pct:-3.969534198\n",
      "[Iter.   50]  loss:63.642120  pct:-3.769483285\n",
      "[Iter.   60]  loss:61.353577  pct:-3.595957659\n",
      "[Iter.   70]  loss:59.242504  pct:-3.440830438\n",
      "[Iter.   80]  loss:57.286171  pct:-3.302245895\n",
      "[Iter.   90]  loss:55.464832  pct:-3.179368813\n",
      "[Iter.  100]  loss:53.761871  pct:-3.070343670\n",
      "[Iter.  110]  loss:52.163898  pct:-2.972316309\n",
      "[Iter.  120]  loss:50.659283  pct:-2.884400568\n",
      "[Iter.  130]  loss:49.238480  pct:-2.804625322\n",
      "[Iter.  140]  loss:47.893337  pct:-2.731892567\n",
      "[Iter.  150]  loss:46.617001  pct:-2.664956637\n",
      "[Iter.  160]  loss:45.403728  pct:-2.602638693\n",
      "[Iter.  170]  loss:44.248108  pct:-2.545210743\n",
      "[Iter.  180]  loss:43.145794  pct:-2.491211596\n",
      "[Iter.  190]  loss:42.092793  pct:-2.440565599\n",
      "[Iter.  200]  loss:41.085152  pct:-2.393855999\n",
      "[Iter.  210]  loss:40.119801  pct:-2.349635003\n",
      "[Iter.  220]  loss:39.193966  pct:-2.307675120\n",
      "[Iter.  230]  loss:38.304771  pct:-2.268702510\n",
      "[Iter.  240]  loss:37.449726  pct:-2.232216214\n",
      "[Iter.  250]  loss:36.626614  pct:-2.197913238\n",
      "[Iter.  260]  loss:35.833431  pct:-2.165590249\n",
      "[Iter.  270]  loss:35.068409  pct:-2.134940058\n",
      "[Iter.  280]  loss:34.329910  pct:-2.105880220\n",
      "[Iter.  290]  loss:33.616508  pct:-2.078076490\n",
      "[Iter.  300]  loss:32.926750  pct:-2.051843966\n",
      "[Iter.  310]  loss:32.259430  pct:-2.026681187\n",
      "[Iter.  320]  loss:31.613361  pct:-2.002727805\n",
      "[Iter.  330]  loss:30.987511  pct:-1.979703045\n",
      "[Iter.  340]  loss:30.380873  pct:-1.957685343\n",
      "[Iter.  350]  loss:29.792557  pct:-1.936468281\n",
      "[Iter.  360]  loss:29.221670  pct:-1.916205502\n",
      "[Iter.  370]  loss:28.667374  pct:-1.896867943\n",
      "[Iter.  380]  loss:28.128992  pct:-1.878028950\n",
      "[Iter.  390]  loss:27.605778  pct:-1.860053637\n",
      "[Iter.  400]  loss:27.097097  pct:-1.842658984\n",
      "[Iter.  410]  loss:26.602322  pct:-1.825936427\n",
      "[Iter.  420]  loss:26.120935  pct:-1.809564562\n",
      "[Iter.  430]  loss:25.652325  pct:-1.794004524\n",
      "[Iter.  440]  loss:25.196043  pct:-1.778714669\n",
      "[Iter.  450]  loss:24.751575  pct:-1.764037092\n",
      "[Iter.  460]  loss:24.318478  pct:-1.749778877\n",
      "[Iter.  470]  loss:23.896357  pct:-1.735803755\n",
      "[Iter.  480]  loss:23.484756  pct:-1.722438781\n",
      "[Iter.  490]  loss:23.083313  pct:-1.709378941\n",
      "[Iter.  500]  loss:22.691673  pct:-1.696635616\n",
      "[Iter.  510]  loss:22.309464  pct:-1.684361365\n",
      "[Iter.  520]  loss:21.936398  pct:-1.672231824\n",
      "[Iter.  530]  loss:21.572142  pct:-1.660509226\n",
      "[Iter.  540]  loss:21.216475  pct:-1.648733446\n",
      "[Iter.  550]  loss:20.869036  pct:-1.637589750\n",
      "[Iter.  560]  loss:20.529572  pct:-1.626640503\n",
      "[Iter.  570]  loss:20.197821  pct:-1.615965873\n",
      "[Iter.  580]  loss:19.873581  pct:-1.605320377\n",
      "[Iter.  590]  loss:19.556597  pct:-1.595002822\n",
      "[Iter.  600]  loss:19.246626  pct:-1.584993849\n",
      "[Iter.  610]  loss:18.943460  pct:-1.575161472\n",
      "[Iter.  620]  loss:18.646946  pct:-1.565260538\n",
      "[Iter.  630]  loss:18.356844  pct:-1.555761494\n",
      "[Iter.  640]  loss:18.072966  pct:-1.546444079\n",
      "[Iter.  650]  loss:17.795147  pct:-1.537205822\n",
      "[Iter.  660]  loss:17.523180  pct:-1.528320812\n",
      "[Iter.  670]  loss:17.256947  pct:-1.519321516\n",
      "[Iter.  680]  loss:16.996279  pct:-1.510509405\n",
      "[Iter.  690]  loss:16.741037  pct:-1.501748692\n",
      "[Iter.  700]  loss:16.491066  pct:-1.493165473\n",
      "[Iter.  710]  loss:16.246191  pct:-1.484894636\n",
      "[Iter.  720]  loss:16.006325  pct:-1.476446118\n",
      "[Iter.  730]  loss:15.771328  pct:-1.468149616\n",
      "[Iter.  740]  loss:15.541041  pct:-1.460159846\n",
      "[Iter.  750]  loss:15.315344  pct:-1.452267657\n",
      "[Iter.  760]  loss:15.094170  pct:-1.444134994\n",
      "[Iter.  770]  loss:14.877369  pct:-1.436320746\n",
      "[Iter.  780]  loss:14.664824  pct:-1.428649084\n",
      "[Iter.  790]  loss:14.456475  pct:-1.420734957\n",
      "[Iter.  800]  loss:14.252170  pct:-1.413246626\n",
      "[Iter.  810]  loss:14.051820  pct:-1.405749533\n",
      "[Iter.  820]  loss:13.855362  pct:-1.398095518\n",
      "[Iter.  830]  loss:13.662684  pct:-1.390634894\n",
      "[Iter.  840]  loss:13.473689  pct:-1.383295956\n",
      "[Iter.  850]  loss:13.288310  pct:-1.375859479\n",
      "[Iter.  860]  loss:13.106453  pct:-1.368549563\n",
      "[Iter.  870]  loss:12.928012  pct:-1.361474752\n",
      "[Iter.  880]  loss:12.752948  pct:-1.354145466\n",
      "[Iter.  890]  loss:12.581173  pct:-1.346942423\n",
      "[Iter.  900]  loss:12.412604  pct:-1.339848136\n",
      "[Iter.  910]  loss:12.247169  pct:-1.332804838\n",
      "[Iter.  920]  loss:12.084827  pct:-1.325540000\n",
      "[Iter.  930]  loss:11.925480  pct:-1.318575174\n",
      "[Iter.  940]  loss:11.769071  pct:-1.311555301\n",
      "[Iter.  950]  loss:11.615519  pct:-1.304708420\n",
      "[Iter.  960]  loss:11.464797  pct:-1.297587784\n",
      "[Iter.  970]  loss:11.316832  pct:-1.290606637\n",
      "[Iter.  980]  loss:11.171555  pct:-1.283725239\n",
      "[Iter.  990]  loss:11.028900  pct:-1.276943313\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.780055  pct:100.000000000\n",
      "[Iter.    2]  loss:2.780125  pct:0.002521356\n",
      "[Iter.    4]  loss:2.780125  pct:0.000025727\n",
      "[Iter.    6]  loss:2.780125  pct:-0.000008576\n",
      "[Iter.    8]  loss:2.780125  pct:0.000008576\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.780125\n",
      "Best loss: 2.780125 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  3%|▎         | 266/10000 [00:05<03:25, 47.40it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5517.987305  pct:100.000000000\n",
      "[Iter.   10]  loss:749.523743  pct:-86.416715291\n",
      "[Iter.   20]  loss:742.032104  pct:-0.999519796\n",
      "[Iter.   30]  loss:734.629028  pct:-0.997675994\n",
      "[Iter.   40]  loss:728.354492  pct:-0.854109474\n",
      "[Iter.   50]  loss:722.618408  pct:-0.787540140\n",
      "[Iter.   60]  loss:717.088928  pct:-0.765200543\n",
      "[Iter.   70]  loss:711.692322  pct:-0.752571436\n",
      "[Iter.   80]  loss:706.412537  pct:-0.741863442\n",
      "[Iter.   90]  loss:701.242432  pct:-0.731881827\n",
      "[Iter.  100]  loss:696.176697  pct:-0.722394230\n",
      "[Iter.  110]  loss:691.210571  pct:-0.713342677\n",
      "[Iter.  120]  loss:686.339111  pct:-0.704772202\n",
      "[Iter.  130]  loss:681.558411  pct:-0.696550816\n",
      "[Iter.  140]  loss:676.864380  pct:-0.688720246\n",
      "[Iter.  150]  loss:672.253479  pct:-0.681214881\n",
      "[Iter.  160]  loss:667.722473  pct:-0.674002590\n",
      "[Iter.  170]  loss:663.269592  pct:-0.666875991\n",
      "[Iter.  180]  loss:658.892273  pct:-0.659960804\n",
      "[Iter.  190]  loss:654.589111  pct:-0.653090315\n",
      "[Iter.  200]  loss:650.358398  pct:-0.646315806\n",
      "[Iter.  210]  loss:646.197754  pct:-0.639746414\n",
      "[Iter.  220]  loss:642.104736  pct:-0.633400156\n",
      "[Iter.  230]  loss:638.077026  pct:-0.627266820\n",
      "[Iter.  240]  loss:634.112732  pct:-0.621287755\n",
      "[Iter.  250]  loss:630.209656  pct:-0.615517711\n",
      "[Iter.  260]  loss:626.365601  pct:-0.609964500\n",
      "[Iter.  270]  loss:622.580139  pct:-0.604353340\n",
      "[Iter.  280]  loss:618.850464  pct:-0.599067503\n",
      "[Iter.  290]  loss:615.176025  pct:-0.593752238\n",
      "[Iter.  300]  loss:611.555420  pct:-0.588547882\n",
      "[Iter.  310]  loss:607.986755  pct:-0.583539028\n",
      "[Iter.  320]  loss:604.468811  pct:-0.578621870\n",
      "[Iter.  330]  loss:601.000488  pct:-0.573780266\n",
      "[Iter.  340]  loss:597.580261  pct:-0.569088897\n",
      "[Iter.  350]  loss:594.207458  pct:-0.564409997\n",
      "[Iter.  360]  loss:590.880493  pct:-0.559899625\n",
      "[Iter.  370]  loss:587.598022  pct:-0.555521927\n",
      "[Iter.  380]  loss:584.359558  pct:-0.551136020\n",
      "[Iter.  390]  loss:581.164185  pct:-0.546816338\n",
      "[Iter.  400]  loss:578.010071  pct:-0.542723357\n",
      "[Iter.  410]  loss:574.896667  pct:-0.538641708\n",
      "[Iter.  420]  loss:571.823486  pct:-0.534562353\n",
      "[Iter.  430]  loss:568.789368  pct:-0.530604063\n",
      "[Iter.  440]  loss:565.792847  pct:-0.526824369\n",
      "[Iter.  450]  loss:562.833679  pct:-0.523012530\n",
      "[Iter.  460]  loss:559.910645  pct:-0.519342530\n",
      "[Iter.  470]  loss:557.022888  pct:-0.515753072\n",
      "[Iter.  480]  loss:554.169861  pct:-0.512192121\n",
      "[Iter.  490]  loss:551.350952  pct:-0.508672321\n",
      "[Iter.  500]  loss:548.565125  pct:-0.505273025\n",
      "[Iter.  510]  loss:545.811584  pct:-0.501953171\n",
      "[Iter.  520]  loss:543.089966  pct:-0.498637026\n",
      "[Iter.  530]  loss:540.399475  pct:-0.495404241\n",
      "[Iter.  540]  loss:537.739502  pct:-0.492223488\n",
      "[Iter.  550]  loss:535.109070  pct:-0.489164757\n",
      "[Iter.  560]  loss:532.507874  pct:-0.486105812\n",
      "[Iter.  570]  loss:529.934875  pct:-0.483184977\n",
      "[Iter.  580]  loss:527.389771  pct:-0.480267500\n",
      "[Iter.  590]  loss:524.872070  pct:-0.477388895\n",
      "[Iter.  600]  loss:522.381531  pct:-0.474504111\n",
      "[Iter.  610]  loss:519.917053  pct:-0.471777311\n",
      "[Iter.  620]  loss:517.478088  pct:-0.469106529\n",
      "[Iter.  630]  loss:515.064453  pct:-0.466422697\n",
      "[Iter.  640]  loss:512.675903  pct:-0.463738041\n",
      "[Iter.  650]  loss:510.311920  pct:-0.461106742\n",
      "[Iter.  660]  loss:507.971771  pct:-0.458572264\n",
      "[Iter.  670]  loss:505.654968  pct:-0.456088923\n",
      "[Iter.  680]  loss:503.360931  pct:-0.453676323\n",
      "[Iter.  690]  loss:501.089630  pct:-0.451227167\n",
      "[Iter.  700]  loss:498.840546  pct:-0.448838758\n",
      "[Iter.  710]  loss:496.613220  pct:-0.446500482\n",
      "[Iter.  720]  loss:494.407288  pct:-0.444195307\n",
      "[Iter.  730]  loss:492.222260  pct:-0.441949003\n",
      "[Iter.  740]  loss:490.057983  pct:-0.439694890\n",
      "[Iter.  750]  loss:487.914276  pct:-0.437439517\n",
      "[Iter.  760]  loss:485.790314  pct:-0.435314666\n",
      "[Iter.  770]  loss:483.685974  pct:-0.433178583\n",
      "[Iter.  780]  loss:481.600769  pct:-0.431107204\n",
      "[Iter.  790]  loss:479.534637  pct:-0.429013350\n",
      "[Iter.  800]  loss:477.487213  pct:-0.426960673\n",
      "[Iter.  810]  loss:475.458191  pct:-0.424937498\n",
      "[Iter.  820]  loss:473.447083  pct:-0.422983227\n",
      "[Iter.  830]  loss:471.453613  pct:-0.421054287\n",
      "[Iter.  840]  loss:469.477753  pct:-0.419099682\n",
      "[Iter.  850]  loss:467.519073  pct:-0.417203837\n",
      "[Iter.  860]  loss:465.577148  pct:-0.415368091\n",
      "[Iter.  870]  loss:463.652039  pct:-0.413488907\n",
      "[Iter.  880]  loss:461.743317  pct:-0.411671203\n",
      "[Iter.  890]  loss:459.850647  pct:-0.409896496\n",
      "[Iter.  900]  loss:457.973724  pct:-0.408159175\n",
      "[Iter.  910]  loss:456.112823  pct:-0.406333547\n",
      "[Iter.  920]  loss:454.267029  pct:-0.404679409\n",
      "[Iter.  930]  loss:452.436523  pct:-0.402958008\n",
      "[Iter.  940]  loss:450.620941  pct:-0.401289945\n",
      "[Iter.  950]  loss:448.820129  pct:-0.399628957\n",
      "[Iter.  960]  loss:447.033844  pct:-0.397995830\n",
      "[Iter.  970]  loss:445.261963  pct:-0.396363973\n",
      "[Iter.  980]  loss:443.504211  pct:-0.394767937\n",
      "[Iter.  990]  loss:441.760101  pct:-0.393256718\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.836841  pct:100.000000000\n",
      "[Iter.    2]  loss:2.835918  pct:-0.032533313\n",
      "[Iter.    4]  loss:2.835412  pct:-0.017839875\n",
      "[Iter.    6]  loss:2.835156  pct:-0.009030841\n",
      "[Iter.    8]  loss:2.835005  pct:-0.005323127\n",
      "[Iter.   10]  loss:2.834894  pct:-0.003918972\n",
      "[Iter.   12]  loss:2.834816  pct:-0.002750116\n",
      "[Iter.   14]  loss:2.834759  pct:-0.001993258\n",
      "[Iter.   16]  loss:2.834734  pct:-0.000899928\n",
      "[Iter.   18]  loss:2.834675  pct:-0.002094243\n",
      "[Iter.   20]  loss:2.834646  pct:-0.001017706\n",
      "[Iter.   22]  loss:2.834606  pct:-0.001387795\n",
      "[Iter.   24]  loss:2.834585  pct:-0.000756989\n",
      "[Iter.   26]  loss:2.834573  pct:-0.000437375\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.834573\n",
      "Best loss: 2.834573 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 19%|█▊        | 1860/10000 [00:31<02:19, 58.16it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:331.124298  pct:100.000000000\n",
      "[Iter.   10]  loss:65.324623  pct:-80.271874986\n",
      "[Iter.   20]  loss:59.062038  pct:-9.586866924\n",
      "[Iter.   30]  loss:55.844292  pct:-5.448079377\n",
      "[Iter.   40]  loss:53.050404  pct:-5.002996739\n",
      "[Iter.   50]  loss:50.585892  pct:-4.645604377\n",
      "[Iter.   60]  loss:48.374001  pct:-4.372545583\n",
      "[Iter.   70]  loss:46.368778  pct:-4.145248063\n",
      "[Iter.   80]  loss:44.536880  pct:-3.950713833\n",
      "[Iter.   90]  loss:42.852638  pct:-3.781679879\n",
      "[Iter.  100]  loss:41.295673  pct:-3.633299927\n",
      "[Iter.  110]  loss:39.849594  pct:-3.501769401\n",
      "[Iter.  120]  loss:38.500942  pct:-3.384355389\n",
      "[Iter.  130]  loss:37.238575  pct:-3.278795727\n",
      "[Iter.  140]  loss:36.053173  pct:-3.183263369\n",
      "[Iter.  150]  loss:34.936840  pct:-3.096351619\n",
      "[Iter.  160]  loss:33.882793  pct:-3.017006201\n",
      "[Iter.  170]  loss:32.885284  pct:-2.943998714\n",
      "[Iter.  180]  loss:31.939217  pct:-2.876872822\n",
      "[Iter.  190]  loss:31.040222  pct:-2.814704120\n",
      "[Iter.  200]  loss:30.184462  pct:-2.756940880\n",
      "[Iter.  210]  loss:29.368557  pct:-2.703061689\n",
      "[Iter.  220]  loss:28.589485  pct:-2.652741190\n",
      "[Iter.  230]  loss:27.844587  pct:-2.605495825\n",
      "[Iter.  240]  loss:27.131422  pct:-2.561234881\n",
      "[Iter.  250]  loss:26.447826  pct:-2.519571795\n",
      "[Iter.  260]  loss:25.791924  pct:-2.479987780\n",
      "[Iter.  270]  loss:25.161873  pct:-2.442821524\n",
      "[Iter.  280]  loss:24.556097  pct:-2.407514879\n",
      "[Iter.  290]  loss:23.973148  pct:-2.373946820\n",
      "[Iter.  300]  loss:23.411699  pct:-2.341991310\n",
      "[Iter.  310]  loss:22.870548  pct:-2.311455653\n",
      "[Iter.  320]  loss:22.348562  pct:-2.282350218\n",
      "[Iter.  330]  loss:21.844736  pct:-2.254400690\n",
      "[Iter.  340]  loss:21.358091  pct:-2.227743758\n",
      "[Iter.  350]  loss:20.887758  pct:-2.202130760\n",
      "[Iter.  360]  loss:20.432911  pct:-2.177578514\n",
      "[Iter.  370]  loss:19.992836  pct:-2.153755392\n",
      "[Iter.  380]  loss:19.566824  pct:-2.130823457\n",
      "[Iter.  390]  loss:19.154215  pct:-2.108717803\n",
      "[Iter.  400]  loss:18.754410  pct:-2.087295522\n",
      "[Iter.  410]  loss:18.366785  pct:-2.066845851\n",
      "[Iter.  420]  loss:17.990860  pct:-2.046765741\n",
      "[Iter.  430]  loss:17.626110  pct:-2.027417860\n",
      "[Iter.  440]  loss:17.272104  pct:-2.008417127\n",
      "[Iter.  450]  loss:16.928373  pct:-1.990092934\n",
      "[Iter.  460]  loss:16.594524  pct:-1.972126598\n",
      "[Iter.  470]  loss:16.270153  pct:-1.954688971\n",
      "[Iter.  480]  loss:15.954888  pct:-1.937687377\n",
      "[Iter.  490]  loss:15.648377  pct:-1.921109811\n",
      "[Iter.  500]  loss:15.350307  pct:-1.904797833\n",
      "[Iter.  510]  loss:15.060346  pct:-1.888964215\n",
      "[Iter.  520]  loss:14.778222  pct:-1.873287455\n",
      "[Iter.  530]  loss:14.503643  pct:-1.857997847\n",
      "[Iter.  540]  loss:14.236334  pct:-1.843048593\n",
      "[Iter.  550]  loss:13.976046  pct:-1.828337557\n",
      "[Iter.  560]  loss:13.722555  pct:-1.813749433\n",
      "[Iter.  570]  loss:13.475621  pct:-1.799474910\n",
      "[Iter.  580]  loss:13.235014  pct:-1.785500332\n",
      "[Iter.  590]  loss:13.000533  pct:-1.771670650\n",
      "[Iter.  600]  loss:12.771983  pct:-1.758004502\n",
      "[Iter.  610]  loss:12.549171  pct:-1.744534865\n",
      "[Iter.  620]  loss:12.331919  pct:-1.731211756\n",
      "[Iter.  630]  loss:12.120046  pct:-1.718086693\n",
      "[Iter.  640]  loss:11.913383  pct:-1.705134927\n",
      "[Iter.  650]  loss:11.711786  pct:-1.692183220\n",
      "[Iter.  660]  loss:11.515085  pct:-1.679513656\n",
      "[Iter.  670]  loss:11.323141  pct:-1.666892764\n",
      "[Iter.  680]  loss:11.135837  pct:-1.654174360\n",
      "[Iter.  690]  loss:10.952998  pct:-1.641892266\n",
      "[Iter.  700]  loss:10.774526  pct:-1.629439869\n",
      "[Iter.  710]  loss:10.600288  pct:-1.617122248\n",
      "[Iter.  720]  loss:10.430155  pct:-1.604990208\n",
      "[Iter.  730]  loss:10.264006  pct:-1.592969065\n",
      "[Iter.  740]  loss:10.101750  pct:-1.580818372\n",
      "[Iter.  750]  loss:9.943255  pct:-1.568985012\n",
      "[Iter.  760]  loss:9.788439  pct:-1.557001413\n",
      "[Iter.  770]  loss:9.637208  pct:-1.544994204\n",
      "[Iter.  780]  loss:9.489414  pct:-1.533574559\n",
      "[Iter.  790]  loss:9.345023  pct:-1.521601403\n",
      "[Iter.  800]  loss:9.203939  pct:-1.509720361\n",
      "[Iter.  810]  loss:9.066035  pct:-1.498316760\n",
      "[Iter.  820]  loss:8.931264  pct:-1.486552203\n",
      "[Iter.  830]  loss:8.799521  pct:-1.475070925\n",
      "[Iter.  840]  loss:8.670766  pct:-1.463211042\n",
      "[Iter.  850]  loss:8.544898  pct:-1.451634670\n",
      "[Iter.  860]  loss:8.421830  pct:-1.440249554\n",
      "[Iter.  870]  loss:8.301524  pct:-1.428502030\n",
      "[Iter.  880]  loss:8.183881  pct:-1.417129602\n",
      "[Iter.  890]  loss:8.068843  pct:-1.405664633\n",
      "[Iter.  900]  loss:7.956353  pct:-1.394124311\n",
      "[Iter.  910]  loss:7.846347  pct:-1.382622538\n",
      "[Iter.  920]  loss:7.738759  pct:-1.371189589\n",
      "[Iter.  930]  loss:7.633536  pct:-1.359678356\n",
      "[Iter.  940]  loss:7.530613  pct:-1.348299292\n",
      "[Iter.  950]  loss:7.429947  pct:-1.336757585\n",
      "[Iter.  960]  loss:7.331472  pct:-1.325379231\n",
      "[Iter.  970]  loss:7.235132  pct:-1.314069686\n",
      "[Iter.  980]  loss:7.140905  pct:-1.302344777\n",
      "[Iter.  990]  loss:7.048707  pct:-1.291130004\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.735616  pct:100.000000000\n",
      "[Iter.    2]  loss:2.735681  pct:0.002379291\n",
      "[Iter.    4]  loss:2.735682  pct:0.000061006\n",
      "[Iter.    6]  loss:2.735683  pct:0.000026145\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.735683\n",
      "Best loss: 2.735683 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1696/10000 [00:25<02:06, 65.69it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:281.134491  pct:100.000000000\n",
      "[Iter.   10]  loss:59.760750  pct:-78.743005889\n",
      "[Iter.   20]  loss:54.547680  pct:-8.723233781\n",
      "[Iter.   30]  loss:51.207363  pct:-6.123664248\n",
      "[Iter.   40]  loss:48.343674  pct:-5.592339163\n",
      "[Iter.   50]  loss:45.840740  pct:-5.177375467\n",
      "[Iter.   60]  loss:43.621670  pct:-4.840825922\n",
      "[Iter.   70]  loss:41.631809  pct:-4.561633118\n",
      "[Iter.   80]  loss:39.830536  pct:-4.326675634\n",
      "[Iter.   90]  loss:38.186928  pct:-4.126502585\n",
      "[Iter.  100]  loss:36.676781  pct:-3.954617933\n",
      "[Iter.  110]  loss:35.281204  pct:-3.805068085\n",
      "[Iter.  120]  loss:33.985363  pct:-3.672893955\n",
      "[Iter.  130]  loss:32.777996  pct:-3.552608643\n",
      "[Iter.  140]  loss:31.650131  pct:-3.440920657\n",
      "[Iter.  150]  loss:30.593779  pct:-3.337593161\n",
      "[Iter.  160]  loss:29.601542  pct:-3.243264272\n",
      "[Iter.  170]  loss:28.666691  pct:-3.158114898\n",
      "[Iter.  180]  loss:27.783491  pct:-3.080926561\n",
      "[Iter.  190]  loss:26.947079  pct:-3.010465552\n",
      "[Iter.  200]  loss:26.153385  pct:-2.945378797\n",
      "[Iter.  210]  loss:25.398939  pct:-2.884697430\n",
      "[Iter.  220]  loss:24.680773  pct:-2.827544676\n",
      "[Iter.  230]  loss:23.996120  pct:-2.774031164\n",
      "[Iter.  240]  loss:23.340881  pct:-2.730604335\n",
      "[Iter.  250]  loss:22.715273  pct:-2.680312002\n",
      "[Iter.  260]  loss:22.117065  pct:-2.633503354\n",
      "[Iter.  270]  loss:21.543903  pct:-2.591492441\n",
      "[Iter.  280]  loss:20.994135  pct:-2.551851626\n",
      "[Iter.  290]  loss:20.466249  pct:-2.514442436\n",
      "[Iter.  300]  loss:19.958757  pct:-2.479653472\n",
      "[Iter.  310]  loss:19.470589  pct:-2.445887320\n",
      "[Iter.  320]  loss:19.000830  pct:-2.412659396\n",
      "[Iter.  330]  loss:18.547447  pct:-2.386119445\n",
      "[Iter.  340]  loss:18.110542  pct:-2.355606690\n",
      "[Iter.  350]  loss:17.689743  pct:-2.323504445\n",
      "[Iter.  360]  loss:17.283623  pct:-2.295795362\n",
      "[Iter.  370]  loss:16.891443  pct:-2.269081517\n",
      "[Iter.  380]  loss:16.512461  pct:-2.243636250\n",
      "[Iter.  390]  loss:16.146112  pct:-2.218617038\n",
      "[Iter.  400]  loss:15.791755  pct:-2.194693742\n",
      "[Iter.  410]  loss:15.448858  pct:-2.171363902\n",
      "[Iter.  420]  loss:15.116907  pct:-2.148709864\n",
      "[Iter.  430]  loss:14.795398  pct:-2.126819717\n",
      "[Iter.  440]  loss:14.483896  pct:-2.105394583\n",
      "[Iter.  450]  loss:14.181997  pct:-2.084376683\n",
      "[Iter.  460]  loss:13.889280  pct:-2.064003919\n",
      "[Iter.  470]  loss:13.605369  pct:-2.044106667\n",
      "[Iter.  480]  loss:13.329937  pct:-2.024433448\n",
      "[Iter.  490]  loss:13.062613  pct:-2.005444197\n",
      "[Iter.  500]  loss:12.803097  pct:-1.986706424\n",
      "[Iter.  510]  loss:12.551106  pct:-1.968198185\n",
      "[Iter.  520]  loss:12.306346  pct:-1.950111046\n",
      "[Iter.  530]  loss:12.068534  pct:-1.932434237\n",
      "[Iter.  540]  loss:11.837431  pct:-1.914921443\n",
      "[Iter.  550]  loss:11.612804  pct:-1.897595365\n",
      "[Iter.  560]  loss:11.394410  pct:-1.880633409\n",
      "[Iter.  570]  loss:11.182045  pct:-1.863766074\n",
      "[Iter.  580]  loss:10.975487  pct:-1.847231234\n",
      "[Iter.  590]  loss:10.774555  pct:-1.830730186\n",
      "[Iter.  600]  loss:10.579039  pct:-1.814613992\n",
      "[Iter.  610]  loss:10.388787  pct:-1.798380337\n",
      "[Iter.  620]  loss:10.203603  pct:-1.782541830\n",
      "[Iter.  630]  loss:10.023329  pct:-1.766768203\n",
      "[Iter.  640]  loss:9.847815  pct:-1.751057209\n",
      "[Iter.  650]  loss:9.676898  pct:-1.735578552\n",
      "[Iter.  660]  loss:9.510445  pct:-1.720110737\n",
      "[Iter.  670]  loss:9.348330  pct:-1.704590579\n",
      "[Iter.  680]  loss:9.190402  pct:-1.689376160\n",
      "[Iter.  690]  loss:9.036531  pct:-1.674253009\n",
      "[Iter.  700]  loss:8.886598  pct:-1.659196516\n",
      "[Iter.  710]  loss:8.740471  pct:-1.644349763\n",
      "[Iter.  720]  loss:8.598053  pct:-1.629407724\n",
      "[Iter.  730]  loss:8.459238  pct:-1.614492566\n",
      "[Iter.  740]  loss:8.323905  pct:-1.599825663\n",
      "[Iter.  750]  loss:8.191983  pct:-1.584854324\n",
      "[Iter.  760]  loss:8.063352  pct:-1.570213077\n",
      "[Iter.  770]  loss:7.937907  pct:-1.555735356\n",
      "[Iter.  780]  loss:7.815580  pct:-1.541046620\n",
      "[Iter.  790]  loss:7.696275  pct:-1.526509933\n",
      "[Iter.  800]  loss:7.579926  pct:-1.511760221\n",
      "[Iter.  810]  loss:7.466434  pct:-1.497258222\n",
      "[Iter.  820]  loss:7.355731  pct:-1.482688229\n",
      "[Iter.  830]  loss:7.247724  pct:-1.468337559\n",
      "[Iter.  840]  loss:7.142355  pct:-1.453810099\n",
      "[Iter.  850]  loss:7.039555  pct:-1.439306264\n",
      "[Iter.  860]  loss:6.939234  pct:-1.425108431\n",
      "[Iter.  870]  loss:6.841352  pct:-1.410556224\n",
      "[Iter.  880]  loss:6.745838  pct:-1.396131900\n",
      "[Iter.  890]  loss:6.652609  pct:-1.382012797\n",
      "[Iter.  900]  loss:6.561633  pct:-1.367527138\n",
      "[Iter.  910]  loss:6.472838  pct:-1.353240981\n",
      "[Iter.  920]  loss:6.386169  pct:-1.338963880\n",
      "[Iter.  930]  loss:6.301578  pct:-1.324602950\n",
      "[Iter.  940]  loss:6.218988  pct:-1.310625728\n",
      "[Iter.  950]  loss:6.138370  pct:-1.296318716\n",
      "[Iter.  960]  loss:6.059680  pct:-1.281929482\n",
      "[Iter.  970]  loss:5.982839  pct:-1.268076012\n",
      "[Iter.  980]  loss:5.907816  pct:-1.253972787\n",
      "[Iter.  990]  loss:5.834584  pct:-1.239573100\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.731102  pct:100.000000000\n",
      "[Iter.    2]  loss:2.731150  pct:0.001763411\n",
      "[Iter.    4]  loss:2.731146  pct:-0.000157133\n",
      "[Iter.    6]  loss:2.731144  pct:-0.000061107\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.731144\n",
      "Best loss: 2.731144 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 80%|███████▉  | 7956/10000 [01:56<00:29, 68.47it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:626.169434  pct:100.000000000\n",
      "[Iter.   10]  loss:104.390648  pct:-83.328693577\n",
      "[Iter.   20]  loss:99.894730  pct:-4.306820932\n",
      "[Iter.   30]  loss:95.960915  pct:-3.937960509\n",
      "[Iter.   40]  loss:92.402702  pct:-3.707980791\n",
      "[Iter.   50]  loss:89.157288  pct:-3.512250889\n",
      "[Iter.   60]  loss:86.176529  pct:-3.343258580\n",
      "[Iter.   70]  loss:83.422638  pct:-3.195639260\n",
      "[Iter.   80]  loss:80.865250  pct:-3.065580721\n",
      "[Iter.   90]  loss:78.479607  pct:-2.950146096\n",
      "[Iter.  100]  loss:76.245346  pct:-2.846931394\n",
      "[Iter.  110]  loss:74.145638  pct:-2.753884224\n",
      "[Iter.  120]  loss:72.166229  pct:-2.669622018\n",
      "[Iter.  130]  loss:70.295090  pct:-2.592818755\n",
      "[Iter.  140]  loss:68.522057  pct:-2.522271682\n",
      "[Iter.  150]  loss:66.838440  pct:-2.457043355\n",
      "[Iter.  160]  loss:65.236534  pct:-2.396683442\n",
      "[Iter.  170]  loss:63.709709  pct:-2.340444617\n",
      "[Iter.  180]  loss:62.251984  pct:-2.288074367\n",
      "[Iter.  190]  loss:60.857811  pct:-2.239563443\n",
      "[Iter.  200]  loss:59.522415  pct:-2.194288279\n",
      "[Iter.  210]  loss:58.241386  pct:-2.152178711\n",
      "[Iter.  220]  loss:57.010971  pct:-2.112613418\n",
      "[Iter.  230]  loss:55.827629  pct:-2.075639053\n",
      "[Iter.  240]  loss:54.688267  pct:-2.040857464\n",
      "[Iter.  250]  loss:53.590130  pct:-2.007993610\n",
      "[Iter.  260]  loss:52.530712  pct:-1.976889639\n",
      "[Iter.  270]  loss:51.507664  pct:-1.947524333\n",
      "[Iter.  280]  loss:50.518898  pct:-1.919647767\n",
      "[Iter.  290]  loss:49.562618  pct:-1.892914914\n",
      "[Iter.  300]  loss:48.636982  pct:-1.867609751\n",
      "[Iter.  310]  loss:47.740345  pct:-1.843529197\n",
      "[Iter.  320]  loss:46.871197  pct:-1.820573886\n",
      "[Iter.  330]  loss:46.028164  pct:-1.798615985\n",
      "[Iter.  340]  loss:45.209915  pct:-1.777713207\n",
      "[Iter.  350]  loss:44.415348  pct:-1.757506302\n",
      "[Iter.  360]  loss:43.643353  pct:-1.738127873\n",
      "[Iter.  370]  loss:42.892746  pct:-1.719864524\n",
      "[Iter.  380]  loss:42.162621  pct:-1.702211903\n",
      "[Iter.  390]  loss:41.452068  pct:-1.685265779\n",
      "[Iter.  400]  loss:40.760262  pct:-1.668931904\n",
      "[Iter.  410]  loss:40.086365  pct:-1.653318120\n",
      "[Iter.  420]  loss:39.429718  pct:-1.638080012\n",
      "[Iter.  430]  loss:38.789585  pct:-1.623478270\n",
      "[Iter.  440]  loss:38.165329  pct:-1.609339549\n",
      "[Iter.  450]  loss:37.556293  pct:-1.595782110\n",
      "[Iter.  460]  loss:36.961948  pct:-1.582544595\n",
      "[Iter.  470]  loss:36.381706  pct:-1.569836500\n",
      "[Iter.  480]  loss:35.814922  pct:-1.557881594\n",
      "[Iter.  490]  loss:35.261253  pct:-1.545917008\n",
      "[Iter.  500]  loss:34.720367  pct:-1.533938456\n",
      "[Iter.  510]  loss:34.191658  pct:-1.522764448\n",
      "[Iter.  520]  loss:33.674690  pct:-1.511970473\n",
      "[Iter.  530]  loss:33.169109  pct:-1.501367640\n",
      "[Iter.  540]  loss:32.674496  pct:-1.491187606\n",
      "[Iter.  550]  loss:32.190514  pct:-1.481222819\n",
      "[Iter.  560]  loss:31.716831  pct:-1.471496880\n",
      "[Iter.  570]  loss:31.253103  pct:-1.462087899\n",
      "[Iter.  580]  loss:30.799068  pct:-1.452767111\n",
      "[Iter.  590]  loss:30.354359  pct:-1.443906586\n",
      "[Iter.  600]  loss:29.918722  pct:-1.435169575\n",
      "[Iter.  610]  loss:29.491940  pct:-1.426473383\n",
      "[Iter.  620]  loss:29.073648  pct:-1.418323442\n",
      "[Iter.  630]  loss:28.663692  pct:-1.410060313\n",
      "[Iter.  640]  loss:28.261755  pct:-1.402252990\n",
      "[Iter.  650]  loss:27.867704  pct:-1.394289202\n",
      "[Iter.  660]  loss:27.481228  pct:-1.386825808\n",
      "[Iter.  670]  loss:27.102175  pct:-1.379316520\n",
      "[Iter.  680]  loss:26.730286  pct:-1.372174439\n",
      "[Iter.  690]  loss:26.365433  pct:-1.364942037\n",
      "[Iter.  700]  loss:26.007395  pct:-1.357982447\n",
      "[Iter.  710]  loss:25.656033  pct:-1.351008939\n",
      "[Iter.  720]  loss:25.311085  pct:-1.344509577\n",
      "[Iter.  730]  loss:24.972475  pct:-1.337792113\n",
      "[Iter.  740]  loss:24.640017  pct:-1.331299743\n",
      "[Iter.  750]  loss:24.313507  pct:-1.325118735\n",
      "[Iter.  760]  loss:23.992882  pct:-1.318712698\n",
      "[Iter.  770]  loss:23.677914  pct:-1.312756475\n",
      "[Iter.  780]  loss:23.368509  pct:-1.306721435\n",
      "[Iter.  790]  loss:23.064573  pct:-1.300622136\n",
      "[Iter.  800]  loss:22.765905  pct:-1.294920587\n",
      "[Iter.  810]  loss:22.472433  pct:-1.289086839\n",
      "[Iter.  820]  loss:22.184046  pct:-1.283293613\n",
      "[Iter.  830]  loss:21.900581  pct:-1.277785100\n",
      "[Iter.  840]  loss:21.621960  pct:-1.272211313\n",
      "[Iter.  850]  loss:21.348032  pct:-1.266895751\n",
      "[Iter.  860]  loss:21.078720  pct:-1.261530360\n",
      "[Iter.  870]  loss:20.813908  pct:-1.256302415\n",
      "[Iter.  880]  loss:20.553473  pct:-1.251255214\n",
      "[Iter.  890]  loss:20.297367  pct:-1.246044544\n",
      "[Iter.  900]  loss:20.045485  pct:-1.240961707\n",
      "[Iter.  910]  loss:19.797766  pct:-1.235783603\n",
      "[Iter.  920]  loss:19.554104  pct:-1.230754439\n",
      "[Iter.  930]  loss:19.314375  pct:-1.225977572\n",
      "[Iter.  940]  loss:19.078512  pct:-1.221177143\n",
      "[Iter.  950]  loss:18.846451  pct:-1.216349492\n",
      "[Iter.  960]  loss:18.618116  pct:-1.211551338\n",
      "[Iter.  970]  loss:18.393402  pct:-1.206965703\n",
      "[Iter.  980]  loss:18.172316  pct:-1.201988087\n",
      "[Iter.  990]  loss:17.954695  pct:-1.197540558\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.736405  pct:100.000000000\n",
      "[Iter.    2]  loss:2.736320  pct:-0.003101771\n",
      "[Iter.    4]  loss:2.736282  pct:-0.001367958\n",
      "[Iter.    6]  loss:2.736264  pct:-0.000670919\n",
      "[Iter.    8]  loss:2.736251  pct:-0.000479231\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.736251\n",
      "Best loss: 2.736251 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 18%|█▊        | 1784/10000 [00:23<01:46, 77.20it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:405.693695  pct:100.000000000\n",
      "[Iter.   10]  loss:84.357872  pct:-79.206509575\n",
      "[Iter.   20]  loss:78.713562  pct:-6.690910834\n",
      "[Iter.   30]  loss:74.371078  pct:-5.516817445\n",
      "[Iter.   40]  loss:70.629868  pct:-5.030464817\n",
      "[Iter.   50]  loss:67.343239  pct:-4.653312879\n",
      "[Iter.   60]  loss:64.415169  pct:-4.347979276\n",
      "[Iter.   70]  loss:61.777840  pct:-4.094267161\n",
      "[Iter.   80]  loss:59.381660  pct:-3.878703452\n",
      "[Iter.   90]  loss:57.189182  pct:-3.692180655\n",
      "[Iter.  100]  loss:55.171104  pct:-3.528775495\n",
      "[Iter.  110]  loss:53.303650  pct:-3.384841663\n",
      "[Iter.  120]  loss:51.567425  pct:-3.257234976\n",
      "[Iter.  130]  loss:49.946434  pct:-3.143439410\n",
      "[Iter.  140]  loss:48.427460  pct:-3.041206713\n",
      "[Iter.  150]  loss:46.999435  pct:-2.948790418\n",
      "[Iter.  160]  loss:45.652985  pct:-2.864823361\n",
      "[Iter.  170]  loss:44.380035  pct:-2.788315440\n",
      "[Iter.  180]  loss:43.173580  pct:-2.718463876\n",
      "[Iter.  190]  loss:42.027611  pct:-2.654330232\n",
      "[Iter.  200]  loss:40.936943  pct:-2.595121884\n",
      "[Iter.  210]  loss:39.897045  pct:-2.540243216\n",
      "[Iter.  220]  loss:38.904087  pct:-2.488801026\n",
      "[Iter.  230]  loss:37.954880  pct:-2.439865262\n",
      "[Iter.  240]  loss:37.046547  pct:-2.393191153\n",
      "[Iter.  250]  loss:36.175869  pct:-2.350226998\n",
      "[Iter.  260]  loss:35.340172  pct:-2.310095645\n",
      "[Iter.  270]  loss:34.537010  pct:-2.272659073\n",
      "[Iter.  280]  loss:33.764324  pct:-2.237269527\n",
      "[Iter.  290]  loss:33.020222  pct:-2.203812740\n",
      "[Iter.  300]  loss:32.302944  pct:-2.172237162\n",
      "[Iter.  310]  loss:31.610922  pct:-2.142288702\n",
      "[Iter.  320]  loss:30.942760  pct:-2.113704229\n",
      "[Iter.  330]  loss:30.297140  pct:-2.086498865\n",
      "[Iter.  340]  loss:29.672842  pct:-2.060584244\n",
      "[Iter.  350]  loss:29.068768  pct:-2.035782341\n",
      "[Iter.  360]  loss:28.483883  pct:-2.012072382\n",
      "[Iter.  370]  loss:27.917229  pct:-1.989385391\n",
      "[Iter.  380]  loss:27.367922  pct:-1.967626785\n",
      "[Iter.  390]  loss:26.835163  pct:-1.946653882\n",
      "[Iter.  400]  loss:26.318182  pct:-1.926506363\n",
      "[Iter.  410]  loss:25.816275  pct:-1.907074542\n",
      "[Iter.  420]  loss:25.328758  pct:-1.888407254\n",
      "[Iter.  430]  loss:24.854988  pct:-1.870483097\n",
      "[Iter.  440]  loss:24.394375  pct:-1.853202459\n",
      "[Iter.  450]  loss:23.946388  pct:-1.836434037\n",
      "[Iter.  460]  loss:23.510515  pct:-1.820203645\n",
      "[Iter.  470]  loss:23.086292  pct:-1.804396638\n",
      "[Iter.  480]  loss:22.673227  pct:-1.789221725\n",
      "[Iter.  490]  loss:22.270950  pct:-1.774237903\n",
      "[Iter.  500]  loss:21.878998  pct:-1.759927210\n",
      "[Iter.  510]  loss:21.496994  pct:-1.745983923\n",
      "[Iter.  520]  loss:21.124580  pct:-1.732398655\n",
      "[Iter.  530]  loss:20.761429  pct:-1.719094740\n",
      "[Iter.  540]  loss:20.407183  pct:-1.706270519\n",
      "[Iter.  550]  loss:20.061583  pct:-1.693521998\n",
      "[Iter.  560]  loss:19.724308  pct:-1.681196138\n",
      "[Iter.  570]  loss:19.395067  pct:-1.669213433\n",
      "[Iter.  580]  loss:19.073593  pct:-1.657504312\n",
      "[Iter.  590]  loss:18.759632  pct:-1.646050782\n",
      "[Iter.  600]  loss:18.452927  pct:-1.634922652\n",
      "[Iter.  610]  loss:18.153257  pct:-1.623966061\n",
      "[Iter.  620]  loss:17.860439  pct:-1.613033207\n",
      "[Iter.  630]  loss:17.574263  pct:-1.602293632\n",
      "[Iter.  640]  loss:17.294500  pct:-1.591886238\n",
      "[Iter.  650]  loss:17.020947  pct:-1.581738950\n",
      "[Iter.  660]  loss:16.753433  pct:-1.571670971\n",
      "[Iter.  670]  loss:16.491776  pct:-1.561815488\n",
      "[Iter.  680]  loss:16.235813  pct:-1.552060733\n",
      "[Iter.  690]  loss:15.985344  pct:-1.542695802\n",
      "[Iter.  700]  loss:15.740262  pct:-1.533166271\n",
      "[Iter.  710]  loss:15.500401  pct:-1.523872270\n",
      "[Iter.  720]  loss:15.265625  pct:-1.514641783\n",
      "[Iter.  730]  loss:15.035789  pct:-1.505581749\n",
      "[Iter.  740]  loss:14.810758  pct:-1.496635168\n",
      "[Iter.  750]  loss:14.590386  pct:-1.487913392\n",
      "[Iter.  760]  loss:14.374615  pct:-1.478861967\n",
      "[Iter.  770]  loss:14.163275  pct:-1.470230366\n",
      "[Iter.  780]  loss:13.956252  pct:-1.461686442\n",
      "[Iter.  790]  loss:13.753455  pct:-1.453090232\n",
      "[Iter.  800]  loss:13.554742  pct:-1.444824593\n",
      "[Iter.  810]  loss:13.360023  pct:-1.436540191\n",
      "[Iter.  820]  loss:13.169194  pct:-1.428353303\n",
      "[Iter.  830]  loss:12.982186  pct:-1.420040596\n",
      "[Iter.  840]  loss:12.798866  pct:-1.412089158\n",
      "[Iter.  850]  loss:12.619180  pct:-1.403925492\n",
      "[Iter.  860]  loss:12.443031  pct:-1.395878484\n",
      "[Iter.  870]  loss:12.270340  pct:-1.387855908\n",
      "[Iter.  880]  loss:12.101027  pct:-1.379851557\n",
      "[Iter.  890]  loss:11.935001  pct:-1.372000151\n",
      "[Iter.  900]  loss:11.772197  pct:-1.364093715\n",
      "[Iter.  910]  loss:11.612540  pct:-1.356216922\n",
      "[Iter.  920]  loss:11.455931  pct:-1.348624262\n",
      "[Iter.  930]  loss:11.302334  pct:-1.340762981\n",
      "[Iter.  940]  loss:11.151677  pct:-1.332969831\n",
      "[Iter.  950]  loss:11.003897  pct:-1.325185590\n",
      "[Iter.  960]  loss:10.858893  pct:-1.317745182\n",
      "[Iter.  970]  loss:10.716637  pct:-1.310048194\n",
      "[Iter.  980]  loss:10.577068  pct:-1.302351972\n",
      "[Iter.  990]  loss:10.440113  pct:-1.294831961\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.791959  pct:100.000000000\n",
      "[Iter.    2]  loss:2.791984  pct:0.000905184\n",
      "[Iter.    4]  loss:2.791981  pct:-0.000093933\n",
      "[Iter.    6]  loss:2.791981  pct:-0.000025618\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.791981\n",
      "Best loss: 2.791981 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 20%|█▉        | 1991/10000 [00:39<02:38, 50.52it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:669.788208  pct:100.000000000\n",
      "[Iter.   10]  loss:105.643410  pct:-84.227337634\n",
      "[Iter.   20]  loss:101.460953  pct:-3.959032543\n",
      "[Iter.   30]  loss:97.741531  pct:-3.665864833\n",
      "[Iter.   40]  loss:94.342194  pct:-3.477884703\n",
      "[Iter.   50]  loss:91.310570  pct:-3.213433698\n",
      "[Iter.   60]  loss:88.518921  pct:-3.057311845\n",
      "[Iter.   70]  loss:85.922920  pct:-2.932707092\n",
      "[Iter.   80]  loss:83.497185  pct:-2.823152969\n",
      "[Iter.   90]  loss:81.221848  pct:-2.725046630\n",
      "[Iter.  100]  loss:79.080482  pct:-2.636439722\n",
      "[Iter.  110]  loss:77.059113  pct:-2.556092060\n",
      "[Iter.  120]  loss:75.145981  pct:-2.482680699\n",
      "[Iter.  130]  loss:73.330826  pct:-2.415505140\n",
      "[Iter.  140]  loss:71.604858  pct:-2.353672399\n",
      "[Iter.  150]  loss:69.960396  pct:-2.296579621\n",
      "[Iter.  160]  loss:68.390717  pct:-2.243668353\n",
      "[Iter.  170]  loss:66.889931  pct:-2.194429161\n",
      "[Iter.  180]  loss:65.452728  pct:-2.148608076\n",
      "[Iter.  190]  loss:64.074432  pct:-2.105788307\n",
      "[Iter.  200]  loss:62.750858  pct:-2.065682078\n",
      "[Iter.  210]  loss:61.478241  pct:-2.028047702\n",
      "[Iter.  220]  loss:60.253170  pct:-1.992690315\n",
      "[Iter.  230]  loss:59.072659  pct:-1.959252060\n",
      "[Iter.  240]  loss:57.933895  pct:-1.927733500\n",
      "[Iter.  250]  loss:56.834366  pct:-1.897903230\n",
      "[Iter.  260]  loss:55.771748  pct:-1.869675574\n",
      "[Iter.  270]  loss:54.743904  pct:-1.842946509\n",
      "[Iter.  280]  loss:53.748978  pct:-1.817419617\n",
      "[Iter.  290]  loss:52.785099  pct:-1.793296679\n",
      "[Iter.  300]  loss:51.850735  pct:-1.770128949\n",
      "[Iter.  310]  loss:50.944328  pct:-1.748107153\n",
      "[Iter.  320]  loss:50.064507  pct:-1.727025965\n",
      "[Iter.  330]  loss:49.209980  pct:-1.706850979\n",
      "[Iter.  340]  loss:48.379517  pct:-1.687591438\n",
      "[Iter.  350]  loss:47.571941  pct:-1.669250300\n",
      "[Iter.  360]  loss:46.786331  pct:-1.651415049\n",
      "[Iter.  370]  loss:46.021587  pct:-1.634545359\n",
      "[Iter.  380]  loss:45.276939  pct:-1.618040625\n",
      "[Iter.  390]  loss:44.551495  pct:-1.602239028\n",
      "[Iter.  400]  loss:43.844406  pct:-1.587126261\n",
      "[Iter.  410]  loss:43.154961  pct:-1.572482231\n",
      "[Iter.  420]  loss:42.482403  pct:-1.558471659\n",
      "[Iter.  430]  loss:41.826065  pct:-1.544963784\n",
      "[Iter.  440]  loss:41.185413  pct:-1.531704457\n",
      "[Iter.  450]  loss:40.559807  pct:-1.519000262\n",
      "[Iter.  460]  loss:39.948608  pct:-1.506906648\n",
      "[Iter.  470]  loss:39.351421  pct:-1.494888223\n",
      "[Iter.  480]  loss:38.767658  pct:-1.483461340\n",
      "[Iter.  490]  loss:38.196911  pct:-1.472225565\n",
      "[Iter.  500]  loss:37.638641  pct:-1.461556676\n",
      "[Iter.  510]  loss:37.092525  pct:-1.450944709\n",
      "[Iter.  520]  loss:36.558102  pct:-1.440785768\n",
      "[Iter.  530]  loss:36.035000  pct:-1.430877926\n",
      "[Iter.  540]  loss:35.522835  pct:-1.421298936\n",
      "[Iter.  550]  loss:35.021286  pct:-1.411905244\n",
      "[Iter.  560]  loss:34.530006  pct:-1.402802861\n",
      "[Iter.  570]  loss:34.048637  pct:-1.394060032\n",
      "[Iter.  580]  loss:33.576920  pct:-1.385423531\n",
      "[Iter.  590]  loss:33.114498  pct:-1.377200242\n",
      "[Iter.  600]  loss:32.661201  pct:-1.368876736\n",
      "[Iter.  610]  loss:32.216717  pct:-1.360895162\n",
      "[Iter.  620]  loss:31.780851  pct:-1.352916889\n",
      "[Iter.  630]  loss:31.353262  pct:-1.345430969\n",
      "[Iter.  640]  loss:30.933758  pct:-1.337992093\n",
      "[Iter.  650]  loss:30.522120  pct:-1.330708874\n",
      "[Iter.  660]  loss:30.118132  pct:-1.323590533\n",
      "[Iter.  670]  loss:29.721581  pct:-1.316652497\n",
      "[Iter.  680]  loss:29.332239  pct:-1.309961811\n",
      "[Iter.  690]  loss:28.949945  pct:-1.303322597\n",
      "[Iter.  700]  loss:28.574518  pct:-1.296815038\n",
      "[Iter.  710]  loss:28.205832  pct:-1.290263841\n",
      "[Iter.  720]  loss:27.843681  pct:-1.283955029\n",
      "[Iter.  730]  loss:27.487898  pct:-1.277788875\n",
      "[Iter.  740]  loss:27.138315  pct:-1.271769394\n",
      "[Iter.  750]  loss:26.794827  pct:-1.265696454\n",
      "[Iter.  760]  loss:26.457239  pct:-1.259897527\n",
      "[Iter.  770]  loss:26.125370  pct:-1.254360379\n",
      "[Iter.  780]  loss:25.799147  pct:-1.248684222\n",
      "[Iter.  790]  loss:25.478357  pct:-1.243410650\n",
      "[Iter.  800]  loss:25.162916  pct:-1.238074840\n",
      "[Iter.  810]  loss:24.852776  pct:-1.232530473\n",
      "[Iter.  820]  loss:24.547741  pct:-1.227366483\n",
      "[Iter.  830]  loss:24.247753  pct:-1.222058656\n",
      "[Iter.  840]  loss:23.952648  pct:-1.217040518\n",
      "[Iter.  850]  loss:23.662334  pct:-1.212031834\n",
      "[Iter.  860]  loss:23.376734  pct:-1.206984302\n",
      "[Iter.  870]  loss:23.095699  pct:-1.202197331\n",
      "[Iter.  880]  loss:22.819155  pct:-1.197385570\n",
      "[Iter.  890]  loss:22.547007  pct:-1.192630207\n",
      "[Iter.  900]  loss:22.279144  pct:-1.188017215\n",
      "[Iter.  910]  loss:22.015522  pct:-1.183269342\n",
      "[Iter.  920]  loss:21.756006  pct:-1.178785415\n",
      "[Iter.  930]  loss:21.500528  pct:-1.174286781\n",
      "[Iter.  940]  loss:21.249048  pct:-1.169646153\n",
      "[Iter.  950]  loss:21.001364  pct:-1.165626225\n",
      "[Iter.  960]  loss:20.757496  pct:-1.161200182\n",
      "[Iter.  970]  loss:20.517378  pct:-1.156777427\n",
      "[Iter.  980]  loss:20.280937  pct:-1.152392183\n",
      "[Iter.  990]  loss:20.047993  pct:-1.148588383\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.767225  pct:100.000000000\n",
      "[Iter.    2]  loss:2.767240  pct:0.000568643\n",
      "[Iter.    4]  loss:2.767233  pct:-0.000275704\n",
      "[Iter.    6]  loss:2.767230  pct:-0.000112005\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.767230\n",
      "Best loss: 2.767230 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 25%|██▌       | 2535/10000 [00:32<01:36, 77.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:989.296814  pct:100.000000000\n",
      "[Iter.   10]  loss:164.863098  pct:-83.335325070\n",
      "[Iter.   20]  loss:159.397079  pct:-3.315489481\n",
      "[Iter.   30]  loss:154.626617  pct:-2.992816463\n",
      "[Iter.   40]  loss:150.226776  pct:-2.845461785\n",
      "[Iter.   50]  loss:146.107224  pct:-2.742222604\n",
      "[Iter.   60]  loss:142.213501  pct:-2.664976064\n",
      "[Iter.   70]  loss:138.594589  pct:-2.544703364\n",
      "[Iter.   80]  loss:135.235138  pct:-2.423941160\n",
      "[Iter.   90]  loss:132.082367  pct:-2.331325308\n",
      "[Iter.  100]  loss:129.105896  pct:-2.253496069\n",
      "[Iter.  110]  loss:126.285797  pct:-2.184330046\n",
      "[Iter.  120]  loss:123.606773  pct:-2.121397500\n",
      "[Iter.  130]  loss:121.056030  pct:-2.063594926\n",
      "[Iter.  140]  loss:118.622658  pct:-2.010120844\n",
      "[Iter.  150]  loss:116.296867  pct:-1.960662869\n",
      "[Iter.  160]  loss:114.070152  pct:-1.914681916\n",
      "[Iter.  170]  loss:111.935013  pct:-1.871777518\n",
      "[Iter.  180]  loss:109.884727  pct:-1.831674726\n",
      "[Iter.  190]  loss:107.913223  pct:-1.794156710\n",
      "[Iter.  200]  loss:106.015259  pct:-1.758787682\n",
      "[Iter.  210]  loss:104.185760  pct:-1.725693369\n",
      "[Iter.  220]  loss:102.420486  pct:-1.694352510\n",
      "[Iter.  230]  loss:100.715416  pct:-1.664774846\n",
      "[Iter.  240]  loss:99.066887  pct:-1.636818989\n",
      "[Iter.  250]  loss:97.471588  pct:-1.610324920\n",
      "[Iter.  260]  loss:95.926529  pct:-1.585138022\n",
      "[Iter.  270]  loss:94.428886  pct:-1.561239142\n",
      "[Iter.  280]  loss:92.976089  pct:-1.538509021\n",
      "[Iter.  290]  loss:91.565910  pct:-1.516711604\n",
      "[Iter.  300]  loss:90.195984  pct:-1.496109685\n",
      "[Iter.  310]  loss:88.864441  pct:-1.476277448\n",
      "[Iter.  320]  loss:87.569374  pct:-1.457351017\n",
      "[Iter.  330]  loss:86.309036  pct:-1.439244990\n",
      "[Iter.  340]  loss:85.081718  pct:-1.422003840\n",
      "[Iter.  350]  loss:83.886055  pct:-1.405311827\n",
      "[Iter.  360]  loss:82.720604  pct:-1.389326331\n",
      "[Iter.  370]  loss:81.584007  pct:-1.374018836\n",
      "[Iter.  380]  loss:80.475090  pct:-1.359233597\n",
      "[Iter.  390]  loss:79.392647  pct:-1.345066202\n",
      "[Iter.  400]  loss:78.335625  pct:-1.331385383\n",
      "[Iter.  410]  loss:77.302994  pct:-1.318213679\n",
      "[Iter.  420]  loss:76.293747  pct:-1.305572756\n",
      "[Iter.  430]  loss:75.306976  pct:-1.293383363\n",
      "[Iter.  440]  loss:74.341888  pct:-1.281538495\n",
      "[Iter.  450]  loss:73.397636  pct:-1.270148007\n",
      "[Iter.  460]  loss:72.473656  pct:-1.258869847\n",
      "[Iter.  470]  loss:71.568993  pct:-1.248264735\n",
      "[Iter.  480]  loss:70.682999  pct:-1.237957843\n",
      "[Iter.  490]  loss:69.815056  pct:-1.227937165\n",
      "[Iter.  500]  loss:68.964531  pct:-1.218254275\n",
      "[Iter.  510]  loss:68.130913  pct:-1.208763625\n",
      "[Iter.  520]  loss:67.313507  pct:-1.199757448\n",
      "[Iter.  530]  loss:66.511955  pct:-1.190774116\n",
      "[Iter.  540]  loss:65.725655  pct:-1.182194473\n",
      "[Iter.  550]  loss:64.954193  pct:-1.173760066\n",
      "[Iter.  560]  loss:64.197067  pct:-1.165630452\n",
      "[Iter.  570]  loss:63.453796  pct:-1.157795684\n",
      "[Iter.  580]  loss:62.724030  pct:-1.150075941\n",
      "[Iter.  590]  loss:62.007332  pct:-1.142620616\n",
      "[Iter.  600]  loss:61.303329  pct:-1.135353448\n",
      "[Iter.  610]  loss:60.611645  pct:-1.128298787\n",
      "[Iter.  620]  loss:59.932018  pct:-1.121280354\n",
      "[Iter.  630]  loss:59.263988  pct:-1.114645901\n",
      "[Iter.  640]  loss:58.607315  pct:-1.108047987\n",
      "[Iter.  650]  loss:57.961647  pct:-1.101685053\n",
      "[Iter.  660]  loss:57.326702  pct:-1.095456993\n",
      "[Iter.  670]  loss:56.702213  pct:-1.089350700\n",
      "[Iter.  680]  loss:56.087830  pct:-1.083526836\n",
      "[Iter.  690]  loss:55.483467  pct:-1.077528748\n",
      "[Iter.  700]  loss:54.888733  pct:-1.071912451\n",
      "[Iter.  710]  loss:54.303391  pct:-1.066416323\n",
      "[Iter.  720]  loss:53.727310  pct:-1.060855164\n",
      "[Iter.  730]  loss:53.160213  pct:-1.055509216\n",
      "[Iter.  740]  loss:52.601830  pct:-1.050379419\n",
      "[Iter.  750]  loss:52.052021  pct:-1.045226957\n",
      "[Iter.  760]  loss:51.510525  pct:-1.040298275\n",
      "[Iter.  770]  loss:50.977142  pct:-1.035482396\n",
      "[Iter.  780]  loss:50.451653  pct:-1.030834180\n",
      "[Iter.  790]  loss:49.934032  pct:-1.025972512\n",
      "[Iter.  800]  loss:49.423946  pct:-1.021519863\n",
      "[Iter.  810]  loss:48.921230  pct:-1.017150797\n",
      "[Iter.  820]  loss:48.425819  pct:-1.012670605\n",
      "[Iter.  830]  loss:47.937504  pct:-1.008378564\n",
      "[Iter.  840]  loss:47.456169  pct:-1.004087923\n",
      "[Iter.  850]  loss:46.981583  pct:-1.000052249\n",
      "[Iter.  860]  loss:46.513672  pct:-0.995945092\n",
      "[Iter.  870]  loss:46.052261  pct:-0.991989030\n",
      "[Iter.  880]  loss:45.597202  pct:-0.988136170\n",
      "[Iter.  890]  loss:45.148457  pct:-0.984151888\n",
      "[Iter.  900]  loss:44.705784  pct:-0.980482530\n",
      "[Iter.  910]  loss:44.269073  pct:-0.976854268\n",
      "[Iter.  920]  loss:43.838264  pct:-0.973160238\n",
      "[Iter.  930]  loss:43.413208  pct:-0.969601472\n",
      "[Iter.  940]  loss:42.993763  pct:-0.966169185\n",
      "[Iter.  950]  loss:42.579849  pct:-0.962729704\n",
      "[Iter.  960]  loss:42.171387  pct:-0.959285981\n",
      "[Iter.  970]  loss:41.768192  pct:-0.956085296\n",
      "[Iter.  980]  loss:41.370270  pct:-0.952692693\n",
      "[Iter.  990]  loss:40.977470  pct:-0.949472603\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.752218  pct:100.000000000\n",
      "[Iter.    2]  loss:2.752240  pct:0.000788313\n",
      "[Iter.    4]  loss:2.752233  pct:-0.000251219\n",
      "[Iter.    6]  loss:2.752229  pct:-0.000129941\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.752229\n",
      "Best loss: 2.752229 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 13%|█▎        | 1319/10000 [00:22<02:30, 57.85it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:464.658539  pct:100.000000000\n",
      "[Iter.   10]  loss:78.964012  pct:-83.006011258\n",
      "[Iter.   20]  loss:74.220222  pct:-6.007533741\n",
      "[Iter.   30]  loss:71.024765  pct:-4.305373053\n",
      "[Iter.   40]  loss:68.147339  pct:-4.051299778\n",
      "[Iter.   50]  loss:65.532501  pct:-3.837035591\n",
      "[Iter.   60]  loss:63.139378  pct:-3.651811822\n",
      "[Iter.   70]  loss:60.935886  pct:-3.489884277\n",
      "[Iter.   80]  loss:58.896114  pct:-3.347406848\n",
      "[Iter.   90]  loss:56.999035  pct:-3.221060487\n",
      "[Iter.  100]  loss:55.227367  pct:-3.108241191\n",
      "[Iter.  110]  loss:53.566738  pct:-3.006895586\n",
      "[Iter.  120]  loss:52.005024  pct:-2.915455051\n",
      "[Iter.  130]  loss:50.532005  pct:-2.832454510\n",
      "[Iter.  140]  loss:49.139091  pct:-2.756498203\n",
      "[Iter.  150]  loss:47.818729  pct:-2.686989220\n",
      "[Iter.  160]  loss:46.564495  pct:-2.622893435\n",
      "[Iter.  170]  loss:45.370651  pct:-2.563850074\n",
      "[Iter.  180]  loss:44.232315  pct:-2.508970337\n",
      "[Iter.  190]  loss:43.145111  pct:-2.457940485\n",
      "[Iter.  200]  loss:42.105183  pct:-2.410304227\n",
      "[Iter.  210]  loss:41.109039  pct:-2.365844959\n",
      "[Iter.  220]  loss:40.153648  pct:-2.324041005\n",
      "[Iter.  230]  loss:39.236210  pct:-2.284819796\n",
      "[Iter.  240]  loss:38.354240  pct:-2.247845689\n",
      "[Iter.  250]  loss:37.505493  pct:-2.212916340\n",
      "[Iter.  260]  loss:36.687881  pct:-2.179978519\n",
      "[Iter.  270]  loss:35.899540  pct:-2.148779081\n",
      "[Iter.  280]  loss:35.138779  pct:-2.119139304\n",
      "[Iter.  290]  loss:34.404041  pct:-2.090958832\n",
      "[Iter.  300]  loss:33.693878  pct:-2.064185165\n",
      "[Iter.  310]  loss:33.006977  pct:-2.038652508\n",
      "[Iter.  320]  loss:32.342152  pct:-2.014196689\n",
      "[Iter.  330]  loss:31.698275  pct:-1.990829295\n",
      "[Iter.  340]  loss:31.074282  pct:-1.968539069\n",
      "[Iter.  350]  loss:30.469248  pct:-1.947056671\n",
      "[Iter.  360]  loss:29.882240  pct:-1.926557315\n",
      "[Iter.  370]  loss:29.312481  pct:-1.906682241\n",
      "[Iter.  380]  loss:28.759098  pct:-1.887874571\n",
      "[Iter.  390]  loss:28.221426  pct:-1.869571994\n",
      "[Iter.  400]  loss:27.698780  pct:-1.851947347\n",
      "[Iter.  410]  loss:27.190496  pct:-1.835039717\n",
      "[Iter.  420]  loss:26.696028  pct:-1.818534980\n",
      "[Iter.  430]  loss:26.214756  pct:-1.802784100\n",
      "[Iter.  440]  loss:25.746202  pct:-1.787365646\n",
      "[Iter.  450]  loss:25.289843  pct:-1.772532721\n",
      "[Iter.  460]  loss:24.845251  pct:-1.757984536\n",
      "[Iter.  470]  loss:24.411901  pct:-1.744194929\n",
      "[Iter.  480]  loss:23.989435  pct:-1.730575058\n",
      "[Iter.  490]  loss:23.577440  pct:-1.717401559\n",
      "[Iter.  500]  loss:23.175535  pct:-1.704617021\n",
      "[Iter.  510]  loss:22.783396  pct:-1.692040470\n",
      "[Iter.  520]  loss:22.400640  pct:-1.679974677\n",
      "[Iter.  530]  loss:22.026981  pct:-1.668073438\n",
      "[Iter.  540]  loss:21.662090  pct:-1.656564040\n",
      "[Iter.  550]  loss:21.305681  pct:-1.645312469\n",
      "[Iter.  560]  loss:20.957523  pct:-1.634108194\n",
      "[Iter.  570]  loss:20.617290  pct:-1.623440153\n",
      "[Iter.  580]  loss:20.284760  pct:-1.612874278\n",
      "[Iter.  590]  loss:19.959688  pct:-1.602539752\n",
      "[Iter.  600]  loss:19.641851  pct:-1.592393421\n",
      "[Iter.  610]  loss:19.331049  pct:-1.582348084\n",
      "[Iter.  620]  loss:19.026997  pct:-1.572870430\n",
      "[Iter.  630]  loss:18.729574  pct:-1.563160046\n",
      "[Iter.  640]  loss:18.438549  pct:-1.553826897\n",
      "[Iter.  650]  loss:18.153749  pct:-1.544587783\n",
      "[Iter.  660]  loss:17.874971  pct:-1.535650124\n",
      "[Iter.  670]  loss:17.602089  pct:-1.526617613\n",
      "[Iter.  680]  loss:17.334917  pct:-1.517841779\n",
      "[Iter.  690]  loss:17.073292  pct:-1.509238774\n",
      "[Iter.  700]  loss:16.817053  pct:-1.500817421\n",
      "[Iter.  710]  loss:16.566088  pct:-1.492325206\n",
      "[Iter.  720]  loss:16.320234  pct:-1.484076556\n",
      "[Iter.  730]  loss:16.079376  pct:-1.475824878\n",
      "[Iter.  740]  loss:15.843379  pct:-1.467701214\n",
      "[Iter.  750]  loss:15.612123  pct:-1.459641183\n",
      "[Iter.  760]  loss:15.385466  pct:-1.451800761\n",
      "[Iter.  770]  loss:15.163324  pct:-1.443838434\n",
      "[Iter.  780]  loss:14.945547  pct:-1.436210471\n",
      "[Iter.  790]  loss:14.732039  pct:-1.428570335\n",
      "[Iter.  800]  loss:14.522706  pct:-1.420939853\n",
      "[Iter.  810]  loss:14.317435  pct:-1.413447100\n",
      "[Iter.  820]  loss:14.116145  pct:-1.405909137\n",
      "[Iter.  830]  loss:13.918729  pct:-1.398514280\n",
      "[Iter.  840]  loss:13.725086  pct:-1.391237797\n",
      "[Iter.  850]  loss:13.535138  pct:-1.383948188\n",
      "[Iter.  860]  loss:13.348805  pct:-1.376659040\n",
      "[Iter.  870]  loss:13.166003  pct:-1.369427409\n",
      "[Iter.  880]  loss:12.986634  pct:-1.362364642\n",
      "[Iter.  890]  loss:12.810635  pct:-1.355236761\n",
      "[Iter.  900]  loss:12.637924  pct:-1.348180039\n",
      "[Iter.  910]  loss:12.468430  pct:-1.341158772\n",
      "[Iter.  920]  loss:12.302079  pct:-1.334172550\n",
      "[Iter.  930]  loss:12.138819  pct:-1.327096479\n",
      "[Iter.  940]  loss:11.978580  pct:-1.320056120\n",
      "[Iter.  950]  loss:11.821258  pct:-1.313360484\n",
      "[Iter.  960]  loss:11.666818  pct:-1.306459359\n",
      "[Iter.  970]  loss:11.515210  pct:-1.299476153\n",
      "[Iter.  980]  loss:11.366344  pct:-1.292774494\n",
      "[Iter.  990]  loss:11.220175  pct:-1.285986564\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.748954  pct:100.000000000\n",
      "[Iter.    2]  loss:2.749004  pct:0.001838690\n",
      "[Iter.    4]  loss:2.748999  pct:-0.000190804\n",
      "[Iter.    6]  loss:2.748995  pct:-0.000138767\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.748995\n",
      "Best loss: 2.748995 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 22%|██▏       | 2177/10000 [00:39<02:22, 55.04it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:3979.082520  pct:100.000000000\n",
      "[Iter.   10]  loss:652.077759  pct:-83.612362058\n",
      "[Iter.   20]  loss:635.794800  pct:-2.497088540\n",
      "[Iter.   30]  loss:626.830566  pct:-1.409925561\n",
      "[Iter.   40]  loss:618.727905  pct:-1.292639761\n",
      "[Iter.   50]  loss:610.956238  pct:-1.256071920\n",
      "[Iter.   60]  loss:603.470825  pct:-1.225196198\n",
      "[Iter.   70]  loss:596.254822  pct:-1.195750170\n",
      "[Iter.   80]  loss:589.283081  pct:-1.169255236\n",
      "[Iter.   90]  loss:582.542908  pct:-1.143792102\n",
      "[Iter.  100]  loss:576.022278  pct:-1.119338987\n",
      "[Iter.  110]  loss:569.702087  pct:-1.097212846\n",
      "[Iter.  120]  loss:563.570801  pct:-1.076226813\n",
      "[Iter.  130]  loss:557.618713  pct:-1.056138358\n",
      "[Iter.  140]  loss:551.836914  pct:-1.036873257\n",
      "[Iter.  150]  loss:546.216492  pct:-1.018493366\n",
      "[Iter.  160]  loss:540.750122  pct:-1.000769789\n",
      "[Iter.  170]  loss:535.429688  pct:-0.983898912\n",
      "[Iter.  180]  loss:530.248474  pct:-0.967673907\n",
      "[Iter.  190]  loss:525.199951  pct:-0.952105135\n",
      "[Iter.  200]  loss:520.277954  pct:-0.937166323\n",
      "[Iter.  210]  loss:515.476929  pct:-0.922780862\n",
      "[Iter.  220]  loss:510.791443  pct:-0.908961309\n",
      "[Iter.  230]  loss:506.216583  pct:-0.895641398\n",
      "[Iter.  240]  loss:501.747650  pct:-0.882810491\n",
      "[Iter.  250]  loss:497.380127  pct:-0.870462112\n",
      "[Iter.  260]  loss:493.110016  pct:-0.858520647\n",
      "[Iter.  270]  loss:488.933105  pct:-0.847054464\n",
      "[Iter.  280]  loss:484.846008  pct:-0.835921545\n",
      "[Iter.  290]  loss:480.845123  pct:-0.825186748\n",
      "[Iter.  300]  loss:476.927002  pct:-0.814840610\n",
      "[Iter.  310]  loss:473.088257  pct:-0.804891546\n",
      "[Iter.  320]  loss:469.326660  pct:-0.795115208\n",
      "[Iter.  330]  loss:465.638947  pct:-0.785745609\n",
      "[Iter.  340]  loss:462.022461  pct:-0.776671630\n",
      "[Iter.  350]  loss:458.474731  pct:-0.767869485\n",
      "[Iter.  360]  loss:454.993408  pct:-0.759327178\n",
      "[Iter.  370]  loss:451.576050  pct:-0.751078661\n",
      "[Iter.  380]  loss:448.220642  pct:-0.743043772\n",
      "[Iter.  390]  loss:444.924866  pct:-0.735302228\n",
      "[Iter.  400]  loss:441.687073  pct:-0.727716794\n",
      "[Iter.  410]  loss:438.505066  pct:-0.720421093\n",
      "[Iter.  420]  loss:435.377258  pct:-0.713288822\n",
      "[Iter.  430]  loss:432.302216  pct:-0.706293833\n",
      "[Iter.  440]  loss:429.278015  pct:-0.699557007\n",
      "[Iter.  450]  loss:426.303131  pct:-0.692997062\n",
      "[Iter.  460]  loss:423.376129  pct:-0.686601092\n",
      "[Iter.  470]  loss:420.495422  pct:-0.680413134\n",
      "[Iter.  480]  loss:417.659790  pct:-0.674355100\n",
      "[Iter.  490]  loss:414.867950  pct:-0.668448260\n",
      "[Iter.  500]  loss:412.118652  pct:-0.662692332\n",
      "[Iter.  510]  loss:409.410614  pct:-0.657101617\n",
      "[Iter.  520]  loss:406.743011  pct:-0.651571417\n",
      "[Iter.  530]  loss:404.114288  pct:-0.646286001\n",
      "[Iter.  540]  loss:401.523529  pct:-0.641095688\n",
      "[Iter.  550]  loss:398.970001  pct:-0.635959700\n",
      "[Iter.  560]  loss:396.452484  pct:-0.631004106\n",
      "[Iter.  570]  loss:393.970184  pct:-0.626127948\n",
      "[Iter.  580]  loss:391.522247  pct:-0.621350830\n",
      "[Iter.  590]  loss:389.107605  pct:-0.616731833\n",
      "[Iter.  600]  loss:386.725647  pct:-0.612159202\n",
      "[Iter.  610]  loss:384.375427  pct:-0.607722747\n",
      "[Iter.  620]  loss:382.056183  pct:-0.603379982\n",
      "[Iter.  630]  loss:379.767120  pct:-0.599142902\n",
      "[Iter.  640]  loss:377.507599  pct:-0.594975542\n",
      "[Iter.  650]  loss:375.276886  pct:-0.590905427\n",
      "[Iter.  660]  loss:373.074219  pct:-0.586944552\n",
      "[Iter.  670]  loss:370.899109  pct:-0.583023365\n",
      "[Iter.  680]  loss:368.750854  pct:-0.579201821\n",
      "[Iter.  690]  loss:366.628906  pct:-0.575442258\n",
      "[Iter.  700]  loss:364.532745  pct:-0.571739122\n",
      "[Iter.  710]  loss:362.461517  pct:-0.568187098\n",
      "[Iter.  720]  loss:360.414856  pct:-0.564656185\n",
      "[Iter.  730]  loss:358.392426  pct:-0.561139583\n",
      "[Iter.  740]  loss:356.393463  pct:-0.557757994\n",
      "[Iter.  750]  loss:354.417511  pct:-0.554429964\n",
      "[Iter.  760]  loss:352.464050  pct:-0.551174994\n",
      "[Iter.  770]  loss:350.532745  pct:-0.547943806\n",
      "[Iter.  780]  loss:348.623199  pct:-0.544755354\n",
      "[Iter.  790]  loss:346.734772  pct:-0.541681603\n",
      "[Iter.  800]  loss:344.867188  pct:-0.538620404\n",
      "[Iter.  810]  loss:343.019867  pct:-0.535661444\n",
      "[Iter.  820]  loss:341.192444  pct:-0.532745556\n",
      "[Iter.  830]  loss:339.384796  pct:-0.529802971\n",
      "[Iter.  840]  loss:337.596313  pct:-0.526977839\n",
      "[Iter.  850]  loss:335.826599  pct:-0.524210213\n",
      "[Iter.  860]  loss:334.075409  pct:-0.521456665\n",
      "[Iter.  870]  loss:332.342224  pct:-0.518800477\n",
      "[Iter.  880]  loss:330.626831  pct:-0.516152611\n",
      "[Iter.  890]  loss:328.928864  pct:-0.513560114\n",
      "[Iter.  900]  loss:327.248169  pct:-0.510959896\n",
      "[Iter.  910]  loss:325.584229  pct:-0.508464397\n",
      "[Iter.  920]  loss:323.936890  pct:-0.505963964\n",
      "[Iter.  930]  loss:322.305695  pct:-0.503553353\n",
      "[Iter.  940]  loss:320.690582  pct:-0.501111936\n",
      "[Iter.  950]  loss:319.091187  pct:-0.498734868\n",
      "[Iter.  960]  loss:317.507050  pct:-0.496452748\n",
      "[Iter.  970]  loss:315.938141  pct:-0.494133498\n",
      "[Iter.  980]  loss:314.384155  pct:-0.491863879\n",
      "[Iter.  990]  loss:312.844635  pct:-0.489693974\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.782462  pct:100.000000000\n",
      "[Iter.    2]  loss:2.782054  pct:-0.014669477\n",
      "[Iter.    4]  loss:2.781990  pct:-0.002288157\n",
      "[Iter.    6]  loss:2.781969  pct:-0.000754166\n",
      "[Iter.    8]  loss:2.781955  pct:-0.000505638\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.781955\n",
      "Best loss: 2.781955 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 26%|██▌       | 2569/10000 [00:54<02:38, 46.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:463.613464  pct:100.000000000\n",
      "[Iter.   10]  loss:76.616013  pct:-83.474158168\n",
      "[Iter.   20]  loss:72.827278  pct:-4.945094777\n",
      "[Iter.   30]  loss:69.720558  pct:-4.265874066\n",
      "[Iter.   40]  loss:66.926056  pct:-4.008146710\n",
      "[Iter.   50]  loss:64.385239  pct:-3.796454499\n",
      "[Iter.   60]  loss:62.057697  pct:-3.615023257\n",
      "[Iter.   70]  loss:59.912254  pct:-3.457174623\n",
      "[Iter.   80]  loss:57.924164  pct:-3.318337020\n",
      "[Iter.   90]  loss:56.073475  pct:-3.195020545\n",
      "[Iter.  100]  loss:54.343697  pct:-3.084842331\n",
      "[Iter.  110]  loss:52.721138  pct:-2.985734677\n",
      "[Iter.  120]  loss:51.194378  pct:-2.895916437\n",
      "[Iter.  130]  loss:49.753616  pct:-2.814296462\n",
      "[Iter.  140]  loss:48.390545  pct:-2.739642949\n",
      "[Iter.  150]  loss:47.098015  pct:-2.671038449\n",
      "[Iter.  160]  loss:45.869732  pct:-2.607929300\n",
      "[Iter.  170]  loss:44.700291  pct:-2.549483449\n",
      "[Iter.  180]  loss:43.584869  pct:-2.495333427\n",
      "[Iter.  190]  loss:42.519245  pct:-2.444940761\n",
      "[Iter.  200]  loss:41.499702  pct:-2.397838180\n",
      "[Iter.  210]  loss:40.522820  pct:-2.353951659\n",
      "[Iter.  220]  loss:39.585709  pct:-2.312551081\n",
      "[Iter.  230]  loss:38.685627  pct:-2.273754004\n",
      "[Iter.  240]  loss:37.820152  pct:-2.237199623\n",
      "[Iter.  250]  loss:36.987122  pct:-2.202610646\n",
      "[Iter.  260]  loss:36.184574  pct:-2.169802408\n",
      "[Iter.  270]  loss:35.410595  pct:-2.138975532\n",
      "[Iter.  280]  loss:34.663609  pct:-2.109499686\n",
      "[Iter.  290]  loss:33.942020  pct:-2.081687871\n",
      "[Iter.  300]  loss:33.244514  pct:-2.054992432\n",
      "[Iter.  310]  loss:32.569790  pct:-2.029581691\n",
      "[Iter.  320]  loss:31.916641  pct:-2.005381838\n",
      "[Iter.  330]  loss:31.284004  pct:-1.982154135\n",
      "[Iter.  340]  loss:30.670828  pct:-1.960031528\n",
      "[Iter.  350]  loss:30.076220  pct:-1.938677070\n",
      "[Iter.  360]  loss:29.499283  pct:-1.918248803\n",
      "[Iter.  370]  loss:28.939133  pct:-1.898860218\n",
      "[Iter.  380]  loss:28.395109  pct:-1.879888798\n",
      "[Iter.  390]  loss:27.866470  pct:-1.861724977\n",
      "[Iter.  400]  loss:27.352537  pct:-1.844270823\n",
      "[Iter.  410]  loss:26.852690  pct:-1.827426133\n",
      "[Iter.  420]  loss:26.366364  pct:-1.811089400\n",
      "[Iter.  430]  loss:25.893026  pct:-1.795231159\n",
      "[Iter.  440]  loss:25.432135  pct:-1.779983990\n",
      "[Iter.  450]  loss:24.983221  pct:-1.765143118\n",
      "[Iter.  460]  loss:24.545750  pct:-1.751060797\n",
      "[Iter.  470]  loss:24.119381  pct:-1.737036836\n",
      "[Iter.  480]  loss:23.703629  pct:-1.723727536\n",
      "[Iter.  490]  loss:23.298195  pct:-1.710428655\n",
      "[Iter.  500]  loss:22.902670  pct:-1.697663620\n",
      "[Iter.  510]  loss:22.516737  pct:-1.685100139\n",
      "[Iter.  520]  loss:22.140015  pct:-1.673076948\n",
      "[Iter.  530]  loss:21.772213  pct:-1.661253039\n",
      "[Iter.  540]  loss:21.413057  pct:-1.649605647\n",
      "[Iter.  550]  loss:21.062191  pct:-1.638562455\n",
      "[Iter.  560]  loss:20.719406  pct:-1.627489189\n",
      "[Iter.  570]  loss:20.384434  pct:-1.616708411\n",
      "[Iter.  580]  loss:20.057032  pct:-1.606137894\n",
      "[Iter.  590]  loss:19.736959  pct:-1.595810287\n",
      "[Iter.  600]  loss:19.423962  pct:-1.585846182\n",
      "[Iter.  610]  loss:19.117840  pct:-1.576000982\n",
      "[Iter.  620]  loss:18.818441  pct:-1.566068265\n",
      "[Iter.  630]  loss:18.525505  pct:-1.556644990\n",
      "[Iter.  640]  loss:18.238905  pct:-1.547056946\n",
      "[Iter.  650]  loss:17.958395  pct:-1.537975824\n",
      "[Iter.  660]  loss:17.683828  pct:-1.528904172\n",
      "[Iter.  670]  loss:17.415056  pct:-1.519875221\n",
      "[Iter.  680]  loss:17.151890  pct:-1.511143140\n",
      "[Iter.  690]  loss:16.894173  pct:-1.502558234\n",
      "[Iter.  700]  loss:16.641809  pct:-1.493788479\n",
      "[Iter.  710]  loss:16.394592  pct:-1.485518620\n",
      "[Iter.  720]  loss:16.152386  pct:-1.477356492\n",
      "[Iter.  730]  loss:15.915093  pct:-1.469085087\n",
      "[Iter.  740]  loss:15.682566  pct:-1.461051636\n",
      "[Iter.  750]  loss:15.454720  pct:-1.452862689\n",
      "[Iter.  760]  loss:15.231423  pct:-1.444841266\n",
      "[Iter.  770]  loss:15.012515  pct:-1.437215055\n",
      "[Iter.  780]  loss:14.797928  pct:-1.429388818\n",
      "[Iter.  790]  loss:14.587551  pct:-1.421663503\n",
      "[Iter.  800]  loss:14.381271  pct:-1.414080766\n",
      "[Iter.  810]  loss:14.178997  pct:-1.406512105\n",
      "[Iter.  820]  loss:13.980636  pct:-1.398980451\n",
      "[Iter.  830]  loss:13.786078  pct:-1.391619057\n",
      "[Iter.  840]  loss:13.595284  pct:-1.383961304\n",
      "[Iter.  850]  loss:13.408110  pct:-1.376762638\n",
      "[Iter.  860]  loss:13.224511  pct:-1.369309492\n",
      "[Iter.  870]  loss:13.044345  pct:-1.362366007\n",
      "[Iter.  880]  loss:12.867591  pct:-1.355023952\n",
      "[Iter.  890]  loss:12.694133  pct:-1.348023112\n",
      "[Iter.  900]  loss:12.523912  pct:-1.340937405\n",
      "[Iter.  910]  loss:12.356878  pct:-1.333721791\n",
      "[Iter.  920]  loss:12.192936  pct:-1.326729400\n",
      "[Iter.  930]  loss:12.032039  pct:-1.319594031\n",
      "[Iter.  940]  loss:11.874084  pct:-1.312788080\n",
      "[Iter.  950]  loss:11.719055  pct:-1.305602600\n",
      "[Iter.  960]  loss:11.566808  pct:-1.299144228\n",
      "[Iter.  970]  loss:11.417337  pct:-1.292234924\n",
      "[Iter.  980]  loss:11.270610  pct:-1.285129418\n",
      "[Iter.  990]  loss:11.126548  pct:-1.278209823\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.776538  pct:100.000000000\n",
      "[Iter.    2]  loss:2.776611  pct:0.002601831\n",
      "[Iter.    4]  loss:2.776611  pct:0.000000000\n",
      "[Iter.    6]  loss:2.776611  pct:0.000000000\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.776611\n",
      "Best loss: 2.776611 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 13%|█▎        | 1346/10000 [00:22<02:27, 58.60it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:5526.054199  pct:100.000000000\n",
      "[Iter.   10]  loss:779.048889  pct:-85.902258645\n",
      "[Iter.   20]  loss:770.514954  pct:-1.095430039\n",
      "[Iter.   30]  loss:763.498535  pct:-0.910614184\n",
      "[Iter.   40]  loss:757.068726  pct:-0.842150872\n",
      "[Iter.   50]  loss:750.876099  pct:-0.817974213\n",
      "[Iter.   60]  loss:744.843689  pct:-0.803382832\n",
      "[Iter.   70]  loss:738.954224  pct:-0.790698158\n",
      "[Iter.   80]  loss:733.198059  pct:-0.778960911\n",
      "[Iter.   90]  loss:727.566772  pct:-0.768044398\n",
      "[Iter.  100]  loss:722.053650  pct:-0.757747985\n",
      "[Iter.  110]  loss:716.653137  pct:-0.747937871\n",
      "[Iter.  120]  loss:711.360596  pct:-0.738508105\n",
      "[Iter.  130]  loss:706.173157  pct:-0.729227764\n",
      "[Iter.  140]  loss:701.086853  pct:-0.720262964\n",
      "[Iter.  150]  loss:696.097656  pct:-0.711637475\n",
      "[Iter.  160]  loss:691.202454  pct:-0.703235041\n",
      "[Iter.  170]  loss:686.397156  pct:-0.695208448\n",
      "[Iter.  180]  loss:681.679321  pct:-0.687333045\n",
      "[Iter.  190]  loss:677.046326  pct:-0.679644440\n",
      "[Iter.  200]  loss:672.495850  pct:-0.672107048\n",
      "[Iter.  210]  loss:668.025269  pct:-0.664774520\n",
      "[Iter.  220]  loss:663.631653  pct:-0.657702026\n",
      "[Iter.  230]  loss:659.312622  pct:-0.650817474\n",
      "[Iter.  240]  loss:655.065857  pct:-0.644120102\n",
      "[Iter.  250]  loss:650.888550  pct:-0.637692697\n",
      "[Iter.  260]  loss:646.779175  pct:-0.631348485\n",
      "[Iter.  270]  loss:642.735352  pct:-0.625224713\n",
      "[Iter.  280]  loss:638.755188  pct:-0.619253876\n",
      "[Iter.  290]  loss:634.837097  pct:-0.613394755\n",
      "[Iter.  300]  loss:630.979126  pct:-0.607710420\n",
      "[Iter.  310]  loss:627.179688  pct:-0.602149631\n",
      "[Iter.  320]  loss:623.436951  pct:-0.596756702\n",
      "[Iter.  330]  loss:619.749695  pct:-0.591440057\n",
      "[Iter.  340]  loss:616.116455  pct:-0.586243088\n",
      "[Iter.  350]  loss:612.535645  pct:-0.581190539\n",
      "[Iter.  360]  loss:609.005676  pct:-0.576287812\n",
      "[Iter.  370]  loss:605.525696  pct:-0.571420038\n",
      "[Iter.  380]  loss:602.094055  pct:-0.566720892\n",
      "[Iter.  390]  loss:598.709473  pct:-0.562135183\n",
      "[Iter.  400]  loss:595.371094  pct:-0.557595805\n",
      "[Iter.  410]  loss:592.077515  pct:-0.553197684\n",
      "[Iter.  420]  loss:588.827515  pct:-0.548914613\n",
      "[Iter.  430]  loss:585.620361  pct:-0.544667707\n",
      "[Iter.  440]  loss:582.454956  pct:-0.540521724\n",
      "[Iter.  450]  loss:579.330322  pct:-0.536459302\n",
      "[Iter.  460]  loss:576.245239  pct:-0.532525727\n",
      "[Iter.  470]  loss:573.199097  pct:-0.528619131\n",
      "[Iter.  480]  loss:570.190857  pct:-0.524815856\n",
      "[Iter.  490]  loss:567.219666  pct:-0.521087171\n",
      "[Iter.  500]  loss:564.284607  pct:-0.517446551\n",
      "[Iter.  510]  loss:561.384705  pct:-0.513907753\n",
      "[Iter.  520]  loss:558.519470  pct:-0.510386968\n",
      "[Iter.  530]  loss:555.687988  pct:-0.506962082\n",
      "[Iter.  540]  loss:552.889893  pct:-0.503537194\n",
      "[Iter.  550]  loss:550.123962  pct:-0.500267813\n",
      "[Iter.  560]  loss:547.389832  pct:-0.497002684\n",
      "[Iter.  570]  loss:544.686768  pct:-0.493809678\n",
      "[Iter.  580]  loss:542.014099  pct:-0.490679895\n",
      "[Iter.  590]  loss:539.371094  pct:-0.487626683\n",
      "[Iter.  600]  loss:536.757385  pct:-0.484584459\n",
      "[Iter.  610]  loss:534.171997  pct:-0.481667929\n",
      "[Iter.  620]  loss:531.614685  pct:-0.478743181\n",
      "[Iter.  630]  loss:529.084717  pct:-0.475902629\n",
      "[Iter.  640]  loss:526.581543  pct:-0.473113993\n",
      "[Iter.  650]  loss:524.104553  pct:-0.470390537\n",
      "[Iter.  660]  loss:521.653931  pct:-0.467582764\n",
      "[Iter.  670]  loss:519.228394  pct:-0.464970542\n",
      "[Iter.  680]  loss:516.827698  pct:-0.462358344\n",
      "[Iter.  690]  loss:514.451599  pct:-0.459746767\n",
      "[Iter.  700]  loss:512.099243  pct:-0.457255058\n",
      "[Iter.  710]  loss:509.770569  pct:-0.454731060\n",
      "[Iter.  720]  loss:507.464691  pct:-0.452336370\n",
      "[Iter.  730]  loss:505.181702  pct:-0.449881448\n",
      "[Iter.  740]  loss:502.920654  pct:-0.447571113\n",
      "[Iter.  750]  loss:500.681641  pct:-0.445202171\n",
      "[Iter.  760]  loss:498.464203  pct:-0.442883774\n",
      "[Iter.  770]  loss:496.267456  pct:-0.440703026\n",
      "[Iter.  780]  loss:494.091431  pct:-0.438478358\n",
      "[Iter.  790]  loss:491.935608  pct:-0.436320612\n",
      "[Iter.  800]  loss:489.799927  pct:-0.434138354\n",
      "[Iter.  810]  loss:487.683990  pct:-0.432000122\n",
      "[Iter.  820]  loss:485.587158  pct:-0.429957168\n",
      "[Iter.  830]  loss:483.509644  pct:-0.427835583\n",
      "[Iter.  840]  loss:481.450653  pct:-0.425842691\n",
      "[Iter.  850]  loss:479.410065  pct:-0.423841647\n",
      "[Iter.  860]  loss:477.387756  pct:-0.421832685\n",
      "[Iter.  870]  loss:475.383392  pct:-0.419860792\n",
      "[Iter.  880]  loss:473.396332  pct:-0.417991158\n",
      "[Iter.  890]  loss:471.426239  pct:-0.416161394\n",
      "[Iter.  900]  loss:469.473267  pct:-0.414268925\n",
      "[Iter.  910]  loss:467.537262  pct:-0.412378036\n",
      "[Iter.  920]  loss:465.617371  pct:-0.410639218\n",
      "[Iter.  930]  loss:463.713715  pct:-0.408845573\n",
      "[Iter.  940]  loss:461.826080  pct:-0.407068891\n",
      "[Iter.  950]  loss:459.953918  pct:-0.405382447\n",
      "[Iter.  960]  loss:458.097198  pct:-0.403675215\n",
      "[Iter.  970]  loss:456.255981  pct:-0.401927156\n",
      "[Iter.  980]  loss:454.429657  pct:-0.400285045\n",
      "[Iter.  990]  loss:452.618011  pct:-0.398663573\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.877881  pct:100.000000000\n",
      "[Iter.    2]  loss:2.876909  pct:-0.033767702\n",
      "[Iter.    4]  loss:2.876357  pct:-0.019193428\n",
      "[Iter.    6]  loss:2.876080  pct:-0.009606845\n",
      "[Iter.    8]  loss:2.875873  pct:-0.007195464\n",
      "[Iter.   10]  loss:2.875734  pct:-0.004858117\n",
      "[Iter.   12]  loss:2.875623  pct:-0.003863468\n",
      "[Iter.   14]  loss:2.875543  pct:-0.002777493\n",
      "[Iter.   16]  loss:2.875475  pct:-0.002346425\n",
      "[Iter.   18]  loss:2.875402  pct:-0.002545475\n",
      "[Iter.   20]  loss:2.875318  pct:-0.002935248\n",
      "[Iter.   22]  loss:2.875242  pct:-0.002628534\n",
      "[Iter.   24]  loss:2.875177  pct:-0.002272041\n",
      "[Iter.   26]  loss:2.875108  pct:-0.002396478\n",
      "[Iter.   28]  loss:2.875034  pct:-0.002562385\n",
      "[Iter.   30]  loss:2.874971  pct:-0.002197571\n",
      "[Iter.   32]  loss:2.874906  pct:-0.002247377\n",
      "[Iter.   34]  loss:2.874831  pct:-0.002612323\n",
      "[Iter.   36]  loss:2.874773  pct:-0.002031860\n",
      "[Iter.   38]  loss:2.874707  pct:-0.002297293\n",
      "[Iter.   40]  loss:2.874635  pct:-0.002504687\n",
      "[Iter.   42]  loss:2.874578  pct:-0.001990530\n",
      "[Iter.   44]  loss:2.874508  pct:-0.002421859\n",
      "[Iter.   46]  loss:2.874440  pct:-0.002347270\n",
      "[Iter.   48]  loss:2.874382  pct:-0.002015548\n",
      "New best!\n",
      "Trial 0 loss: 2.874382\n",
      "Best loss: 2.874382 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 14%|█▍        | 1397/10000 [00:23<02:26, 58.86it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:331.364441  pct:100.000000000\n",
      "[Iter.   10]  loss:64.306595  pct:-80.593397586\n",
      "[Iter.   20]  loss:60.110935  pct:-6.524462456\n",
      "[Iter.   30]  loss:56.812351  pct:-5.487494035\n",
      "[Iter.   40]  loss:53.940178  pct:-5.055543816\n",
      "[Iter.   50]  loss:51.397434  pct:-4.714006852\n",
      "[Iter.   60]  loss:49.118523  pct:-4.433901467\n",
      "[Iter.   70]  loss:47.056309  pct:-4.198444470\n",
      "[Iter.   80]  loss:45.175453  pct:-3.997031664\n",
      "[Iter.   90]  loss:43.448654  pct:-3.822427645\n",
      "[Iter.  100]  loss:41.854370  pct:-3.669351992\n",
      "[Iter.  110]  loss:40.375240  pct:-3.533991282\n",
      "[Iter.  120]  loss:38.997112  pct:-3.413299935\n",
      "[Iter.  130]  loss:37.708271  pct:-3.304965861\n",
      "[Iter.  140]  loss:36.498962  pct:-3.207011595\n",
      "[Iter.  150]  loss:35.360943  pct:-3.117950448\n",
      "[Iter.  160]  loss:34.287136  pct:-3.036702860\n",
      "[Iter.  170]  loss:33.271492  pct:-2.962172376\n",
      "[Iter.  180]  loss:32.308762  pct:-2.893559470\n",
      "[Iter.  190]  loss:31.394361  pct:-2.830192355\n",
      "[Iter.  200]  loss:30.524313  pct:-2.771352821\n",
      "[Iter.  210]  loss:29.695139  pct:-2.716438016\n",
      "[Iter.  220]  loss:28.903690  pct:-2.665246305\n",
      "[Iter.  230]  loss:28.147226  pct:-2.617188309\n",
      "[Iter.  240]  loss:27.423201  pct:-2.572280898\n",
      "[Iter.  250]  loss:26.729412  pct:-2.529932732\n",
      "[Iter.  260]  loss:26.063860  pct:-2.489961759\n",
      "[Iter.  270]  loss:25.424744  pct:-2.452116796\n",
      "[Iter.  280]  loss:24.810385  pct:-2.416381893\n",
      "[Iter.  290]  loss:24.219313  pct:-2.382357583\n",
      "[Iter.  300]  loss:23.650145  pct:-2.350058809\n",
      "[Iter.  310]  loss:23.101650  pct:-2.319200786\n",
      "[Iter.  320]  loss:22.572689  pct:-2.289711671\n",
      "[Iter.  330]  loss:22.062220  pct:-2.261447165\n",
      "[Iter.  340]  loss:21.569223  pct:-2.234572152\n",
      "[Iter.  350]  loss:21.092821  pct:-2.208713192\n",
      "[Iter.  360]  loss:20.632210  pct:-2.183735124\n",
      "[Iter.  370]  loss:20.186588  pct:-2.159834042\n",
      "[Iter.  380]  loss:19.755241  pct:-2.136799380\n",
      "[Iter.  390]  loss:19.337543  pct:-2.114364984\n",
      "[Iter.  400]  loss:18.932848  pct:-2.092796901\n",
      "[Iter.  410]  loss:18.540577  pct:-2.071907208\n",
      "[Iter.  420]  loss:18.160162  pct:-2.051796792\n",
      "[Iter.  430]  loss:17.791113  pct:-2.032190422\n",
      "[Iter.  440]  loss:17.432964  pct:-2.013075724\n",
      "[Iter.  450]  loss:17.085236  pct:-1.994662083\n",
      "[Iter.  460]  loss:16.747515  pct:-1.976682552\n",
      "[Iter.  470]  loss:16.419394  pct:-1.959223149\n",
      "[Iter.  480]  loss:16.100561  pct:-1.941803738\n",
      "[Iter.  490]  loss:15.790599  pct:-1.925164408\n",
      "[Iter.  500]  loss:15.489147  pct:-1.909057950\n",
      "[Iter.  510]  loss:15.195932  pct:-1.893033841\n",
      "[Iter.  520]  loss:14.910650  pct:-1.877358544\n",
      "[Iter.  530]  loss:14.633007  pct:-1.862046249\n",
      "[Iter.  540]  loss:14.362728  pct:-1.847049822\n",
      "[Iter.  550]  loss:14.099575  pct:-1.832194232\n",
      "[Iter.  560]  loss:13.843295  pct:-1.817643047\n",
      "[Iter.  570]  loss:13.593670  pct:-1.803221012\n",
      "[Iter.  580]  loss:13.350449  pct:-1.789224580\n",
      "[Iter.  590]  loss:13.113440  pct:-1.775289021\n",
      "[Iter.  600]  loss:12.882435  pct:-1.761587522\n",
      "[Iter.  610]  loss:12.657232  pct:-1.748136615\n",
      "[Iter.  620]  loss:12.437661  pct:-1.734748234\n",
      "[Iter.  630]  loss:12.223507  pct:-1.721820851\n",
      "[Iter.  640]  loss:12.014647  pct:-1.708678193\n",
      "[Iter.  650]  loss:11.810895  pct:-1.695859828\n",
      "[Iter.  660]  loss:11.612123  pct:-1.682958243\n",
      "[Iter.  670]  loss:11.418147  pct:-1.670456439\n",
      "[Iter.  680]  loss:11.228857  pct:-1.657800038\n",
      "[Iter.  690]  loss:11.044083  pct:-1.645531670\n",
      "[Iter.  700]  loss:10.863744  pct:-1.632900309\n",
      "[Iter.  710]  loss:10.687659  pct:-1.620845649\n",
      "[Iter.  720]  loss:10.515709  pct:-1.608868098\n",
      "[Iter.  730]  loss:10.347801  pct:-1.596732242\n",
      "[Iter.  740]  loss:10.183819  pct:-1.584707592\n",
      "[Iter.  750]  loss:10.023644  pct:-1.572832085\n",
      "[Iter.  760]  loss:9.867157  pct:-1.561183317\n",
      "[Iter.  770]  loss:9.714296  pct:-1.549186273\n",
      "[Iter.  780]  loss:9.564920  pct:-1.537691566\n",
      "[Iter.  790]  loss:9.418970  pct:-1.525891601\n",
      "[Iter.  800]  loss:9.276369  pct:-1.513976704\n",
      "[Iter.  810]  loss:9.137003  pct:-1.502378231\n",
      "[Iter.  820]  loss:9.000809  pct:-1.490578803\n",
      "[Iter.  830]  loss:8.867689  pct:-1.478973583\n",
      "[Iter.  840]  loss:8.737567  pct:-1.467374226\n",
      "[Iter.  850]  loss:8.610343  pct:-1.456057153\n",
      "[Iter.  860]  loss:8.485961  pct:-1.444565209\n",
      "[Iter.  870]  loss:8.364364  pct:-1.432923043\n",
      "[Iter.  880]  loss:8.245471  pct:-1.421419182\n",
      "[Iter.  890]  loss:8.129212  pct:-1.409969439\n",
      "[Iter.  900]  loss:8.015513  pct:-1.398646684\n",
      "[Iter.  910]  loss:7.904330  pct:-1.387099749\n",
      "[Iter.  920]  loss:7.795603  pct:-1.375542920\n",
      "[Iter.  930]  loss:7.689248  pct:-1.364297195\n",
      "[Iter.  940]  loss:7.585224  pct:-1.352849313\n",
      "[Iter.  950]  loss:7.483460  pct:-1.341604805\n",
      "[Iter.  960]  loss:7.383913  pct:-1.330225725\n",
      "[Iter.  970]  loss:7.286551  pct:-1.318576177\n",
      "[Iter.  980]  loss:7.191282  pct:-1.307453358\n",
      "[Iter.  990]  loss:7.098075  pct:-1.296109358\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.732374  pct:100.000000000\n",
      "[Iter.    2]  loss:2.732412  pct:0.001387385\n",
      "[Iter.    4]  loss:2.732388  pct:-0.000846381\n",
      "[Iter.    6]  loss:2.732369  pct:-0.000706777\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.732369\n",
      "Best loss: 2.732369 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 15%|█▍        | 1464/10000 [00:24<02:25, 58.65it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:280.650757  pct:100.000000000\n",
      "[Iter.   10]  loss:59.593666  pct:-78.765901393\n",
      "[Iter.   20]  loss:54.730904  pct:-8.159864582\n",
      "[Iter.   30]  loss:51.346645  pct:-6.183450384\n",
      "[Iter.   40]  loss:48.446857  pct:-5.647472942\n",
      "[Iter.   50]  loss:45.915169  pct:-5.225702601\n",
      "[Iter.   60]  loss:43.673580  pct:-4.882021896\n",
      "[Iter.   70]  loss:41.666412  pct:-4.595839884\n",
      "[Iter.   80]  loss:39.852280  pct:-4.353945031\n",
      "[Iter.   90]  loss:38.199657  pct:-4.146869983\n",
      "[Iter.  100]  loss:36.684071  pct:-3.967540430\n",
      "[Iter.  110]  loss:35.286247  pct:-3.810436823\n",
      "[Iter.  120]  loss:33.990707  pct:-3.671514986\n",
      "[Iter.  130]  loss:32.784801  pct:-3.547751743\n",
      "[Iter.  140]  loss:31.658175  pct:-3.436430655\n",
      "[Iter.  150]  loss:30.602055  pct:-3.336010162\n",
      "[Iter.  160]  loss:29.609154  pct:-3.244556163\n",
      "[Iter.  170]  loss:28.673134  pct:-3.161251772\n",
      "[Iter.  180]  loss:27.788671  pct:-3.084637909\n",
      "[Iter.  190]  loss:26.951103  pct:-3.014063782\n",
      "[Iter.  200]  loss:26.156404  pct:-2.948668591\n",
      "[Iter.  210]  loss:25.401031  pct:-2.887908394\n",
      "[Iter.  220]  loss:24.681847  pct:-2.831321538\n",
      "[Iter.  230]  loss:23.996090  pct:-2.778384835\n",
      "[Iter.  240]  loss:23.341314  pct:-2.728676302\n",
      "[Iter.  250]  loss:22.715326  pct:-2.681888424\n",
      "[Iter.  260]  loss:22.116117  pct:-2.637905455\n",
      "[Iter.  270]  loss:21.541908  pct:-2.596338231\n",
      "[Iter.  280]  loss:20.991095  pct:-2.556940027\n",
      "[Iter.  290]  loss:20.462263  pct:-2.519313510\n",
      "[Iter.  300]  loss:19.954063  pct:-2.483594748\n",
      "[Iter.  310]  loss:19.465260  pct:-2.449645736\n",
      "[Iter.  320]  loss:18.994785  pct:-2.416994451\n",
      "[Iter.  330]  loss:18.541590  pct:-2.385894679\n",
      "[Iter.  340]  loss:18.104757  pct:-2.355959948\n",
      "[Iter.  350]  loss:17.683403  pct:-2.327312577\n",
      "[Iter.  360]  loss:17.276751  pct:-2.299627793\n",
      "[Iter.  370]  loss:16.884045  pct:-2.273031123\n",
      "[Iter.  380]  loss:16.504614  pct:-2.247274150\n",
      "[Iter.  390]  loss:16.137819  pct:-2.222376052\n",
      "[Iter.  400]  loss:15.783086  pct:-2.198149953\n",
      "[Iter.  410]  loss:15.439848  pct:-2.174719701\n",
      "[Iter.  420]  loss:15.107583  pct:-2.151995935\n",
      "[Iter.  430]  loss:14.785801  pct:-2.129937735\n",
      "[Iter.  440]  loss:14.474054  pct:-2.108418737\n",
      "[Iter.  450]  loss:14.171916  pct:-2.087447798\n",
      "[Iter.  460]  loss:13.879012  pct:-2.066791110\n",
      "[Iter.  470]  loss:13.594919  pct:-2.046924528\n",
      "[Iter.  480]  loss:13.319332  pct:-2.027132915\n",
      "[Iter.  490]  loss:13.051871  pct:-2.008064823\n",
      "[Iter.  500]  loss:12.792249  pct:-1.989159776\n",
      "[Iter.  510]  loss:12.540168  pct:-1.970575485\n",
      "[Iter.  520]  loss:12.295323  pct:-1.952481341\n",
      "[Iter.  530]  loss:12.057467  pct:-1.934531185\n",
      "[Iter.  540]  loss:11.826317  pct:-1.917066685\n",
      "[Iter.  550]  loss:11.601681  pct:-1.899459325\n",
      "[Iter.  560]  loss:11.383289  pct:-1.882411894\n",
      "[Iter.  570]  loss:11.170921  pct:-1.865611997\n",
      "[Iter.  580]  loss:10.964407  pct:-1.848677942\n",
      "[Iter.  590]  loss:10.763493  pct:-1.832423619\n",
      "[Iter.  600]  loss:10.567990  pct:-1.816346132\n",
      "[Iter.  610]  loss:10.377663  pct:-1.800982390\n",
      "[Iter.  620]  loss:10.192395  pct:-1.785252176\n",
      "[Iter.  630]  loss:10.012162  pct:-1.768308606\n",
      "[Iter.  640]  loss:9.836718  pct:-1.752314828\n",
      "[Iter.  650]  loss:9.665875  pct:-1.736780271\n",
      "[Iter.  660]  loss:9.499535  pct:-1.720908045\n",
      "[Iter.  670]  loss:9.337514  pct:-1.705564430\n",
      "[Iter.  680]  loss:9.179679  pct:-1.690332223\n",
      "[Iter.  690]  loss:9.025902  pct:-1.675190645\n",
      "[Iter.  700]  loss:8.876070  pct:-1.660019965\n",
      "[Iter.  710]  loss:8.730054  pct:-1.645053729\n",
      "[Iter.  720]  loss:8.587760  pct:-1.629931861\n",
      "[Iter.  730]  loss:8.449071  pct:-1.614961778\n",
      "[Iter.  740]  loss:8.313889  pct:-1.599967402\n",
      "[Iter.  750]  loss:8.182084  pct:-1.585352816\n",
      "[Iter.  760]  loss:8.053586  pct:-1.570481018\n",
      "[Iter.  770]  loss:7.928276  pct:-1.555952144\n",
      "[Iter.  780]  loss:7.806096  pct:-1.541066230\n",
      "[Iter.  790]  loss:7.686934  pct:-1.526532063\n",
      "[Iter.  800]  loss:7.570727  pct:-1.511742566\n",
      "[Iter.  810]  loss:7.457369  pct:-1.497320174\n",
      "[Iter.  820]  loss:7.346803  pct:-1.482636364\n",
      "[Iter.  830]  loss:7.238948  pct:-1.468051347\n",
      "[Iter.  840]  loss:7.133725  pct:-1.453569964\n",
      "[Iter.  850]  loss:7.031055  pct:-1.439222711\n",
      "[Iter.  860]  loss:6.930881  pct:-1.424735699\n",
      "[Iter.  870]  loss:6.833118  pct:-1.410543064\n",
      "[Iter.  880]  loss:6.737752  pct:-1.395637026\n",
      "[Iter.  890]  loss:6.644662  pct:-1.381618844\n",
      "[Iter.  900]  loss:6.553828  pct:-1.367024170\n",
      "[Iter.  910]  loss:6.465173  pct:-1.352728033\n",
      "[Iter.  920]  loss:6.378653  pct:-1.338250316\n",
      "[Iter.  930]  loss:6.294189  pct:-1.324160477\n",
      "[Iter.  940]  loss:6.211745  pct:-1.309838558\n",
      "[Iter.  950]  loss:6.131284  pct:-1.295312327\n",
      "[Iter.  960]  loss:6.052725  pct:-1.281280151\n",
      "[Iter.  970]  loss:5.976026  pct:-1.267185589\n",
      "[Iter.  980]  loss:5.901141  pct:-1.253080558\n",
      "[Iter.  990]  loss:5.828048  pct:-1.238631855\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.734715  pct:100.000000000\n",
      "[Iter.    2]  loss:2.734764  pct:0.001795954\n",
      "[Iter.    4]  loss:2.734761  pct:-0.000104617\n",
      "[Iter.    6]  loss:2.734761  pct:-0.000008718\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.734761\n",
      "Best loss: 2.734761 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  2%|▏         | 233/10000 [00:03<02:29, 65.13it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:627.754089  pct:100.000000000\n",
      "[Iter.   10]  loss:103.700272  pct:-83.480751762\n",
      "[Iter.   20]  loss:99.197685  pct:-4.341923406\n",
      "[Iter.   30]  loss:95.331879  pct:-3.897073374\n",
      "[Iter.   40]  loss:91.832115  pct:-3.671136600\n",
      "[Iter.   50]  loss:88.636078  pct:-3.480304561\n",
      "[Iter.   60]  loss:85.697662  pct:-3.315146154\n",
      "[Iter.   70]  loss:82.980598  pct:-3.170522776\n",
      "[Iter.   80]  loss:80.455833  pct:-3.042596778\n",
      "[Iter.   90]  loss:78.099564  pct:-2.928650088\n",
      "[Iter.  100]  loss:75.892113  pct:-2.826457364\n",
      "[Iter.  110]  loss:73.817047  pct:-2.734230921\n",
      "[Iter.  120]  loss:71.860535  pct:-2.650488644\n",
      "[Iter.  130]  loss:70.010696  pct:-2.574206086\n",
      "[Iter.  140]  loss:68.257393  pct:-2.504336648\n",
      "[Iter.  150]  loss:66.591843  pct:-2.440102327\n",
      "[Iter.  160]  loss:65.006340  pct:-2.380926194\n",
      "[Iter.  170]  loss:63.494312  pct:-2.325969651\n",
      "[Iter.  180]  loss:62.049789  pct:-2.275042922\n",
      "[Iter.  190]  loss:60.667572  pct:-2.227594034\n",
      "[Iter.  200]  loss:59.343060  pct:-2.183229751\n",
      "[Iter.  210]  loss:58.072021  pct:-2.141847868\n",
      "[Iter.  220]  loss:56.850796  pct:-2.102950280\n",
      "[Iter.  230]  loss:55.676090  pct:-2.066295625\n",
      "[Iter.  240]  loss:54.544807  pct:-2.031900591\n",
      "[Iter.  250]  loss:53.454250  pct:-1.999378400\n",
      "[Iter.  260]  loss:52.401810  pct:-1.968862414\n",
      "[Iter.  270]  loss:51.385380  pct:-1.939684731\n",
      "[Iter.  280]  loss:50.402840  pct:-1.912100552\n",
      "[Iter.  290]  loss:49.452347  pct:-1.885792279\n",
      "[Iter.  300]  loss:48.532112  pct:-1.860851385\n",
      "[Iter.  310]  loss:47.640629  pct:-1.836893694\n",
      "[Iter.  320]  loss:46.776230  pct:-1.814415506\n",
      "[Iter.  330]  loss:45.937622  pct:-1.792807566\n",
      "[Iter.  340]  loss:45.123611  pct:-1.771991199\n",
      "[Iter.  350]  loss:44.332954  pct:-1.752202490\n",
      "[Iter.  360]  loss:43.564590  pct:-1.733166587\n",
      "[Iter.  370]  loss:42.817455  pct:-1.715005592\n",
      "[Iter.  380]  loss:42.090672  pct:-1.697400622\n",
      "[Iter.  390]  loss:41.383343  pct:-1.680488266\n",
      "[Iter.  400]  loss:40.694569  pct:-1.664375237\n",
      "[Iter.  410]  loss:40.023571  pct:-1.648862839\n",
      "[Iter.  420]  loss:39.369644  pct:-1.633854333\n",
      "[Iter.  430]  loss:38.732071  pct:-1.619453911\n",
      "[Iter.  440]  loss:38.110214  pct:-1.605534315\n",
      "[Iter.  450]  loss:37.503525  pct:-1.591933988\n",
      "[Iter.  460]  loss:36.911385  pct:-1.578892121\n",
      "[Iter.  470]  loss:36.333286  pct:-1.566178846\n",
      "[Iter.  480]  loss:35.768635  pct:-1.554088680\n",
      "[Iter.  490]  loss:35.216999  pct:-1.542233147\n",
      "[Iter.  500]  loss:34.677921  pct:-1.530731673\n",
      "[Iter.  510]  loss:34.150948  pct:-1.519623163\n",
      "[Iter.  520]  loss:33.635681  pct:-1.508790986\n",
      "[Iter.  530]  loss:33.131676  pct:-1.498424931\n",
      "[Iter.  540]  loss:32.638645  pct:-1.488094210\n",
      "[Iter.  550]  loss:32.156189  pct:-1.478174737\n",
      "[Iter.  560]  loss:31.683945  pct:-1.468595247\n",
      "[Iter.  570]  loss:31.221592  pct:-1.459265117\n",
      "[Iter.  580]  loss:30.768856  pct:-1.450073083\n",
      "[Iter.  590]  loss:30.325451  pct:-1.441084292\n",
      "[Iter.  600]  loss:29.891075  pct:-1.432380229\n",
      "[Iter.  610]  loss:29.465450  pct:-1.423919499\n",
      "[Iter.  620]  loss:29.048326  pct:-1.415636926\n",
      "[Iter.  630]  loss:28.639359  pct:-1.407888237\n",
      "[Iter.  640]  loss:28.238451  pct:-1.399848101\n",
      "[Iter.  650]  loss:27.845371  pct:-1.392001840\n",
      "[Iter.  660]  loss:27.459826  pct:-1.384595404\n",
      "[Iter.  670]  loss:27.081652  pct:-1.377189480\n",
      "[Iter.  680]  loss:26.710676  pct:-1.369840727\n",
      "[Iter.  690]  loss:26.346704  pct:-1.362645062\n",
      "[Iter.  700]  loss:25.989491  pct:-1.355820324\n",
      "[Iter.  710]  loss:25.638922  pct:-1.348886663\n",
      "[Iter.  720]  loss:25.294827  pct:-1.342081518\n",
      "[Iter.  730]  loss:24.956999  pct:-1.335560386\n",
      "[Iter.  740]  loss:24.625284  pct:-1.329144712\n",
      "[Iter.  750]  loss:24.299543  pct:-1.322790071\n",
      "[Iter.  760]  loss:23.979610  pct:-1.316621192\n",
      "[Iter.  770]  loss:23.665329  pct:-1.310619555\n",
      "[Iter.  780]  loss:23.356583  pct:-1.304635732\n",
      "[Iter.  790]  loss:23.053253  pct:-1.298689421\n",
      "[Iter.  800]  loss:22.755222  pct:-1.292793043\n",
      "[Iter.  810]  loss:22.462315  pct:-1.287210956\n",
      "[Iter.  820]  loss:22.174427  pct:-1.281646964\n",
      "[Iter.  830]  loss:21.891489  pct:-1.275965341\n",
      "[Iter.  840]  loss:21.613335  pct:-1.270605087\n",
      "[Iter.  850]  loss:21.339848  pct:-1.265362775\n",
      "[Iter.  860]  loss:21.070986  pct:-1.259904832\n",
      "[Iter.  870]  loss:20.806557  pct:-1.254944097\n",
      "[Iter.  880]  loss:20.546583  pct:-1.249478853\n",
      "[Iter.  890]  loss:20.290871  pct:-1.244550040\n",
      "[Iter.  900]  loss:20.039379  pct:-1.239432012\n",
      "[Iter.  910]  loss:19.791998  pct:-1.234475424\n",
      "[Iter.  920]  loss:19.548649  pct:-1.229532645\n",
      "[Iter.  930]  loss:19.309231  pct:-1.224729299\n",
      "[Iter.  940]  loss:19.073696  pct:-1.219803473\n",
      "[Iter.  950]  loss:18.841881  pct:-1.215366631\n",
      "[Iter.  960]  loss:18.613802  pct:-1.210488722\n",
      "[Iter.  970]  loss:18.389353  pct:-1.205821133\n",
      "[Iter.  980]  loss:18.168491  pct:-1.201028864\n",
      "[Iter.  990]  loss:17.951094  pct:-1.196564346\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.722148  pct:100.000000000\n",
      "[Iter.    2]  loss:2.722075  pct:-0.002680093\n",
      "[Iter.    4]  loss:2.722050  pct:-0.000902147\n",
      "[Iter.    6]  loss:2.722040  pct:-0.000367869\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.722040\n",
      "Best loss: 2.722040 (trial 0)\n",
      "iteration 0\n",
      "Switched off factor 1\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 17%|█▋        | 1695/10000 [00:22<01:51, 74.46it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:404.786804  pct:100.000000000\n",
      "[Iter.   10]  loss:79.893143  pct:-80.262912078\n",
      "[Iter.   20]  loss:75.146271  pct:-5.941526128\n",
      "[Iter.   30]  loss:71.266991  pct:-5.162305530\n",
      "[Iter.   40]  loss:67.878662  pct:-4.754415082\n",
      "[Iter.   50]  loss:64.872772  pct:-4.428328136\n",
      "[Iter.   60]  loss:62.175083  pct:-4.158430362\n",
      "[Iter.   70]  loss:59.731106  pct:-3.930798693\n",
      "[Iter.   80]  loss:57.499653  pct:-3.735830623\n",
      "[Iter.   90]  loss:55.448803  pct:-3.566717036\n",
      "[Iter.  100]  loss:53.553169  pct:-3.418709867\n",
      "[Iter.  110]  loss:51.792488  pct:-3.287725408\n",
      "[Iter.  120]  loss:50.150150  pct:-3.170996141\n",
      "[Iter.  130]  loss:48.612362  pct:-3.066368459\n",
      "[Iter.  140]  loss:47.167683  pct:-2.971835154\n",
      "[Iter.  150]  loss:45.806370  pct:-2.886113521\n",
      "[Iter.  160]  loss:44.520164  pct:-2.807917977\n",
      "[Iter.  170]  loss:43.302044  pct:-2.736109781\n",
      "[Iter.  180]  loss:42.145844  pct:-2.670082759\n",
      "[Iter.  190]  loss:41.046188  pct:-2.609166314\n",
      "[Iter.  200]  loss:39.998436  pct:-2.552617971\n",
      "[Iter.  210]  loss:38.998375  pct:-2.500250349\n",
      "[Iter.  220]  loss:38.042454  pct:-2.451182067\n",
      "[Iter.  230]  loss:37.127380  pct:-2.405400557\n",
      "[Iter.  240]  loss:36.250202  pct:-2.362618055\n",
      "[Iter.  250]  loss:35.408382  pct:-2.322248464\n",
      "[Iter.  260]  loss:34.599575  pct:-2.284225706\n",
      "[Iter.  270]  loss:33.821568  pct:-2.248604228\n",
      "[Iter.  280]  loss:33.072544  pct:-2.214632532\n",
      "[Iter.  290]  loss:32.350681  pct:-2.182664844\n",
      "[Iter.  300]  loss:31.654474  pct:-2.152063012\n",
      "[Iter.  310]  loss:30.982367  pct:-2.123262863\n",
      "[Iter.  320]  loss:30.333090  pct:-2.095633115\n",
      "[Iter.  330]  loss:29.705326  pct:-2.069567432\n",
      "[Iter.  340]  loss:29.098003  pct:-2.044490915\n",
      "[Iter.  350]  loss:28.510057  pct:-2.020571413\n",
      "[Iter.  360]  loss:27.940529  pct:-1.997640940\n",
      "[Iter.  370]  loss:27.388494  pct:-1.975747777\n",
      "[Iter.  380]  loss:26.853130  pct:-1.954704561\n",
      "[Iter.  390]  loss:26.333645  pct:-1.934543448\n",
      "[Iter.  400]  loss:25.829369  pct:-1.914950544\n",
      "[Iter.  410]  loss:25.339609  pct:-1.896134021\n",
      "[Iter.  420]  loss:24.863672  pct:-1.878232955\n",
      "[Iter.  430]  loss:24.401047  pct:-1.860648334\n",
      "[Iter.  440]  loss:23.951126  pct:-1.843858007\n",
      "[Iter.  450]  loss:23.513397  pct:-1.827592072\n",
      "[Iter.  460]  loss:23.087362  pct:-1.811881641\n",
      "[Iter.  470]  loss:22.672604  pct:-1.796474959\n",
      "[Iter.  480]  loss:22.268677  pct:-1.781563584\n",
      "[Iter.  490]  loss:21.875183  pct:-1.767027546\n",
      "[Iter.  500]  loss:21.491743  pct:-1.752853980\n",
      "[Iter.  510]  loss:21.117973  pct:-1.739131901\n",
      "[Iter.  520]  loss:20.753511  pct:-1.725837480\n",
      "[Iter.  530]  loss:20.398020  pct:-1.712922844\n",
      "[Iter.  540]  loss:20.051239  pct:-1.700070794\n",
      "[Iter.  550]  loss:19.712837  pct:-1.687685206\n",
      "[Iter.  560]  loss:19.382559  pct:-1.675448303\n",
      "[Iter.  570]  loss:19.060101  pct:-1.663651689\n",
      "[Iter.  580]  loss:18.745207  pct:-1.652109450\n",
      "[Iter.  590]  loss:18.437632  pct:-1.640820657\n",
      "[Iter.  600]  loss:18.137180  pct:-1.629554626\n",
      "[Iter.  610]  loss:17.843616  pct:-1.618574869\n",
      "[Iter.  620]  loss:17.556683  pct:-1.608047893\n",
      "[Iter.  630]  loss:17.276209  pct:-1.597532494\n",
      "[Iter.  640]  loss:17.001999  pct:-1.587211512\n",
      "[Iter.  650]  loss:16.733831  pct:-1.577270398\n",
      "[Iter.  660]  loss:16.471571  pct:-1.567246799\n",
      "[Iter.  670]  loss:16.214907  pct:-1.558225846\n",
      "[Iter.  680]  loss:15.963926  pct:-1.547837320\n",
      "[Iter.  690]  loss:15.718348  pct:-1.538335626\n",
      "[Iter.  700]  loss:15.478011  pct:-1.529018349\n",
      "[Iter.  710]  loss:15.242775  pct:-1.519808753\n",
      "[Iter.  720]  loss:15.012518  pct:-1.510597872\n",
      "[Iter.  730]  loss:14.787099  pct:-1.501540551\n",
      "[Iter.  740]  loss:14.566381  pct:-1.492641562\n",
      "[Iter.  750]  loss:14.350201  pct:-1.484101337\n",
      "[Iter.  760]  loss:14.138498  pct:-1.475257050\n",
      "[Iter.  770]  loss:13.931144  pct:-1.466595257\n",
      "[Iter.  780]  loss:13.727996  pct:-1.458228353\n",
      "[Iter.  790]  loss:13.528946  pct:-1.449956363\n",
      "[Iter.  800]  loss:13.333928  pct:-1.441485654\n",
      "[Iter.  810]  loss:13.142822  pct:-1.433229886\n",
      "[Iter.  820]  loss:12.955509  pct:-1.425211998\n",
      "[Iter.  830]  loss:12.771948  pct:-1.416859210\n",
      "[Iter.  840]  loss:12.592013  pct:-1.408825839\n",
      "[Iter.  850]  loss:12.415604  pct:-1.400965170\n",
      "[Iter.  860]  loss:12.242663  pct:-1.392926669\n",
      "[Iter.  870]  loss:12.073118  pct:-1.384871644\n",
      "[Iter.  880]  loss:11.906876  pct:-1.376964895\n",
      "[Iter.  890]  loss:11.743831  pct:-1.369334281\n",
      "[Iter.  900]  loss:11.583948  pct:-1.361417325\n",
      "[Iter.  910]  loss:11.427129  pct:-1.353764207\n",
      "[Iter.  920]  loss:11.273329  pct:-1.345919990\n",
      "[Iter.  930]  loss:11.122447  pct:-1.338395874\n",
      "[Iter.  940]  loss:10.974476  pct:-1.330383081\n",
      "[Iter.  950]  loss:10.829314  pct:-1.322720379\n",
      "[Iter.  960]  loss:10.686887  pct:-1.315202804\n",
      "[Iter.  970]  loss:10.547146  pct:-1.307592629\n",
      "[Iter.  980]  loss:10.410029  pct:-1.300033528\n",
      "[Iter.  990]  loss:10.275494  pct:-1.292367045\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.763800  pct:100.000000000\n",
      "[Iter.    2]  loss:2.763826  pct:0.000948912\n",
      "[Iter.    4]  loss:2.763824  pct:-0.000077638\n",
      "[Iter.    6]  loss:2.763824  pct:-0.000017253\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.763824\n",
      "Best loss: 2.763824 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/sklearn/decomposition/_nmf.py:1641: ConvergenceWarning: Maximum number of iterations 200 reached. Increase it to improve convergence.\n",
      "  ConvergenceWarning,\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      "  9%|▊         | 873/10000 [00:11<01:58, 76.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:680.937988  pct:100.000000000\n",
      "[Iter.   10]  loss:105.383095  pct:-84.523830438\n",
      "[Iter.   20]  loss:100.899002  pct:-4.255039882\n",
      "[Iter.   30]  loss:97.404640  pct:-3.463227391\n",
      "[Iter.   40]  loss:94.209511  pct:-3.280264049\n",
      "[Iter.   50]  loss:91.261215  pct:-3.129509503\n",
      "[Iter.   60]  loss:88.522163  pct:-3.001331740\n",
      "[Iter.   70]  loss:85.964233  pct:-2.889592724\n",
      "[Iter.   80]  loss:83.567825  pct:-2.787680395\n",
      "[Iter.   90]  loss:81.318214  pct:-2.691958170\n",
      "[Iter.  100]  loss:79.201515  pct:-2.602982904\n",
      "[Iter.  110]  loss:77.203682  pct:-2.522468474\n",
      "[Iter.  120]  loss:75.312256  pct:-2.449916945\n",
      "[Iter.  130]  loss:73.516876  pct:-2.383914302\n",
      "[Iter.  140]  loss:71.808708  pct:-2.323504639\n",
      "[Iter.  150]  loss:70.180206  pct:-2.267833433\n",
      "[Iter.  160]  loss:68.624718  pct:-2.216420653\n",
      "[Iter.  170]  loss:67.136574  pct:-2.168524652\n",
      "[Iter.  180]  loss:65.710602  pct:-2.123986829\n",
      "[Iter.  190]  loss:64.342354  pct:-2.082233229\n",
      "[Iter.  200]  loss:63.027657  pct:-2.043284380\n",
      "[Iter.  210]  loss:61.762928  pct:-2.006624735\n",
      "[Iter.  220]  loss:60.544868  pct:-1.972153165\n",
      "[Iter.  230]  loss:59.370548  pct:-1.939586708\n",
      "[Iter.  240]  loss:58.237270  pct:-1.908821674\n",
      "[Iter.  250]  loss:57.142582  pct:-1.879704198\n",
      "[Iter.  260]  loss:56.084202  pct:-1.852174142\n",
      "[Iter.  270]  loss:55.060204  pct:-1.825823008\n",
      "[Iter.  280]  loss:54.068542  pct:-1.801048685\n",
      "[Iter.  290]  loss:53.107632  pct:-1.777208619\n",
      "[Iter.  300]  loss:52.175751  pct:-1.754702519\n",
      "[Iter.  310]  loss:51.271519  pct:-1.733050339\n",
      "[Iter.  320]  loss:50.393524  pct:-1.712441058\n",
      "[Iter.  330]  loss:49.540508  pct:-1.692709358\n",
      "[Iter.  340]  loss:48.711288  pct:-1.673821782\n",
      "[Iter.  350]  loss:47.904781  pct:-1.655688314\n",
      "[Iter.  360]  loss:47.120014  pct:-1.638181261\n",
      "[Iter.  370]  loss:46.355900  pct:-1.621634443\n",
      "[Iter.  380]  loss:45.611633  pct:-1.605548621\n",
      "[Iter.  390]  loss:44.886333  pct:-1.590164137\n",
      "[Iter.  400]  loss:44.179287  pct:-1.575193281\n",
      "[Iter.  410]  loss:43.489697  pct:-1.560890864\n",
      "[Iter.  420]  loss:42.816933  pct:-1.546949918\n",
      "[Iter.  430]  loss:42.160290  pct:-1.533605685\n",
      "[Iter.  440]  loss:41.519203  pct:-1.520593388\n",
      "[Iter.  450]  loss:40.893044  pct:-1.508120628\n",
      "[Iter.  460]  loss:40.281227  pct:-1.496138105\n",
      "[Iter.  470]  loss:39.683254  pct:-1.484495167\n",
      "[Iter.  480]  loss:39.098656  pct:-1.473161797\n",
      "[Iter.  490]  loss:38.526939  pct:-1.462240321\n",
      "[Iter.  500]  loss:37.967640  pct:-1.451710096\n",
      "[Iter.  510]  loss:37.420403  pct:-1.441325817\n",
      "[Iter.  520]  loss:36.884842  pct:-1.431199484\n",
      "[Iter.  530]  loss:36.360489  pct:-1.421594888\n",
      "[Iter.  540]  loss:35.847061  pct:-1.412048490\n",
      "[Iter.  550]  loss:35.344196  pct:-1.402806315\n",
      "[Iter.  560]  loss:34.851589  pct:-1.393742589\n",
      "[Iter.  570]  loss:34.368904  pct:-1.384972967\n",
      "[Iter.  580]  loss:33.895802  pct:-1.376542493\n",
      "[Iter.  590]  loss:33.431992  pct:-1.368340461\n",
      "[Iter.  600]  loss:32.977280  pct:-1.360110160\n",
      "[Iter.  610]  loss:32.531326  pct:-1.352304901\n",
      "[Iter.  620]  loss:32.093960  pct:-1.344447139\n",
      "[Iter.  630]  loss:31.664835  pct:-1.337089081\n",
      "[Iter.  640]  loss:31.243797  pct:-1.329669566\n",
      "[Iter.  650]  loss:30.830540  pct:-1.322686852\n",
      "[Iter.  660]  loss:30.424936  pct:-1.315589713\n",
      "[Iter.  670]  loss:30.026766  pct:-1.308697798\n",
      "[Iter.  680]  loss:29.635841  pct:-1.301919947\n",
      "[Iter.  690]  loss:29.251951  pct:-1.295357696\n",
      "[Iter.  700]  loss:28.874910  pct:-1.288942608\n",
      "[Iter.  710]  loss:28.504532  pct:-1.282700066\n",
      "[Iter.  720]  loss:28.140711  pct:-1.276362059\n",
      "[Iter.  730]  loss:27.783239  pct:-1.270300058\n",
      "[Iter.  740]  loss:27.431971  pct:-1.264318979\n",
      "[Iter.  750]  loss:27.086721  pct:-1.258564983\n",
      "[Iter.  760]  loss:26.747358  pct:-1.252876245\n",
      "[Iter.  770]  loss:26.413761  pct:-1.247215442\n",
      "[Iter.  780]  loss:26.085773  pct:-1.241730283\n",
      "[Iter.  790]  loss:25.763288  pct:-1.236248450\n",
      "[Iter.  800]  loss:25.446154  pct:-1.230956433\n",
      "[Iter.  810]  loss:25.134260  pct:-1.225699835\n",
      "[Iter.  820]  loss:24.827465  pct:-1.220625227\n",
      "[Iter.  830]  loss:24.525703  pct:-1.215434707\n",
      "[Iter.  840]  loss:24.228848  pct:-1.210387002\n",
      "[Iter.  850]  loss:23.936783  pct:-1.205441847\n",
      "[Iter.  860]  loss:23.649420  pct:-1.200508248\n",
      "[Iter.  870]  loss:23.366646  pct:-1.195690948\n",
      "[Iter.  880]  loss:23.088327  pct:-1.191092668\n",
      "[Iter.  890]  loss:22.814405  pct:-1.186408880\n",
      "[Iter.  900]  loss:22.544813  pct:-1.181675700\n",
      "[Iter.  910]  loss:22.279428  pct:-1.177142930\n",
      "[Iter.  920]  loss:22.018148  pct:-1.172741303\n",
      "[Iter.  930]  loss:21.760944  pct:-1.168145708\n",
      "[Iter.  940]  loss:21.507645  pct:-1.164010665\n",
      "[Iter.  950]  loss:21.258249  pct:-1.159566166\n",
      "[Iter.  960]  loss:21.012686  pct:-1.155144546\n",
      "[Iter.  970]  loss:20.770840  pct:-1.150952749\n",
      "[Iter.  980]  loss:20.532621  pct:-1.146888191\n",
      "[Iter.  990]  loss:20.298021  pct:-1.142572411\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.768071  pct:100.000000000\n",
      "[Iter.    2]  loss:2.768091  pct:0.000749346\n",
      "[Iter.    4]  loss:2.768085  pct:-0.000241167\n",
      "[Iter.    6]  loss:2.768082  pct:-0.000103357\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.768082\n",
      "Best loss: 2.768082 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:171: RuntimeWarning: divide by zero encountered in log\n",
      "  self.kappa = torch.tensor(np.log(kappa/(1- kappa))) #\n",
      "/home/kunesr/miniconda3/envs/spade_baselines/lib/python3.7/site-packages/spectra/spectra.py:175: RuntimeWarning: divide by zero encountered in log\n",
      "  self.rho = torch.tensor(np.log(rho/(1-rho)))\n",
      " 27%|██▋       | 2683/10000 [00:45<02:02, 59.51it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Iter.    0]  loss:983.228149  pct:100.000000000\n",
      "[Iter.   10]  loss:172.568314  pct:-82.448804131\n",
      "[Iter.   20]  loss:166.443481  pct:-3.549221769\n",
      "[Iter.   30]  loss:161.062714  pct:-3.232789759\n",
      "[Iter.   40]  loss:156.147949  pct:-3.051460076\n",
      "[Iter.   50]  loss:151.623260  pct:-2.897693337\n",
      "[Iter.   60]  loss:147.432510  pct:-2.763922968\n",
      "[Iter.   70]  loss:143.531952  pct:-2.645656960\n",
      "[Iter.   80]  loss:139.885788  pct:-2.540315165\n",
      "[Iter.   90]  loss:136.464462  pct:-2.445799343\n",
      "[Iter.  100]  loss:133.243164  pct:-2.360540000\n",
      "[Iter.  110]  loss:130.201172  pct:-2.283038090\n",
      "[Iter.  120]  loss:127.320763  pct:-2.212275972\n",
      "[Iter.  130]  loss:124.586777  pct:-2.147321336\n",
      "[Iter.  140]  loss:121.985985  pct:-2.087534487\n",
      "[Iter.  150]  loss:119.506622  pct:-2.032497825\n",
      "[Iter.  160]  loss:117.138550  pct:-1.981540825\n",
      "[Iter.  170]  loss:114.872971  pct:-1.934102161\n",
      "[Iter.  180]  loss:112.701660  pct:-1.890183926\n",
      "[Iter.  190]  loss:110.617744  pct:-1.849055025\n",
      "[Iter.  200]  loss:108.614815  pct:-1.810676666\n",
      "[Iter.  210]  loss:106.687141  pct:-1.774779384\n",
      "[Iter.  220]  loss:104.829575  pct:-1.741134694\n",
      "[Iter.  230]  loss:103.037399  pct:-1.709608476\n",
      "[Iter.  240]  loss:101.306274  pct:-1.680093723\n",
      "[Iter.  250]  loss:99.632378  pct:-1.652313047\n",
      "[Iter.  260]  loss:98.012505  pct:-1.625850035\n",
      "[Iter.  270]  loss:96.443703  pct:-1.600614010\n",
      "[Iter.  280]  loss:94.923592  pct:-1.576164168\n",
      "[Iter.  290]  loss:93.449890  pct:-1.552513397\n",
      "[Iter.  300]  loss:92.020279  pct:-1.529815823\n",
      "[Iter.  310]  loss:90.632454  pct:-1.508173012\n",
      "[Iter.  320]  loss:89.284233  pct:-1.487569592\n",
      "[Iter.  330]  loss:87.973618  pct:-1.467913756\n",
      "[Iter.  340]  loss:86.698570  pct:-1.449351905\n",
      "[Iter.  350]  loss:85.457397  pct:-1.431595454\n",
      "[Iter.  360]  loss:84.248657  pct:-1.414436047\n",
      "[Iter.  370]  loss:83.070839  pct:-1.398026197\n",
      "[Iter.  380]  loss:81.922462  pct:-1.382406245\n",
      "[Iter.  390]  loss:80.802307  pct:-1.367336016\n",
      "[Iter.  400]  loss:79.709122  pct:-1.352913628\n",
      "[Iter.  410]  loss:78.641808  pct:-1.339011302\n",
      "[Iter.  420]  loss:77.599289  pct:-1.325654443\n",
      "[Iter.  430]  loss:76.580429  pct:-1.312975772\n",
      "[Iter.  440]  loss:75.584557  pct:-1.300426897\n",
      "[Iter.  450]  loss:74.610786  pct:-1.288318918\n",
      "[Iter.  460]  loss:73.658203  pct:-1.276736727\n",
      "[Iter.  470]  loss:72.726089  pct:-1.265458032\n",
      "[Iter.  480]  loss:71.813553  pct:-1.254758269\n",
      "[Iter.  490]  loss:70.920013  pct:-1.244249021\n",
      "[Iter.  500]  loss:70.044762  pct:-1.234139318\n",
      "[Iter.  510]  loss:69.187325  pct:-1.224127420\n",
      "[Iter.  520]  loss:68.347031  pct:-1.214519986\n",
      "[Iter.  530]  loss:67.523201  pct:-1.205362754\n",
      "[Iter.  540]  loss:66.715378  pct:-1.196363871\n",
      "[Iter.  550]  loss:65.923058  pct:-1.187612628\n",
      "[Iter.  560]  loss:65.145714  pct:-1.179168229\n",
      "[Iter.  570]  loss:64.382858  pct:-1.170998804\n",
      "[Iter.  580]  loss:63.634140  pct:-1.162915536\n",
      "[Iter.  590]  loss:62.899101  pct:-1.155101267\n",
      "[Iter.  600]  loss:62.177292  pct:-1.147567092\n",
      "[Iter.  610]  loss:61.468365  pct:-1.140170524\n",
      "[Iter.  620]  loss:60.771912  pct:-1.133026879\n",
      "[Iter.  630]  loss:60.087635  pct:-1.125975081\n",
      "[Iter.  640]  loss:59.415138  pct:-1.119193317\n",
      "[Iter.  650]  loss:58.754139  pct:-1.112509905\n",
      "[Iter.  660]  loss:58.104294  pct:-1.106041438\n",
      "[Iter.  670]  loss:57.465332  pct:-1.099680850\n",
      "[Iter.  680]  loss:56.836933  pct:-1.093526955\n",
      "[Iter.  690]  loss:56.218761  pct:-1.087623237\n",
      "[Iter.  700]  loss:55.610664  pct:-1.081662173\n",
      "[Iter.  710]  loss:55.012283  pct:-1.076018511\n",
      "[Iter.  720]  loss:54.423504  pct:-1.070269063\n",
      "[Iter.  730]  loss:53.843925  pct:-1.064941355\n",
      "[Iter.  740]  loss:53.273441  pct:-1.059514432\n",
      "[Iter.  750]  loss:52.711864  pct:-1.054140355\n",
      "[Iter.  760]  loss:52.158806  pct:-1.049210894\n",
      "[Iter.  770]  loss:51.614132  pct:-1.044260717\n",
      "[Iter.  780]  loss:51.077694  pct:-1.039323860\n",
      "[Iter.  790]  loss:50.549305  pct:-1.034480879\n",
      "[Iter.  800]  loss:50.028828  pct:-1.029642832\n",
      "[Iter.  810]  loss:49.515965  pct:-1.025135273\n",
      "[Iter.  820]  loss:49.010609  pct:-1.020591722\n",
      "[Iter.  830]  loss:48.512478  pct:-1.016373418\n",
      "[Iter.  840]  loss:48.021553  pct:-1.011955803\n",
      "[Iter.  850]  loss:47.537674  pct:-1.007628989\n",
      "[Iter.  860]  loss:47.060703  pct:-1.003352989\n",
      "[Iter.  870]  loss:46.590378  pct:-0.999401703\n",
      "[Iter.  880]  loss:46.126625  pct:-0.995383099\n",
      "[Iter.  890]  loss:45.669350  pct:-0.991348034\n",
      "[Iter.  900]  loss:45.218338  pct:-0.987558748\n",
      "[Iter.  910]  loss:44.773533  pct:-0.983683091\n",
      "[Iter.  920]  loss:44.334751  pct:-0.980002493\n",
      "[Iter.  930]  loss:43.901833  pct:-0.976476776\n",
      "[Iter.  940]  loss:43.474728  pct:-0.972863602\n",
      "[Iter.  950]  loss:43.053349  pct:-0.969250671\n",
      "[Iter.  960]  loss:42.637531  pct:-0.965818629\n",
      "[Iter.  970]  loss:42.227203  pct:-0.962363202\n",
      "[Iter.  980]  loss:41.822205  pct:-0.959094486\n",
      "[Iter.  990]  loss:41.422485  pct:-0.955758412\n",
      "Reprojecting data...\n",
      "[Iter.    0]  loss:2.763025  pct:100.000000000\n",
      "[Iter.    2]  loss:2.763053  pct:0.000992323\n",
      "[Iter.    4]  loss:2.763045  pct:-0.000276122\n",
      "[Iter.    6]  loss:2.763043  pct:-0.000077660\n",
      "converged\n",
      "New best!\n",
      "Trial 0 loss: 2.763043\n",
      "Best loss: 2.763043 (trial 0)\n",
      "iteration 0\n",
      "iteration 100\n",
      "iteration 200\n",
      "iteration 300\n",
      "iteration 400\n",
      "iteration 500\n",
      "iteration 600\n",
      "iteration 700\n",
      "iteration 800\n",
      "iteration 900\n",
      "iteration 1000\n",
      "iteration 1100\n",
      "iteration 1200\n",
      "iteration 1300\n",
      "iteration 1400\n",
      "iteration 1500\n",
      "iteration 1600\n",
      "iteration 1700\n",
      "iteration 1800\n",
      "iteration 1900\n"
     ]
    }
   ],
   "source": [
    "N = 100\n",
    "k = 3\n",
    "p = 500\n",
    "sigma = 2.0\n",
    "lam = 0.02\n",
    "n_top = 20\n",
    "rhos = []\n",
    "models = []\n",
    "corr_group = []\n",
    "results = []\n",
    "model_kwargs = dict(a=0.3, c=0.3)\n",
    "\n",
    "for rho in [0.25,0.5,0.7, 0.85,0.9,0.95,0.99]:\n",
    "    for trial in range(10):\n",
    "        np.random.seed(trial)\n",
    "        data, A_star,theta_star = simulate_corr_data(rho,N,k,p,sigma)\n",
    "        A = construct_A_matrix_w(theta_star)\n",
    "        nmf = NMF(n_components=k)\n",
    "        Y_nmf = nmf.fit_transform(data)\n",
    "        nmf_theta = nmf.components_/nmf.components_.sum(axis = 0, keepdims =True)\n",
    "        theta = theta_star/theta_star.sum(axis = 1, keepdims =True)\n",
    "        \n",
    "        lst = []\n",
    "        for i in range(k):\n",
    "            lst.append(list(theta.argsort(axis = 0)[-1*n_top:,:][:,i]))\n",
    "        \n",
    "\n",
    "        model = spc.SPECTRA_Model(X = data, labels = None, L = k, adj_matrix = A, use_weights = False, lam = lam, delta=0.0,use_cell_types = False, kappa = 0.0, rho = 0.0)\n",
    "        #initialize eta\n",
    "        model.alpha = torch.log(torch.Tensor(Y_nmf) + 0.000000001)\n",
    "        model.internal_model.theta = nn.Parameter(torch.log(torch.Tensor(nmf_theta.T) + 0.000001))\n",
    "        model.internal_model.eta = nn.Parameter(torch.eye(k)*10)\n",
    "        model.train(X = data)\n",
    "        spectra_theta = torch.softmax(model.internal_model.theta, dim = 1).detach().numpy().T\n",
    "        #spectra_theta = theta*(model.internal_model.gene_scaling.exp().detach()/(1.0 + model.internal_model.gene_scaling.exp().detach()) + model.internal_model.delta).numpy().reshape(1,-1)\n",
    "        \n",
    "        theta = theta_star/theta_star.sum(axis = 1, keepdims =True)\n",
    "        \n",
    "        \n",
    "        model = run_trials(scipy.sparse.coo_matrix(data), vcells=None, nfactors=k, \n",
    "                        ntrials=1, min_iter=20,\n",
    "                        max_iter=1000, check_freq=10,\n",
    "                        epsilon=0.001,\n",
    "                        better_than_n_ago=5, dtype=np.float32,\n",
    "                        verbose=True, model_kwargs=model_kwargs,\n",
    "                        return_all=False, reproject=True,\n",
    "                        batchsize=0,\n",
    "                        beta_theta_simultaneous=True,\n",
    "                        loss_smoothing=1\n",
    "                        )\n",
    "        schpf_theta = model.beta.e_x.T / (model.beta.e_x.T.sum(axis = 0, keepdims = True))\n",
    "        \n",
    "        I = create_mask(np.array(lst), G_input = k, D= p).T.astype(int)\n",
    "        terms = np.array(range(k)).astype(str)\n",
    "        FA = slalom.initFA(Y = data.astype(float), terms = terms, I = I, noise='gauss', \n",
    "            nHidden=0, nHiddenSparse=0,do_preTrain=False, minGenes = 1, pruneGenes = False)\n",
    "        FA.train()\n",
    "        temp = (FA.getW()*FA.getZ()).T\n",
    "        slalom_theta = temp/temp.sum(axis = 0, keepdims = True)\n",
    "        \n",
    "        best_schpf = best_permutation(theta,schpf_theta)\n",
    "        best_nmf = best_permutation(theta,nmf_theta)\n",
    "        best_spade = best_permutation(theta,spectra_theta)\n",
    "        best_slalom = best_permutation(theta, slalom_theta)\n",
    "        \n",
    "        nfactors = [np.corrcoef(nmf_theta[best_nmf[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        sfactors = [np.corrcoef(spectra_theta[best_spade[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        hfactors = [np.corrcoef(schpf_theta[best_schpf[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        slfactors = [np.corrcoef(slalom_theta[best_slalom[0],:][i,:], theta[:,i])[0,1] for i in range(k)]\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(nfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"NMF\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(nfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"NMF\")\n",
    "        corr_group.append(\"C\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(sfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"Spectra\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(sfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"Spectra\")\n",
    "        corr_group.append(\"C\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(hfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"scHPF\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(hfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"scHPF\")\n",
    "        corr_group.append(\"C\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(slfactors[2:]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"slalom\")\n",
    "        corr_group.append(\"U\")\n",
    "        \n",
    "        #collect results\n",
    "        results.append(np.mean(slfactors[0:2]))\n",
    "        rhos.append(rho)\n",
    "        models.append(\"slalom\")\n",
    "        corr_group.append(\"C\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "ea0f780f",
   "metadata": {},
   "outputs": [],
   "source": [
    "results_df2 = pd.DataFrame()\n",
    "results_df2[\"rho\"] = rhos\n",
    "results_df2[\"model\"] = models\n",
    "results_df2[\"corr\"] = corr_group\n",
    "results_df2[\"value\"] = results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "9d65e5b7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 722.25x432 with 2 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "g = sns.catplot(x=\"rho\", y=\"value\", hue=\"model\", col=\"corr\",\n",
    "                capsize=.2,  height=6, aspect=.75,\n",
    "                kind=\"point\", data=results_df2)\n",
    "g.despine(left=True)\n",
    "plt.savefig(\"simulation_correlated_factors_N100.svg\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fbdff321",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f217389",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
